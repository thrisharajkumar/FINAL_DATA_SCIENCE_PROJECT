{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3af039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common combos across train devices and unseen 'C2M0025120D': 86335\n",
      "Rows after filtering to common combos & selected devices: 431675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12896\\635828464.py:95: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(sample_per_device).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled rows per device:\n",
      "Part_Number\n",
      "C2M0025120D    21584\n",
      "C2M0040120D    21584\n",
      "C2M0080120D    21584\n",
      "C2M0160120D    21584\n",
      "C2M0280120D    21584\n",
      "Name: count, dtype: int64\n",
      "Train shape: X=(86336, 36), y=(86336, 13)\n",
      "Test  shape: X=(21584, 36), y=(21584, 13)\n",
      "\n",
      "=== Metrics on unseen device: C2M0025120D ===\n",
      "                  target         RMSE         R2  test_count\n",
      "       overshoot_pulse_1 6.664583e+00   0.803265       21584\n",
      "       overshoot_pulse_2 1.224818e+01   0.552303       21584\n",
      "      undershoot_pulse_2 5.759954e+00   0.363720       21584\n",
      "      undershoot_pulse_1 6.194917e+00   0.270770       21584\n",
      "current_fall_time_pulse1 1.531377e-08  -0.925720       21584\n",
      "current_fall_time_pulse2 1.529532e-08  -0.928529       21584\n",
      "voltage_rise_time_pulse2 6.805546e-09  -3.047395       21584\n",
      "current_rise_time_pulse1 4.850171e-08  -3.954743       21584\n",
      "current_rise_time_pulse2 3.986886e-08  -7.521805       21584\n",
      "   ringing_frequency_MHz 7.940080e+00  -7.550750       21584\n",
      "voltage_rise_time_pulse1 7.003976e-09 -12.334110       21584\n",
      "voltage_fall_time_pulse2 1.192045e-08 -15.717433       21584\n",
      "voltage_fall_time_pulse1 1.193565e-08 -15.801246       21584\n",
      "\n",
      "Overall RMSE: 5.03093\n",
      "Overall R2  : -5.0609\n",
      "\n",
      "✅ Saved predictions: C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\xgb_unseen_C2M0025120D_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== XGBoost for unseen MOSFET (25% per device, common Vbus..Ls11 combos) =====\n",
    "# 1) Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# If xgboost isn't available in your environment, uncomment:\n",
    "# !pip install xgboost -q\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# ---- USER SETTINGS ----\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\Train_5_MOSFETs.csv\"     # <-- change to your merged CSV path\n",
    "DEVICE_COL = \"Part_Number\"            # or \"MOSFET\"/\"Device\" if that's your column\n",
    "UNSEEN_DEVICE = \"C2M0025120D\"         # <-- set the unseen MOSFET you want (the \"most similar\" one)\n",
    "SAMPLE_FRAC = 0.25                    # 25% per device\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Core input columns + targets (edit if your names differ)\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]  # used to match rows across devices\n",
    "INPUT_COLS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    \"Tp1\",\"Tp2\",\"Toff\",\"Tstart\",\"L1\",\"L2\",\"L3\",\"R1\",\n",
    "    # \"Part_Number\",  # <- intentionally DROPPED to prevent leakage and unseen label issues\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\"Ciss\",\"Coss\",\"Crss\",\"Rth_JC_typ\",\"Rth_JC_max\",\"Tj_max\",\n",
    "]\n",
    "TARGET_COLS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# 2) Load\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert DEVICE_COL in df.columns, f\"Device column '{DEVICE_COL}' not found. Columns: {df.columns.tolist()}\"\n",
    "\n",
    "# Keep only columns that exist\n",
    "INPUT_COLS = [c for c in INPUT_COLS if c in df.columns]\n",
    "TARGET_COLS = [c for c in TARGET_COLS if c in df.columns]\n",
    "missing_targets = [c for c in [\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\"undershoot_pulse_1\",\"undershoot_pulse_2\",\"ringing_frequency_MHz\"\n",
    "] if c not in TARGET_COLS]\n",
    "if missing_targets:\n",
    "    print(\"Warning: Missing targets:\", missing_targets)\n",
    "\n",
    "# 3) Filter to devices of interest (train devices = all except UNSEEN_DEVICE)\n",
    "devices_all = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices_all, f\"UNSEEN_DEVICE '{UNSEEN_DEVICE}' not found in data. Found: {devices_all}\"\n",
    "train_devices = [d for d in devices_all if d != UNSEEN_DEVICE]\n",
    "\n",
    "# 4) Build a combo key for (Vbus..Ls11) and keep ONLY combos shared by train+test\n",
    "for col in COMBO_COLS:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Combo column '{col}' missing from data. Present: {df.columns.tolist()}\")\n",
    "\n",
    "# Optional: round combo cols to reduce floating-point mismatch (adjust decimals if needed)\n",
    "df[\"_combo_key\"] = (\n",
    "    df[COMBO_COLS]\n",
    "    .apply(lambda row: tuple([float(row[c]) if pd.notna(row[c]) else np.nan for c in COMBO_COLS]), axis=1)\n",
    ")\n",
    "\n",
    "# Gather combos present in the unseen device\n",
    "combos_unseen = set(df.loc[df[DEVICE_COL] == UNSEEN_DEVICE, \"_combo_key\"].dropna().unique().tolist())\n",
    "\n",
    "# Gather combos present in ALL train devices\n",
    "combos_by_train_dev = []\n",
    "for d in train_devices:\n",
    "    combos_by_train_dev.append(set(df.loc[df[DEVICE_COL] == d, \"_combo_key\"].dropna().unique().tolist()))\n",
    "\n",
    "if combos_by_train_dev:\n",
    "    common_train = set.intersection(*combos_by_train_dev)\n",
    "else:\n",
    "    common_train = set()\n",
    "\n",
    "# Final common combos must be in BOTH (unseen) AND (all train devices)\n",
    "common_combos = common_train.intersection(combos_unseen)\n",
    "print(f\"Common combos across train devices and unseen '{UNSEEN_DEVICE}': {len(common_combos)}\")\n",
    "\n",
    "# 5) Filter to only common combos + selected devices\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])].copy()\n",
    "print(f\"Rows after filtering to common combos & selected devices: {len(df_common)}\")\n",
    "\n",
    "# 6) Sample 25% PER DEVICE\n",
    "def sample_per_device(g):\n",
    "    n = int(np.ceil(len(g) * SAMPLE_FRAC))\n",
    "    return g.sample(n=n, random_state=RANDOM_STATE)\n",
    "\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(sample_per_device).reset_index(drop=True)\n",
    "print(\"Sampled rows per device:\")\n",
    "print(df_sampled[DEVICE_COL].value_counts())\n",
    "\n",
    "# 7) Train/test split (leave-one-device-out)\n",
    "df_train = df_sampled[df_sampled[DEVICE_COL] != UNSEEN_DEVICE].copy()\n",
    "df_test  = df_sampled[df_sampled[DEVICE_COL] == UNSEEN_DEVICE].copy()\n",
    "\n",
    "X_train = df_train[INPUT_COLS].copy()\n",
    "y_train = df_train[TARGET_COLS].copy()\n",
    "X_test  = df_test[INPUT_COLS].copy()\n",
    "y_test  = df_test[TARGET_COLS].copy()\n",
    "\n",
    "# Handle any remaining NaNs (simple fill; you can do better imputation if needed)\n",
    "X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
    "X_test  = X_test.fillna(X_train.median(numeric_only=True))  # use train medians\n",
    "y_train = y_train.fillna(y_train.median(numeric_only=True))\n",
    "y_test  = y_test.fillna(y_train.median(numeric_only=True))\n",
    "\n",
    "print(f\"Train shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test  shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# 8) Model: Multi-output XGB (one model per target)\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model = MultiOutputRegressor(xgb)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 9) Evaluate on unseen device\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=TARGET_COLS, index=y_test.index)\n",
    "\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "def r2(a,b):   return float(r2_score(a, b))\n",
    "\n",
    "print(\"\\n=== Metrics on unseen device:\", UNSEEN_DEVICE, \"===\")\n",
    "per_target = []\n",
    "for col in TARGET_COLS:\n",
    "    r = {\n",
    "        \"target\": col,\n",
    "        \"RMSE\": rmse(y_test[col], y_pred[col]),\n",
    "        \"R2\": r2(y_test[col], y_pred[col]),\n",
    "        \"test_count\": int(y_test[col].notna().sum())\n",
    "    }\n",
    "    per_target.append(r)\n",
    "metrics_df = pd.DataFrame(per_target).sort_values(\"R2\", ascending=False)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Overall (macro) metrics\n",
    "overall_rmse = rmse(y_test.values, y_pred.values)\n",
    "overall_r2   = r2(y_test.values, y_pred.values)\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.6g}\")\n",
    "print(f\"Overall R2  : {overall_r2:.6g}\")\n",
    "\n",
    "# 10) Save predictions\n",
    "out_pred = df_test[[DEVICE_COL] + COMBO_COLS].copy()\n",
    "out_pred = out_pred.join(y_test.reset_index(drop=True), rsuffix=\"_true\")\n",
    "for c in TARGET_COLS:\n",
    "    out_pred[c+\"_pred\"] = y_pred[c].values\n",
    "OUT_PATH = os.path.join(os.path.dirname(CSV_PATH), f\"xgb_unseen_{UNSEEN_DEVICE}_predictions.csv\")\n",
    "out_pred.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved predictions: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12896\\818039271.py:57: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Target          RMSE         R2\n",
      "0   voltage_rise_time_pulse1  7.006509e-09 -12.343755\n",
      "1   voltage_rise_time_pulse2  6.807252e-09  -3.049424\n",
      "2   voltage_fall_time_pulse1  1.193986e-08 -15.813116\n",
      "3   voltage_fall_time_pulse2  1.192467e-08 -15.729264\n",
      "4   current_rise_time_pulse1  4.851796e-08  -3.958064\n",
      "5   current_rise_time_pulse2  3.989373e-08  -7.532442\n",
      "6   current_fall_time_pulse1  1.532225e-08  -0.927853\n",
      "7   current_fall_time_pulse2  1.530371e-08  -0.930645\n",
      "8          overshoot_pulse_1  6.765700e+00   0.797249\n",
      "9          overshoot_pulse_2  1.235287e+01   0.544617\n",
      "10        undershoot_pulse_1  6.120259e+00   0.288241\n",
      "11        undershoot_pulse_2  5.850825e+00   0.343485\n",
      "12     ringing_frequency_MHz  7.760237e+00  -7.167788\n",
      "\n",
      "Overall RMSE: 5.040444558727504\n",
      "Overall R2: -5.036827646252018\n"
     ]
    }
   ],
   "source": [
    "# ===== XGBoost for unseen MOSFET (25% per device, with Part_Number encoding) =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---- USER SETTINGS ----\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\Train_5_MOSFETs.csv\"     # <-- change to your merged CSV path\n",
    "DEVICE_COL = \"Part_Number\"            # or \"MOSFET\"/\"Device\" if that's your column\n",
    "UNSEEN_DEVICE = \"C2M0025120D\"         # <-- set the unseen MOSFET you want\n",
    "SAMPLE_FRAC = 0.25                    # 25% per device\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Core input columns + targets\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]\n",
    "INPUT_COLS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    \"Tp1\",\"Tp2\",\"Toff\",\"Tstart\",\"L1\",\"L2\",\"L3\",\"R1\",\n",
    "    \"Part_Number\",  # keep for encoding\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\"Ciss\",\"Coss\",\"Crss\",\n",
    "    \"Rth_JC_typ\",\"Rth_JC_max\",\"Tj_max\"\n",
    "]\n",
    "TARGET_COLS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Filter devices\n",
    "devices_all = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices_all, f\"{UNSEEN_DEVICE} not in dataset\"\n",
    "train_devices = [d for d in devices_all if d != UNSEEN_DEVICE]\n",
    "\n",
    "# Create combo key\n",
    "df[\"_combo_key\"] = df[COMBO_COLS].apply(lambda row: tuple(row.values), axis=1)\n",
    "\n",
    "# Common combos\n",
    "combos_unseen = set(df[df[DEVICE_COL] == UNSEEN_DEVICE][\"_combo_key\"].unique())\n",
    "combos_train_sets = [set(df[df[DEVICE_COL] == d][\"_combo_key\"].unique()) for d in train_devices]\n",
    "common_train = set.intersection(*combos_train_sets)\n",
    "common_combos = combos_unseen.intersection(common_train)\n",
    "\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])]\n",
    "\n",
    "# Sample 25% per device\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Split\n",
    "df_train = df_sampled[df_sampled[DEVICE_COL] != UNSEEN_DEVICE]\n",
    "df_test = df_sampled[df_sampled[DEVICE_COL] == UNSEEN_DEVICE]\n",
    "\n",
    "X_train = df_train[INPUT_COLS].copy()\n",
    "y_train = df_train[TARGET_COLS].copy()\n",
    "X_test = df_test[INPUT_COLS].copy()\n",
    "y_test = df_test[TARGET_COLS].copy()\n",
    "\n",
    "# Encode Part_Number\n",
    "le = LabelEncoder()\n",
    "X_train[DEVICE_COL] = le.fit_transform(X_train[DEVICE_COL])\n",
    "\n",
    "# For unseen device, assign a new unseen label code\n",
    "X_test[DEVICE_COL] = X_test[DEVICE_COL].map(lambda x: -1 if x not in le.classes_ else le.transform([x])[0])\n",
    "\n",
    "# Fill NaNs\n",
    "X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
    "X_test = X_test.fillna(X_train.median(numeric_only=True))\n",
    "y_train = y_train.fillna(y_train.median(numeric_only=True))\n",
    "y_test = y_test.fillna(y_train.median(numeric_only=True))\n",
    "\n",
    "# Train model\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model = MultiOutputRegressor(xgb)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=TARGET_COLS, index=y_test.index)\n",
    "\n",
    "# Metrics\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a,b)))\n",
    "def r2(a,b): return float(r2_score(a,b))\n",
    "\n",
    "metrics = []\n",
    "for col in TARGET_COLS:\n",
    "    metrics.append({\n",
    "        \"Target\": col,\n",
    "        \"RMSE\": rmse(y_test[col], y_pred[col]),\n",
    "        \"R2\": r2(y_test[col], y_pred[col])\n",
    "    })\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n",
    "\n",
    "print(\"\\nOverall RMSE:\", rmse(y_test.values, y_pred.values))\n",
    "print(\"Overall R2:\", r2(y_test.values, y_pred.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12938cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common combos found: 12 | Rows kept: 431675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12896\\342087783.py:113: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(86336, 34), y=(86336, 13) | Test: X=(21584, 34), y=(21584, 13)\n",
      "\n",
      "=== Unseen device: C2M0040120D ===\n",
      "                  target         RMSE        R2  n_test\n",
      "       overshoot_pulse_1 7.326889e+00  0.706456   21584\n",
      "      undershoot_pulse_2 6.252737e+00  0.588121   21584\n",
      "       overshoot_pulse_2 1.356814e+01  0.571090   21584\n",
      "      undershoot_pulse_1 6.535605e+00  0.556091   21584\n",
      "   ringing_frequency_MHz 3.311002e+00  0.247813   21584\n",
      "voltage_rise_time_pulse2 6.107509e-09 -0.713808   21584\n",
      "current_fall_time_pulse1 1.732867e-08 -1.382414   21584\n",
      "current_fall_time_pulse2 1.736815e-08 -1.426522   21584\n",
      "current_rise_time_pulse2 3.460742e-08 -3.261195   21584\n",
      "voltage_rise_time_pulse1 4.370969e-09 -5.443350   21584\n",
      "current_rise_time_pulse1 4.361475e-08 -6.906631   21584\n",
      "voltage_fall_time_pulse2 7.242882e-09 -8.668226   21584\n",
      "voltage_fall_time_pulse1 7.252886e-09 -8.705457   21584\n",
      "\n",
      "Overall RMSE: 5.04252249172598\n",
      "Overall R2  : -2.6029256572257866\n",
      "\n",
      "✅ Saved predictions: C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\xgb_unseen_C2M0040120D_with_derived.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== XGBoost unseen MOSFET with encoded Part_Number + derived features (25% per device) ====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --------- USER SETTINGS ---------\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\Train_5_MOSFETs.csv\"     # <-- change to your merged CSV path\n",
    "DEVICE_COL = \"Part_Number\"            # or \"MOSFET\"/\"Device\" if that's your column name\n",
    "UNSEEN_DEVICE = \"C2M0040120D\"         # <-- set the unseen MOSFET ID you want\n",
    "SAMPLE_FRAC = 0.25                    # 25% per device\n",
    "RANDOM_STATE = 42\n",
    "# ---------------------------------\n",
    "\n",
    "# Core columns\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]  # used to enforce shared configs\n",
    "RAW_INPUTS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    \"Part_Number\",  # will be encoded\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\n",
    "    \"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\n",
    "    \"Ciss\",\"Coss\",\"Crss\",\n",
    "    \"Rth_JC_typ\",\"Rth_JC_max\"\n",
    "]\n",
    "TARGETS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# ---- load & sanity\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert DEVICE_COL in df.columns, f\"Device column '{DEVICE_COL}' not found. Columns: {df.columns.tolist()}\"\n",
    "\n",
    "# keep only cols that exist\n",
    "RAW_INPUTS = [c for c in RAW_INPUTS if c in df.columns]\n",
    "TARGETS = [c for c in TARGETS if c in df.columns]\n",
    "for c in COMBO_COLS:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing combo column: {c}\")\n",
    "\n",
    "# --------- derived features we discussed ----------\n",
    "def add_derived_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "    eps = 1e-12      # numerical safety for divisions\n",
    "    c_stray = 5e-12  # small estimated stray capacitance (adjust if you have a better estimate)\n",
    "\n",
    "    # L_loop = sum of switching-loop inductances\n",
    "    L_pieces = [col for col in [\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"] if col in f.columns]\n",
    "    if L_pieces:\n",
    "        f[\"L_loop\"] = f[L_pieces].sum(axis=1)\n",
    "    else:\n",
    "        f[\"L_loop\"] = np.nan\n",
    "\n",
    "    # C_eq_est = Coss + small stray\n",
    "    if \"Coss\" in f.columns:\n",
    "        f[\"C_eq_est\"] = f[\"Coss\"].fillna(0) + c_stray\n",
    "    else:\n",
    "        f[\"C_eq_est\"] = np.nan\n",
    "\n",
    "    # f_res_est = 1/(2π√(L_loop*C_eq_est))\n",
    "    f[\"f_res_est\"] = 1.0 / (2.0 * np.pi * np.sqrt(np.clip(f[\"L_loop\"] * f[\"C_eq_est\"], eps, None)))\n",
    "\n",
    "    # gate_drive_strength proxy ~ 1 / (Rg + small)\n",
    "    if \"Rg\" in f.columns:\n",
    "        f[\"gate_drive_strength\"] = 1.0 / (f[\"Rg\"].abs() + 1e-3)\n",
    "    else:\n",
    "        f[\"gate_drive_strength\"] = np.nan\n",
    "\n",
    "    # dv/dt proxy ~ gate_drive_strength / Crss\n",
    "    if \"Crss\" in f.columns:\n",
    "        f[\"dvdt_proxy\"] = f[\"gate_drive_strength\"] / (f[\"Crss\"].abs() + eps)\n",
    "    else:\n",
    "        f[\"dvdt_proxy\"] = np.nan\n",
    "\n",
    "    # Miller ratio ~ Crss / Coss\n",
    "    if \"Crss\" in f.columns and \"Coss\" in f.columns:\n",
    "        f[\"miller_ratio\"] = f[\"Crss\"].abs() / (f[\"Coss\"].abs() + eps)\n",
    "    else:\n",
    "        f[\"miller_ratio\"] = np.nan\n",
    "\n",
    "    return f\n",
    "\n",
    "# make a safe combo key (round to limit float glitches)\n",
    "def make_combo_key(frame: pd.DataFrame) -> pd.Series:\n",
    "    rounded = frame[COMBO_COLS].apply(lambda s: np.round(s.astype(float), 6))\n",
    "    return rounded.apply(lambda row: tuple(row.values.tolist()), axis=1)\n",
    "\n",
    "# ---------- filter to devices and common combos ----------\n",
    "devices_all = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices_all, f\"UNSEEN_DEVICE '{UNSEEN_DEVICE}' not found. Devices: {devices_all}\"\n",
    "train_devices = [d for d in devices_all if d != UNSEEN_DEVICE]\n",
    "\n",
    "df[\"_combo_key\"] = make_combo_key(df)\n",
    "\n",
    "combos_unseen = set(df.loc[df[DEVICE_COL]==UNSEEN_DEVICE, \"_combo_key\"].dropna().unique())\n",
    "combos_train_sets = [set(df.loc[df[DEVICE_COL]==d, \"_combo_key\"].dropna().unique()) for d in train_devices]\n",
    "common_train = set.intersection(*combos_train_sets) if combos_train_sets else set()\n",
    "common_combos = combos_unseen.intersection(common_train)\n",
    "\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])].copy()\n",
    "print(f\"Common combos found: {len(common_combos)} | Rows kept: {len(df_common)}\")\n",
    "\n",
    "# ---------- sample 25% per device ----------\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ---------- build feature matrix with derived ----------\n",
    "feature_cols = list(RAW_INPUTS)  # start with raw inputs\n",
    "df_features = add_derived_features(df_sampled)\n",
    "\n",
    "# Add derived names to the feature list\n",
    "for new_c in [\"L_loop\",\"C_eq_est\",\"f_res_est\",\"gate_drive_strength\",\"dvdt_proxy\",\"miller_ratio\"]:\n",
    "    if new_c in df_features.columns:\n",
    "        feature_cols.append(new_c)\n",
    "\n",
    "# ensure uniqueness/order\n",
    "feature_cols = list(dict.fromkeys(feature_cols))\n",
    "\n",
    "# ---------- split train/test (leave-one-device-out) ----------\n",
    "df_train = df_features[df_features[DEVICE_COL] != UNSEEN_DEVICE].copy()\n",
    "df_test  = df_features[df_features[DEVICE_COL] == UNSEEN_DEVICE].copy()\n",
    "\n",
    "X_train = df_train[feature_cols].copy()\n",
    "y_train = df_train[[c for c in TARGETS if c in df_train.columns]].copy()\n",
    "X_test  = df_test[feature_cols].copy()\n",
    "y_test  = df_test[[c for c in TARGETS if c in df_test.columns]].copy()\n",
    "\n",
    "# ---------- encode Part_Number (DEVICE_COL) ----------\n",
    "le = LabelEncoder()\n",
    "if DEVICE_COL in X_train.columns:\n",
    "    X_train[DEVICE_COL] = le.fit_transform(X_train[DEVICE_COL].astype(str))\n",
    "    # unseen → -1\n",
    "    def encode_test_label(x):\n",
    "        x = str(x)\n",
    "        return le.transform([x])[0] if x in le.classes_ else -1\n",
    "    X_test[DEVICE_COL] = X_test[DEVICE_COL].astype(str).map(encode_test_label)\n",
    "\n",
    "# ---------- simple NaN handling ----------\n",
    "X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
    "X_test  = X_test.fillna(X_train.median(numeric_only=True))\n",
    "y_train = y_train.fillna(y_train.median(numeric_only=True))\n",
    "y_test  = y_test.fillna(y_train.median(numeric_only=True))\n",
    "\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape} | Test: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# ---------- model ----------\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model = MultiOutputRegressor(xgb)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ---------- evaluate ----------\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=y_test.columns, index=y_test.index)\n",
    "\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "def r2(a,b):   return float(r2_score(a, b))\n",
    "\n",
    "per_target = []\n",
    "for col in y_test.columns:\n",
    "    per_target.append({\n",
    "        \"target\": col,\n",
    "        \"RMSE\": rmse(y_test[col], y_pred[col]),\n",
    "        \"R2\": r2(y_test[col], y_pred[col]),\n",
    "        \"n_test\": int(y_test[col].notna().sum())\n",
    "    })\n",
    "metrics_df = pd.DataFrame(per_target).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Unseen device:\", UNSEEN_DEVICE, \"===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nOverall RMSE:\", rmse(y_test.values, y_pred.values))\n",
    "print(\"Overall R2  :\", r2(y_test.values, y_pred.values))\n",
    "\n",
    "# ---------- save predictions ----------\n",
    "out_pred = df_test[[DEVICE_COL] + COMBO_COLS].copy()\n",
    "out_pred = out_pred.join(y_test.reset_index(drop=True), rsuffix=\"_true\")\n",
    "for c in y_test.columns:\n",
    "    out_pred[c + \"_pred\"] = y_pred[c].values\n",
    "OUT_PATH = os.path.join(os.path.dirname(CSV_PATH), f\"xgb_unseen_{UNSEEN_DEVICE}_with_derived.csv\")\n",
    "out_pred.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved predictions: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435d88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12896\\3981005050.py:89: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3467 - mae: 0.4017 - val_loss: 0.0454 - val_mae: 0.1293 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1020 - mae: 0.2082 - val_loss: 0.0377 - val_mae: 0.1129 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0857 - mae: 0.1867 - val_loss: 0.0372 - val_mae: 0.1150 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0776 - mae: 0.1767 - val_loss: 0.0352 - val_mae: 0.1097 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0729 - mae: 0.1710 - val_loss: 0.0313 - val_mae: 0.0989 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0696 - mae: 0.1663 - val_loss: 0.0323 - val_mae: 0.1056 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0673 - mae: 0.1636 - val_loss: 0.0310 - val_mae: 0.1008 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0656 - mae: 0.1612 - val_loss: 0.0301 - val_mae: 0.0999 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0640 - mae: 0.1589 - val_loss: 0.0289 - val_mae: 0.0959 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0626 - mae: 0.1572 - val_loss: 0.0305 - val_mae: 0.1020 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0611 - mae: 0.1552 - val_loss: 0.0359 - val_mae: 0.1250 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0607 - mae: 0.1544 - val_loss: 0.0289 - val_mae: 0.1013 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0605 - mae: 0.1541 - val_loss: 0.0273 - val_mae: 0.0927 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0601 - mae: 0.1539 - val_loss: 0.0278 - val_mae: 0.0967 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0596 - mae: 0.1529 - val_loss: 0.0279 - val_mae: 0.0960 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0586 - mae: 0.1519 - val_loss: 0.0292 - val_mae: 0.1033 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0584 - mae: 0.1517 - val_loss: 0.0284 - val_mae: 0.1017 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0587 - mae: 0.1514 - val_loss: 0.0298 - val_mae: 0.1075 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0579 - mae: 0.1508 - val_loss: 0.0320 - val_mae: 0.1159 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0576 - mae: 0.1504 - val_loss: 0.0273 - val_mae: 0.0991 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0571 - mae: 0.1499 - val_loss: 0.0306 - val_mae: 0.1109 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0565 - mae: 0.1488 - val_loss: 0.0259 - val_mae: 0.0909 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0565 - mae: 0.1485 - val_loss: 0.0269 - val_mae: 0.0954 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0559 - mae: 0.1479 - val_loss: 0.0283 - val_mae: 0.1033 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0557 - mae: 0.1480 - val_loss: 0.0260 - val_mae: 0.0917 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0558 - mae: 0.1477 - val_loss: 0.0254 - val_mae: 0.0893 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0561 - mae: 0.1478 - val_loss: 0.0278 - val_mae: 0.1009 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0558 - mae: 0.1477 - val_loss: 0.0272 - val_mae: 0.0964 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0557 - mae: 0.1474 - val_loss: 0.0267 - val_mae: 0.0948 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0556 - mae: 0.1473 - val_loss: 0.0270 - val_mae: 0.0966 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0555 - mae: 0.1474 - val_loss: 0.0252 - val_mae: 0.0908 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0553 - mae: 0.1470 - val_loss: 0.0253 - val_mae: 0.0891 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0551 - mae: 0.1473 - val_loss: 0.0253 - val_mae: 0.0905 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0551 - mae: 0.1469 - val_loss: 0.0261 - val_mae: 0.0927 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0548 - mae: 0.1469 - val_loss: 0.0250 - val_mae: 0.0906 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0550 - mae: 0.1469 - val_loss: 0.0250 - val_mae: 0.0903 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0552 - mae: 0.1465 - val_loss: 0.0253 - val_mae: 0.0914 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0549 - mae: 0.1464 - val_loss: 0.0249 - val_mae: 0.0900 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0548 - mae: 0.1471 - val_loss: 0.0259 - val_mae: 0.0943 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0547 - mae: 0.1462 - val_loss: 0.0257 - val_mae: 0.0940 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0543 - mae: 0.1459 - val_loss: 0.0253 - val_mae: 0.0903 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0545 - mae: 0.1462 - val_loss: 0.0269 - val_mae: 0.0980 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0545 - mae: 0.1461 - val_loss: 0.0272 - val_mae: 0.0991 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0547 - mae: 0.1464 - val_loss: 0.0262 - val_mae: 0.0951 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0544 - mae: 0.1461 - val_loss: 0.0249 - val_mae: 0.0905 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0542 - mae: 0.1460 - val_loss: 0.0246 - val_mae: 0.0888 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0544 - mae: 0.1458 - val_loss: 0.0261 - val_mae: 0.0967 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0545 - mae: 0.1458 - val_loss: 0.0247 - val_mae: 0.0898 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0543 - mae: 0.1459 - val_loss: 0.0261 - val_mae: 0.0952 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0543 - mae: 0.1457 - val_loss: 0.0247 - val_mae: 0.0890 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0541 - mae: 0.1457 - val_loss: 0.0242 - val_mae: 0.0877 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0540 - mae: 0.1458 - val_loss: 0.0252 - val_mae: 0.0919 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0539 - mae: 0.1456 - val_loss: 0.0270 - val_mae: 0.1025 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0537 - mae: 0.1452 - val_loss: 0.0248 - val_mae: 0.0899 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0536 - mae: 0.1453 - val_loss: 0.0242 - val_mae: 0.0887 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0537 - mae: 0.1453 - val_loss: 0.0251 - val_mae: 0.0924 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0540 - mae: 0.1453 - val_loss: 0.0248 - val_mae: 0.0920 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0533 - mae: 0.1448 - val_loss: 0.0265 - val_mae: 0.0974 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0537 - mae: 0.1452 - val_loss: 0.0271 - val_mae: 0.1018 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0536 - mae: 0.1446 - val_loss: 0.0236 - val_mae: 0.0858 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0527 - mae: 0.1438 - val_loss: 0.0235 - val_mae: 0.0846 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0534 - mae: 0.1446 - val_loss: 0.0249 - val_mae: 0.0920 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0530 - mae: 0.1444 - val_loss: 0.0233 - val_mae: 0.0852 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0533 - mae: 0.1443 - val_loss: 0.0249 - val_mae: 0.0910 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0533 - mae: 0.1441 - val_loss: 0.0236 - val_mae: 0.0864 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0532 - mae: 0.1440 - val_loss: 0.0251 - val_mae: 0.0931 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0532 - mae: 0.1438 - val_loss: 0.0245 - val_mae: 0.0906 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0531 - mae: 0.1444 - val_loss: 0.0239 - val_mae: 0.0879 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0532 - mae: 0.1444 - val_loss: 0.0237 - val_mae: 0.0872 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0527 - mae: 0.1436 - val_loss: 0.0234 - val_mae: 0.0851 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0531 - mae: 0.1441 - val_loss: 0.0239 - val_mae: 0.0882 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0528 - mae: 0.1439 - val_loss: 0.0235 - val_mae: 0.0860 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0530 - mae: 0.1439 - val_loss: 0.0238 - val_mae: 0.0866 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0528 - mae: 0.1438 - val_loss: 0.0235 - val_mae: 0.0855 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0525 - mae: 0.1434 - val_loss: 0.0236 - val_mae: 0.0863 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0525 - mae: 0.1434 - val_loss: 0.0232 - val_mae: 0.0845 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0526 - mae: 0.1437 - val_loss: 0.0235 - val_mae: 0.0863 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0525 - mae: 0.1439 - val_loss: 0.0241 - val_mae: 0.0885 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0525 - mae: 0.1433 - val_loss: 0.0236 - val_mae: 0.0869 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0525 - mae: 0.1436 - val_loss: 0.0240 - val_mae: 0.0882 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0528 - mae: 0.1439 - val_loss: 0.0231 - val_mae: 0.0845 - learning_rate: 1.2500e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0524 - mae: 0.1432 - val_loss: 0.0234 - val_mae: 0.0857 - learning_rate: 1.2500e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0528 - mae: 0.1437 - val_loss: 0.0237 - val_mae: 0.0874 - learning_rate: 1.2500e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0524 - mae: 0.1434 - val_loss: 0.0234 - val_mae: 0.0851 - learning_rate: 1.2500e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0526 - mae: 0.1435 - val_loss: 0.0232 - val_mae: 0.0841 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0528 - mae: 0.1433 - val_loss: 0.0235 - val_mae: 0.0855 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0527 - mae: 0.1434 - val_loss: 0.0237 - val_mae: 0.0865 - learning_rate: 6.2500e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0521 - mae: 0.1427 - val_loss: 0.0232 - val_mae: 0.0843 - learning_rate: 6.2500e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0520 - mae: 0.1428 - val_loss: 0.0235 - val_mae: 0.0860 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0524 - mae: 0.1430 - val_loss: 0.0234 - val_mae: 0.0849 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0523 - mae: 0.1429 - val_loss: 0.0237 - val_mae: 0.0872 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0521 - mae: 0.1430 - val_loss: 0.0236 - val_mae: 0.0864 - learning_rate: 6.2500e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0525 - mae: 0.1432 - val_loss: 0.0229 - val_mae: 0.0834 - learning_rate: 3.1250e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0525 - mae: 0.1432 - val_loss: 0.0229 - val_mae: 0.0832 - learning_rate: 3.1250e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0519 - mae: 0.1429 - val_loss: 0.0233 - val_mae: 0.0849 - learning_rate: 3.1250e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0522 - mae: 0.1429 - val_loss: 0.0232 - val_mae: 0.0848 - learning_rate: 3.1250e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0519 - mae: 0.1428 - val_loss: 0.0231 - val_mae: 0.0842 - learning_rate: 3.1250e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0523 - mae: 0.1431 - val_loss: 0.0229 - val_mae: 0.0834 - learning_rate: 3.1250e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0521 - mae: 0.1431 - val_loss: 0.0232 - val_mae: 0.0843 - learning_rate: 3.1250e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0522 - mae: 0.1431 - val_loss: 0.0229 - val_mae: 0.0834 - learning_rate: 3.1250e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0523 - mae: 0.1431 - val_loss: 0.0231 - val_mae: 0.0839 - learning_rate: 3.1250e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0521 - mae: 0.1430 - val_loss: 0.0229 - val_mae: 0.0830 - learning_rate: 1.5625e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0521 - mae: 0.1425 - val_loss: 0.0228 - val_mae: 0.0830 - learning_rate: 1.5625e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0524 - mae: 0.1429 - val_loss: 0.0229 - val_mae: 0.0830 - learning_rate: 1.5625e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0520 - mae: 0.1430 - val_loss: 0.0232 - val_mae: 0.0847 - learning_rate: 1.5625e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0517 - mae: 0.1422 - val_loss: 0.0231 - val_mae: 0.0840 - learning_rate: 1.5625e-05\n",
      "Epoch 107/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0520 - mae: 0.1427 - val_loss: 0.0231 - val_mae: 0.0840 - learning_rate: 1.5625e-05\n",
      "Epoch 108/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0523 - mae: 0.1430 - val_loss: 0.0229 - val_mae: 0.0832 - learning_rate: 1.5625e-05\n",
      "Epoch 109/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0523 - mae: 0.1433 - val_loss: 0.0231 - val_mae: 0.0839 - learning_rate: 1.5625e-05\n",
      "Epoch 110/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0527 - mae: 0.1427 - val_loss: 0.0230 - val_mae: 0.0837 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0518 - mae: 0.1425 - val_loss: 0.0231 - val_mae: 0.0839 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0520 - mae: 0.1425 - val_loss: 0.0230 - val_mae: 0.0839 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0521 - mae: 0.1428 - val_loss: 0.0231 - val_mae: 0.0844 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0516 - mae: 0.1423 - val_loss: 0.0230 - val_mae: 0.0838 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0520 - mae: 0.1430 - val_loss: 0.0229 - val_mae: 0.0832 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0521 - mae: 0.1429 - val_loss: 0.0229 - val_mae: 0.0832 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0521 - mae: 0.1430 - val_loss: 0.0229 - val_mae: 0.0832 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0525 - mae: 0.1429 - val_loss: 0.0230 - val_mae: 0.0838 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0521 - mae: 0.1429 - val_loss: 0.0230 - val_mae: 0.0835 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0523 - mae: 0.1430 - val_loss: 0.0230 - val_mae: 0.0837 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0520 - mae: 0.1427 - val_loss: 0.0229 - val_mae: 0.0834 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0521 - mae: 0.1427 - val_loss: 0.0229 - val_mae: 0.0830 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0519 - mae: 0.1424 - val_loss: 0.0229 - val_mae: 0.0832 - learning_rate: 1.0000e-05\n",
      "\n",
      "=== Unseen device: C2M0040120D ===\n",
      "                  target         RMSE         R2  n_test\n",
      "      undershoot_pulse_2 6.707895e+00   0.525974   21584\n",
      "      undershoot_pulse_1 6.842332e+00   0.513447   21584\n",
      "current_fall_time_pulse1 7.903958e-09   0.504349   21584\n",
      "current_fall_time_pulse2 8.121481e-09   0.469424   21584\n",
      "       overshoot_pulse_2 1.550601e+01   0.439823   21584\n",
      "       overshoot_pulse_1 1.033846e+01   0.415551   21584\n",
      "voltage_rise_time_pulse2 5.022465e-09  -0.158958   21584\n",
      "current_rise_time_pulse1 2.164724e-08  -0.947735   21584\n",
      "current_rise_time_pulse2 2.699670e-08  -1.593070   21584\n",
      "voltage_rise_time_pulse1 3.355754e-09  -2.797838   21584\n",
      "voltage_fall_time_pulse2 5.259093e-09  -4.097365   21584\n",
      "voltage_fall_time_pulse1 5.269385e-09  -4.122879   21584\n",
      "   ringing_frequency_MHz 1.448736e+01 -13.400764   21584\n",
      "\n",
      "Overall RMSE: 7.065718918322362\n",
      "Overall R2  : -1.8653877197317699\n",
      "\n",
      "✅ Saved predictions: C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\ann_unseen_C2M0040120D_noEmb.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== Simple ANN for unseen MOSFET (common combos + 25% per device + derived + optional embedding) =====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, callbacks, Model\n",
    "\n",
    "# ------------ USER SETTINGS ------------\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\Train_5_MOSFETs.csv\"\n",
    "DEVICE_COL = \"Part_Number\"\n",
    "UNSEEN_DEVICE = \"C2M0040120D\"\n",
    "SAMPLE_FRAC = 0.25\n",
    "RANDOM_STATE = 42\n",
    "USE_EMBEDDING = False   # set True to include Part_Number embedding (an 'UNK' slot is used for unseen)\n",
    "# --------------------------------------\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]\n",
    "RAW_INPUTS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    # timing/fixture columns if present (optional); add if you have them in this file\n",
    "    # \"Tp1\",\"Tp2\",\"Toff\",\"Tstart\",\n",
    "    # \"L1\",\"L2\",\"L3\",\"R1\",\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\n",
    "    \"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\n",
    "    \"Ciss\",\"Coss\",\"Crss\",\n",
    "    \"Rth_JC_typ\",\"Rth_JC_max\"\n",
    "]\n",
    "TARGETS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# ---- load ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert DEVICE_COL in df.columns, f\"'{DEVICE_COL}' not found in columns.\"\n",
    "\n",
    "# keep available columns only\n",
    "RAW_INPUTS = [c for c in RAW_INPUTS if c in df.columns]\n",
    "TARGETS = [c for c in TARGETS if c in df.columns]\n",
    "for c in COMBO_COLS:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing combo column: {c}\")\n",
    "\n",
    "# ---- derived features ----\n",
    "def add_derived_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "    eps = 1e-12\n",
    "    c_stray = 5e-12\n",
    "    # L_loop (if L1/L2/L3 not in this file, only sum Ls4..Ls11)\n",
    "    L_pieces = [c for c in [\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\"L1\",\"L2\",\"L3\"] if c in f.columns]\n",
    "    f[\"L_loop\"] = f[L_pieces].sum(axis=1) if L_pieces else np.nan\n",
    "    # C_eq and resonance\n",
    "    f[\"C_eq_est\"] = (f[\"Coss\"].fillna(0) if \"Coss\" in f.columns else 0) + c_stray\n",
    "    f[\"f_res_est\"] = 1.0/(2.0*np.pi*np.sqrt(np.clip(f[\"L_loop\"]*f[\"C_eq_est\"], eps, None)))\n",
    "    # drive proxies\n",
    "    f[\"gate_drive_strength\"] = 1.0/(f[\"Rg\"].abs() + 1e-3) if \"Rg\" in f.columns else np.nan\n",
    "    f[\"dvdt_proxy\"] = f[\"gate_drive_strength\"]/((f[\"Crss\"].abs() if \"Crss\" in f.columns else 0)+eps)\n",
    "    f[\"miller_ratio\"] = (f[\"Crss\"].abs()/(f[\"Coss\"].abs()+eps)) if (\"Crss\" in f.columns and \"Coss\" in f.columns) else np.nan\n",
    "    return f\n",
    "\n",
    "# ---- make combo key with rounding to avoid float mismatches ----\n",
    "def make_combo_key(frame: pd.DataFrame) -> pd.Series:\n",
    "    rounded = frame[COMBO_COLS].apply(lambda s: np.round(s.astype(float), 6))\n",
    "    return rounded.apply(lambda row: tuple(row.values.tolist()), axis=1)\n",
    "\n",
    "# ---- filter to common combos & sample 25%/device ----\n",
    "devices = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices, f\"{UNSEEN_DEVICE} not in dataset devices {devices}\"\n",
    "train_devices = [d for d in devices if d != UNSEEN_DEVICE]\n",
    "\n",
    "df[\"_combo_key\"] = make_combo_key(df)\n",
    "combos_unseen = set(df.loc[df[DEVICE_COL]==UNSEEN_DEVICE, \"_combo_key\"].dropna().unique())\n",
    "combos_train_sets = [set(df.loc[df[DEVICE_COL]==d, \"_combo_key\"].dropna().unique()) for d in train_devices]\n",
    "common_train = set.intersection(*combos_train_sets) if combos_train_sets else set()\n",
    "common_combos = combos_unseen.intersection(common_train)\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])].copy()\n",
    "\n",
    "# sample 25% per device\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ---- features + derived ----\n",
    "feature_cols = list(RAW_INPUTS)  # start with your raw inputs\n",
    "df_feat = add_derived_features(df_sampled)\n",
    "for c in [\"L_loop\",\"C_eq_est\",\"f_res_est\",\"gate_drive_strength\",\"dvdt_proxy\",\"miller_ratio\"]:\n",
    "    if c in df_feat.columns: feature_cols.append(c)\n",
    "feature_cols = list(dict.fromkeys(feature_cols))  # de-dupe\n",
    "\n",
    "# ---- split train/test ----\n",
    "df_train = df_feat[df_feat[DEVICE_COL] != UNSEEN_DEVICE].copy()\n",
    "df_test  = df_feat[df_feat[DEVICE_COL] == UNSEEN_DEVICE].copy()\n",
    "\n",
    "# optional device embedding\n",
    "if USE_EMBEDDING:\n",
    "    le = LabelEncoder()\n",
    "    known = df_train[DEVICE_COL].astype(str)\n",
    "    le.fit(known.unique().tolist())\n",
    "    # map train to ids\n",
    "    df_train[\"_dev_id\"] = le.transform(df_train[DEVICE_COL].astype(str))\n",
    "    # build UNK for unseen\n",
    "    unk_id = len(le.classes_)  # next index\n",
    "    df_test[\"_dev_id\"] = df_test[DEVICE_COL].astype(str).map(lambda s: le.transform([s])[0] if s in le.classes_ else unk_id)\n",
    "else:\n",
    "    # drop device col from features\n",
    "    feature_cols = [c for c in feature_cols if c != DEVICE_COL]\n",
    "\n",
    "# assemble matrices\n",
    "X_num_train = df_train[feature_cols].copy()\n",
    "X_num_test  = df_test[feature_cols].copy()\n",
    "Y_cols = [c for c in TARGETS if c in df_train.columns]\n",
    "y_train = df_train[Y_cols].copy()\n",
    "y_test  = df_test[Y_cols].copy()\n",
    "\n",
    "# ---- scale inputs & outputs ----\n",
    "Xscaler = StandardScaler()\n",
    "Yscalers = {c: StandardScaler() for c in Y_cols}\n",
    "\n",
    "X_train = Xscaler.fit_transform(X_num_train.fillna(X_num_train.median(numeric_only=True)))\n",
    "X_test  = Xscaler.transform(X_num_test.fillna(X_num_train.median(numeric_only=True)))\n",
    "\n",
    "y_train_scaled = np.zeros_like(y_train.values, dtype=float)\n",
    "y_test_scaled  = np.zeros_like(y_test.values, dtype=float)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_train_scaled[:, i] = Yscalers[c].fit_transform(y_train[[c]].values).ravel()\n",
    "    y_test_scaled[:, i]  = Yscalers[c].transform(y_test[[c]].values).ravel()\n",
    "\n",
    "# ---- build model ----\n",
    "def build_model(n_num_features, n_targets, n_devices=None, emb_dim=4):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    if USE_EMBEDDING:\n",
    "        # two-input model: numeric + device id\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        inp_dev = layers.Input(shape=(), dtype=\"int32\", name=\"dev\")\n",
    "        emb = layers.Embedding(input_dim=n_devices, output_dim=emb_dim, name=\"dev_emb\")(inp_dev)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        x = layers.Concatenate()([inp_num, emb])\n",
    "    else:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        x = inp_num\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    out = layers.Dense(n_targets, activation=\"linear\", name=\"y\")(x)\n",
    "\n",
    "    if USE_EMBEDDING:\n",
    "        model = Model(inputs=[inp_num, inp_dev], outputs=out)\n",
    "    else:\n",
    "        model = Model(inputs=inp_num, outputs=out)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "n_devices = (df_train[\"_dev_id\"].max() + 2) if USE_EMBEDDING else None  # +1 for 0-based, +1 for UNK\n",
    "model = build_model(X_train.shape[1], len(Y_cols), n_devices=n_devices, emb_dim=4)\n",
    "\n",
    "# ---- train ----\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=8, min_lr=1e-5)\n",
    "if USE_EMBEDDING:\n",
    "    hist = model.fit(\n",
    "        {\"num\": X_train, \"dev\": df_train[\"_dev_id\"].values},\n",
    "        y_train_scaled,\n",
    "        validation_split=0.15,\n",
    "        epochs=200,\n",
    "        batch_size=256,\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    hist = model.fit(\n",
    "        X_train, y_train_scaled,\n",
    "        validation_split=0.15,\n",
    "        epochs=200,\n",
    "        batch_size=256,\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# ---- predict ----\n",
    "if USE_EMBEDDING:\n",
    "    y_pred_scaled = model.predict({\"num\": X_test, \"dev\": df_test[\"_dev_id\"].values}, verbose=0)\n",
    "else:\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# inverse-scale predictions\n",
    "y_pred = np.zeros_like(y_pred_scaled)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_pred[:, i] = Yscalers[c].inverse_transform(y_pred_scaled[:, [i]]).ravel()\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=Y_cols, index=y_test.index)\n",
    "\n",
    "# ---- metrics ----\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "def r2(a,b):   return float(r2_score(a, b))\n",
    "\n",
    "per_target = []\n",
    "for c in Y_cols:\n",
    "    per_target.append({\"target\": c, \"RMSE\": rmse(y_test[c], y_pred_df[c]), \"R2\": r2(y_test[c], y_pred_df[c]), \"n_test\": int(y_test[c].size)})\n",
    "metrics_df = pd.DataFrame(per_target).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Unseen device:\", UNSEEN_DEVICE, \"===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nOverall RMSE:\", rmse(y_test.values, y_pred_df.values))\n",
    "print(\"Overall R2  :\", r2(y_test.values, y_pred_df.values))\n",
    "\n",
    "# ---- save predictions ----\n",
    "OUT_PATH = os.path.join(os.path.dirname(CSV_PATH), f\"ann_unseen_{UNSEEN_DEVICE}_{'withEmb' if USE_EMBEDDING else 'noEmb'}.csv\")\n",
    "save_cols = [DEVICE_COL] + COMBO_COLS\n",
    "save_cols = [c for c in save_cols if c in df_test.columns]\n",
    "out = df_test[save_cols].copy()\n",
    "for c in Y_cols:\n",
    "    out[c+\"_true\"] = y_test[c].values\n",
    "    out[c+\"_pred\"] = y_pred_df[c].values\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved predictions: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d2a36f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12896\\3560688002.py:89: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3249 - mae: 0.3925 - val_loss: 0.0442 - val_mae: 0.1247 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1106 - mae: 0.2114 - val_loss: 0.0398 - val_mae: 0.1186 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0931 - mae: 0.1900 - val_loss: 0.0353 - val_mae: 0.1071 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0854 - mae: 0.1799 - val_loss: 0.0343 - val_mae: 0.1055 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0812 - mae: 0.1740 - val_loss: 0.0335 - val_mae: 0.1057 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0772 - mae: 0.1690 - val_loss: 0.0327 - val_mae: 0.1050 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0748 - mae: 0.1654 - val_loss: 0.0307 - val_mae: 0.0997 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0729 - mae: 0.1627 - val_loss: 0.0327 - val_mae: 0.1095 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0710 - mae: 0.1600 - val_loss: 0.0300 - val_mae: 0.0999 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0698 - mae: 0.1586 - val_loss: 0.0306 - val_mae: 0.1052 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0686 - mae: 0.1573 - val_loss: 0.0285 - val_mae: 0.0945 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0680 - mae: 0.1562 - val_loss: 0.0294 - val_mae: 0.1013 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0673 - mae: 0.1552 - val_loss: 0.0284 - val_mae: 0.0973 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0669 - mae: 0.1545 - val_loss: 0.0273 - val_mae: 0.0919 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0659 - mae: 0.1536 - val_loss: 0.0286 - val_mae: 0.1010 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0657 - mae: 0.1533 - val_loss: 0.0285 - val_mae: 0.0979 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0647 - mae: 0.1523 - val_loss: 0.0265 - val_mae: 0.0915 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0650 - mae: 0.1525 - val_loss: 0.0303 - val_mae: 0.1097 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0646 - mae: 0.1523 - val_loss: 0.0263 - val_mae: 0.0927 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0644 - mae: 0.1517 - val_loss: 0.0273 - val_mae: 0.0980 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0643 - mae: 0.1518 - val_loss: 0.0267 - val_mae: 0.0937 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0638 - mae: 0.1508 - val_loss: 0.0288 - val_mae: 0.1049 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0641 - mae: 0.1508 - val_loss: 0.0264 - val_mae: 0.0921 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0634 - mae: 0.1502 - val_loss: 0.0264 - val_mae: 0.0944 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0633 - mae: 0.1504 - val_loss: 0.0270 - val_mae: 0.0974 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0630 - mae: 0.1500 - val_loss: 0.0263 - val_mae: 0.0938 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0631 - mae: 0.1496 - val_loss: 0.0294 - val_mae: 0.1087 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0618 - mae: 0.1481 - val_loss: 0.0273 - val_mae: 0.0992 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0618 - mae: 0.1480 - val_loss: 0.0267 - val_mae: 0.0960 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0613 - mae: 0.1476 - val_loss: 0.0258 - val_mae: 0.0911 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0609 - mae: 0.1470 - val_loss: 0.0265 - val_mae: 0.0953 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0615 - mae: 0.1480 - val_loss: 0.0255 - val_mae: 0.0904 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0618 - mae: 0.1477 - val_loss: 0.0266 - val_mae: 0.0971 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0612 - mae: 0.1472 - val_loss: 0.0249 - val_mae: 0.0860 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0611 - mae: 0.1473 - val_loss: 0.0268 - val_mae: 0.0970 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0608 - mae: 0.1470 - val_loss: 0.0264 - val_mae: 0.0942 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0612 - mae: 0.1470 - val_loss: 0.0261 - val_mae: 0.0928 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0606 - mae: 0.1468 - val_loss: 0.0282 - val_mae: 0.1022 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0611 - mae: 0.1472 - val_loss: 0.0267 - val_mae: 0.0966 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0607 - mae: 0.1467 - val_loss: 0.0262 - val_mae: 0.0933 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0607 - mae: 0.1470 - val_loss: 0.0257 - val_mae: 0.0928 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0604 - mae: 0.1464 - val_loss: 0.0247 - val_mae: 0.0864 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0606 - mae: 0.1468 - val_loss: 0.0249 - val_mae: 0.0878 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0604 - mae: 0.1467 - val_loss: 0.0254 - val_mae: 0.0915 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0607 - mae: 0.1469 - val_loss: 0.0265 - val_mae: 0.0981 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0604 - mae: 0.1464 - val_loss: 0.0257 - val_mae: 0.0928 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0605 - mae: 0.1468 - val_loss: 0.0309 - val_mae: 0.1154 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0602 - mae: 0.1463 - val_loss: 0.0256 - val_mae: 0.0929 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0605 - mae: 0.1467 - val_loss: 0.0269 - val_mae: 0.0996 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0608 - mae: 0.1464 - val_loss: 0.0249 - val_mae: 0.0893 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0600 - mae: 0.1454 - val_loss: 0.0243 - val_mae: 0.0863 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0599 - mae: 0.1457 - val_loss: 0.0252 - val_mae: 0.0905 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0594 - mae: 0.1449 - val_loss: 0.0244 - val_mae: 0.0861 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0595 - mae: 0.1454 - val_loss: 0.0250 - val_mae: 0.0893 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0593 - mae: 0.1449 - val_loss: 0.0241 - val_mae: 0.0847 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0594 - mae: 0.1451 - val_loss: 0.0244 - val_mae: 0.0877 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0594 - mae: 0.1451 - val_loss: 0.0252 - val_mae: 0.0914 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0590 - mae: 0.1449 - val_loss: 0.0247 - val_mae: 0.0884 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0593 - mae: 0.1449 - val_loss: 0.0252 - val_mae: 0.0908 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0590 - mae: 0.1446 - val_loss: 0.0252 - val_mae: 0.0921 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0595 - mae: 0.1454 - val_loss: 0.0248 - val_mae: 0.0892 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0590 - mae: 0.1447 - val_loss: 0.0239 - val_mae: 0.0840 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0589 - mae: 0.1444 - val_loss: 0.0243 - val_mae: 0.0868 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0588 - mae: 0.1445 - val_loss: 0.0249 - val_mae: 0.0905 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0593 - mae: 0.1448 - val_loss: 0.0242 - val_mae: 0.0863 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0590 - mae: 0.1444 - val_loss: 0.0239 - val_mae: 0.0846 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0588 - mae: 0.1442 - val_loss: 0.0243 - val_mae: 0.0883 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0589 - mae: 0.1444 - val_loss: 0.0252 - val_mae: 0.0909 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0590 - mae: 0.1445 - val_loss: 0.0240 - val_mae: 0.0858 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0587 - mae: 0.1445 - val_loss: 0.0240 - val_mae: 0.0861 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0588 - mae: 0.1439 - val_loss: 0.0242 - val_mae: 0.0856 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0584 - mae: 0.1441 - val_loss: 0.0240 - val_mae: 0.0856 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0581 - mae: 0.1436 - val_loss: 0.0235 - val_mae: 0.0833 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0587 - mae: 0.1442 - val_loss: 0.0240 - val_mae: 0.0846 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0587 - mae: 0.1440 - val_loss: 0.0236 - val_mae: 0.0833 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0584 - mae: 0.1438 - val_loss: 0.0239 - val_mae: 0.0850 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0585 - mae: 0.1441 - val_loss: 0.0238 - val_mae: 0.0850 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0586 - mae: 0.1439 - val_loss: 0.0241 - val_mae: 0.0856 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0585 - mae: 0.1438 - val_loss: 0.0237 - val_mae: 0.0839 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0583 - mae: 0.1439 - val_loss: 0.0233 - val_mae: 0.0817 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0587 - mae: 0.1442 - val_loss: 0.0237 - val_mae: 0.0839 - learning_rate: 1.2500e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0584 - mae: 0.1438 - val_loss: 0.0240 - val_mae: 0.0854 - learning_rate: 1.2500e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0585 - mae: 0.1440 - val_loss: 0.0246 - val_mae: 0.0876 - learning_rate: 1.2500e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0580 - mae: 0.1433 - val_loss: 0.0241 - val_mae: 0.0863 - learning_rate: 1.2500e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0584 - mae: 0.1435 - val_loss: 0.0235 - val_mae: 0.0836 - learning_rate: 1.2500e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0582 - mae: 0.1440 - val_loss: 0.0241 - val_mae: 0.0864 - learning_rate: 1.2500e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0583 - mae: 0.1435 - val_loss: 0.0233 - val_mae: 0.0822 - learning_rate: 1.2500e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0585 - mae: 0.1439 - val_loss: 0.0239 - val_mae: 0.0855 - learning_rate: 1.2500e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0579 - mae: 0.1434 - val_loss: 0.0235 - val_mae: 0.0834 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0583 - mae: 0.1434 - val_loss: 0.0235 - val_mae: 0.0830 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0585 - mae: 0.1438 - val_loss: 0.0238 - val_mae: 0.0842 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0581 - mae: 0.1435 - val_loss: 0.0238 - val_mae: 0.0842 - learning_rate: 6.2500e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0584 - mae: 0.1433 - val_loss: 0.0237 - val_mae: 0.0838 - learning_rate: 6.2500e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0580 - mae: 0.1429 - val_loss: 0.0235 - val_mae: 0.0834 - learning_rate: 6.2500e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0582 - mae: 0.1436 - val_loss: 0.0239 - val_mae: 0.0847 - learning_rate: 6.2500e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0583 - mae: 0.1435 - val_loss: 0.0234 - val_mae: 0.0821 - learning_rate: 6.2500e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0578 - mae: 0.1433 - val_loss: 0.0237 - val_mae: 0.0838 - learning_rate: 3.1250e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0580 - mae: 0.1434 - val_loss: 0.0234 - val_mae: 0.0827 - learning_rate: 3.1250e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0581 - mae: 0.1436 - val_loss: 0.0235 - val_mae: 0.0834 - learning_rate: 3.1250e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0581 - mae: 0.1436 - val_loss: 0.0235 - val_mae: 0.0834 - learning_rate: 3.1250e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0578 - mae: 0.1430 - val_loss: 0.0234 - val_mae: 0.0823 - learning_rate: 3.1250e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0579 - mae: 0.1434 - val_loss: 0.0235 - val_mae: 0.0832 - learning_rate: 3.1250e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0580 - mae: 0.1434 - val_loss: 0.0237 - val_mae: 0.0846 - learning_rate: 3.1250e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0579 - mae: 0.1432 - val_loss: 0.0236 - val_mae: 0.0831 - learning_rate: 3.1250e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0579 - mae: 0.1435 - val_loss: 0.0233 - val_mae: 0.0826 - learning_rate: 1.5625e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0578 - mae: 0.1428 - val_loss: 0.0235 - val_mae: 0.0832 - learning_rate: 1.5625e-05\n",
      "Epoch 107/200\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0580 - mae: 0.1433 - val_loss: 0.0236 - val_mae: 0.0837 - learning_rate: 1.5625e-05\n",
      "\n",
      "=== Unseen device: C2M0025120D ===\n",
      "                  target         RMSE         R2  n_test\n",
      "current_fall_time_pulse1 3.693695e-09   0.887966   21584\n",
      "current_fall_time_pulse2 3.799078e-09   0.881022   21584\n",
      "       overshoot_pulse_1 6.720131e+00   0.799971   21584\n",
      "       overshoot_pulse_2 9.576101e+00   0.726335   21584\n",
      "      undershoot_pulse_2 4.147528e+00   0.670095   21584\n",
      "      undershoot_pulse_1 4.403263e+00   0.631580   21584\n",
      "current_rise_time_pulse1 2.492940e-08  -0.308973   21584\n",
      "voltage_rise_time_pulse2 4.551174e-09  -0.810075   21584\n",
      "current_rise_time_pulse2 2.107913e-08  -1.382152   21584\n",
      "voltage_fall_time_pulse2 6.901921e-09  -4.604335   21584\n",
      "voltage_fall_time_pulse1 6.912294e-09  -4.634999   21584\n",
      "voltage_rise_time_pulse1 5.126641e-09  -6.143984   21584\n",
      "   ringing_frequency_MHz 1.684190e+01 -37.471294   21584\n",
      "\n",
      "Overall RMSE: 5.929731408756983\n",
      "Overall R2  : -3.904526289509221\n",
      "\n",
      "✅ Saved predictions: C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\ann_unseen_C2M0025120D_noEmb.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== Simple ANN for unseen MOSFET (common combos + 25% per device + derived + optional embedding) =====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, callbacks, Model\n",
    "\n",
    "# ------------ USER SETTINGS ------------\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\Train_5_MOSFETs.csv\"\n",
    "DEVICE_COL = \"Part_Number\"\n",
    "UNSEEN_DEVICE = \"C2M0025120D\"\n",
    "SAMPLE_FRAC = 0.25\n",
    "RANDOM_STATE = 42\n",
    "USE_EMBEDDING = False   # set True to include Part_Number embedding (an 'UNK' slot is used for unseen)\n",
    "# --------------------------------------\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]\n",
    "RAW_INPUTS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    # timing/fixture columns if present (optional); add if you have them in this file\n",
    "    # \"Tp1\",\"Tp2\",\"Toff\",\"Tstart\",\n",
    "    # \"L1\",\"L2\",\"L3\",\"R1\",\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\n",
    "    \"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\n",
    "    \"Ciss\",\"Coss\",\"Crss\",\n",
    "    \"Rth_JC_typ\",\"Rth_JC_max\"\n",
    "]\n",
    "TARGETS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# ---- load ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert DEVICE_COL in df.columns, f\"'{DEVICE_COL}' not found in columns.\"\n",
    "\n",
    "# keep available columns only\n",
    "RAW_INPUTS = [c for c in RAW_INPUTS if c in df.columns]\n",
    "TARGETS = [c for c in TARGETS if c in df.columns]\n",
    "for c in COMBO_COLS:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing combo column: {c}\")\n",
    "\n",
    "# ---- derived features ----\n",
    "def add_derived_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "    eps = 1e-12\n",
    "    c_stray = 5e-12\n",
    "    # L_loop (if L1/L2/L3 not in this file, only sum Ls4..Ls11)\n",
    "    L_pieces = [c for c in [\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\"L1\",\"L2\",\"L3\"] if c in f.columns]\n",
    "    f[\"L_loop\"] = f[L_pieces].sum(axis=1) if L_pieces else np.nan\n",
    "    # C_eq and resonance\n",
    "    f[\"C_eq_est\"] = (f[\"Coss\"].fillna(0) if \"Coss\" in f.columns else 0) + c_stray\n",
    "    f[\"f_res_est\"] = 1.0/(2.0*np.pi*np.sqrt(np.clip(f[\"L_loop\"]*f[\"C_eq_est\"], eps, None)))\n",
    "    # drive proxies\n",
    "    f[\"gate_drive_strength\"] = 1.0/(f[\"Rg\"].abs() + 1e-3) if \"Rg\" in f.columns else np.nan\n",
    "    f[\"dvdt_proxy\"] = f[\"gate_drive_strength\"]/((f[\"Crss\"].abs() if \"Crss\" in f.columns else 0)+eps)\n",
    "    f[\"miller_ratio\"] = (f[\"Crss\"].abs()/(f[\"Coss\"].abs()+eps)) if (\"Crss\" in f.columns and \"Coss\" in f.columns) else np.nan\n",
    "    return f\n",
    "\n",
    "# ---- make combo key with rounding to avoid float mismatches ----\n",
    "def make_combo_key(frame: pd.DataFrame) -> pd.Series:\n",
    "    rounded = frame[COMBO_COLS].apply(lambda s: np.round(s.astype(float), 6))\n",
    "    return rounded.apply(lambda row: tuple(row.values.tolist()), axis=1)\n",
    "\n",
    "# ---- filter to common combos & sample 25%/device ----\n",
    "devices = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices, f\"{UNSEEN_DEVICE} not in dataset devices {devices}\"\n",
    "train_devices = [d for d in devices if d != UNSEEN_DEVICE]\n",
    "\n",
    "df[\"_combo_key\"] = make_combo_key(df)\n",
    "combos_unseen = set(df.loc[df[DEVICE_COL]==UNSEEN_DEVICE, \"_combo_key\"].dropna().unique())\n",
    "combos_train_sets = [set(df.loc[df[DEVICE_COL]==d, \"_combo_key\"].dropna().unique()) for d in train_devices]\n",
    "common_train = set.intersection(*combos_train_sets) if combos_train_sets else set()\n",
    "common_combos = combos_unseen.intersection(common_train)\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])].copy()\n",
    "\n",
    "# sample 25% per device\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ---- features + derived ----\n",
    "feature_cols = list(RAW_INPUTS)  # start with your raw inputs\n",
    "df_feat = add_derived_features(df_sampled)\n",
    "for c in [\"L_loop\",\"C_eq_est\",\"f_res_est\",\"gate_drive_strength\",\"dvdt_proxy\",\"miller_ratio\"]:\n",
    "    if c in df_feat.columns: feature_cols.append(c)\n",
    "feature_cols = list(dict.fromkeys(feature_cols))  # de-dupe\n",
    "\n",
    "# ---- split train/test ----\n",
    "df_train = df_feat[df_feat[DEVICE_COL] != UNSEEN_DEVICE].copy()\n",
    "df_test  = df_feat[df_feat[DEVICE_COL] == UNSEEN_DEVICE].copy()\n",
    "\n",
    "# optional device embedding\n",
    "if USE_EMBEDDING:\n",
    "    le = LabelEncoder()\n",
    "    known = df_train[DEVICE_COL].astype(str)\n",
    "    le.fit(known.unique().tolist())\n",
    "    # map train to ids\n",
    "    df_train[\"_dev_id\"] = le.transform(df_train[DEVICE_COL].astype(str))\n",
    "    # build UNK for unseen\n",
    "    unk_id = len(le.classes_)  # next index\n",
    "    df_test[\"_dev_id\"] = df_test[DEVICE_COL].astype(str).map(lambda s: le.transform([s])[0] if s in le.classes_ else unk_id)\n",
    "else:\n",
    "    # drop device col from features\n",
    "    feature_cols = [c for c in feature_cols if c != DEVICE_COL]\n",
    "\n",
    "# assemble matrices\n",
    "X_num_train = df_train[feature_cols].copy()\n",
    "X_num_test  = df_test[feature_cols].copy()\n",
    "Y_cols = [c for c in TARGETS if c in df_train.columns]\n",
    "y_train = df_train[Y_cols].copy()\n",
    "y_test  = df_test[Y_cols].copy()\n",
    "\n",
    "# ---- scale inputs & outputs ----\n",
    "Xscaler = StandardScaler()\n",
    "Yscalers = {c: StandardScaler() for c in Y_cols}\n",
    "\n",
    "X_train = Xscaler.fit_transform(X_num_train.fillna(X_num_train.median(numeric_only=True)))\n",
    "X_test  = Xscaler.transform(X_num_test.fillna(X_num_train.median(numeric_only=True)))\n",
    "\n",
    "y_train_scaled = np.zeros_like(y_train.values, dtype=float)\n",
    "y_test_scaled  = np.zeros_like(y_test.values, dtype=float)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_train_scaled[:, i] = Yscalers[c].fit_transform(y_train[[c]].values).ravel()\n",
    "    y_test_scaled[:, i]  = Yscalers[c].transform(y_test[[c]].values).ravel()\n",
    "\n",
    "# ---- build model ----\n",
    "def build_model(n_num_features, n_targets, n_devices=None, emb_dim=4):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    if USE_EMBEDDING:\n",
    "        # two-input model: numeric + device id\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        inp_dev = layers.Input(shape=(), dtype=\"int32\", name=\"dev\")\n",
    "        emb = layers.Embedding(input_dim=n_devices, output_dim=emb_dim, name=\"dev_emb\")(inp_dev)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        x = layers.Concatenate()([inp_num, emb])\n",
    "    else:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        x = inp_num\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    out = layers.Dense(n_targets, activation=\"linear\", name=\"y\")(x)\n",
    "\n",
    "    if USE_EMBEDDING:\n",
    "        model = Model(inputs=[inp_num, inp_dev], outputs=out)\n",
    "    else:\n",
    "        model = Model(inputs=inp_num, outputs=out)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "n_devices = (df_train[\"_dev_id\"].max() + 2) if USE_EMBEDDING else None  # +1 for 0-based, +1 for UNK\n",
    "model = build_model(X_train.shape[1], len(Y_cols), n_devices=n_devices, emb_dim=4)\n",
    "\n",
    "# ---- train ----\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=8, min_lr=1e-5)\n",
    "if USE_EMBEDDING:\n",
    "    hist = model.fit(\n",
    "        {\"num\": X_train, \"dev\": df_train[\"_dev_id\"].values},\n",
    "        y_train_scaled,\n",
    "        validation_split=0.15,\n",
    "        epochs=200,\n",
    "        batch_size=256,\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    hist = model.fit(\n",
    "        X_train, y_train_scaled,\n",
    "        validation_split=0.15,\n",
    "        epochs=200,\n",
    "        batch_size=256,\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# ---- predict ----\n",
    "if USE_EMBEDDING:\n",
    "    y_pred_scaled = model.predict({\"num\": X_test, \"dev\": df_test[\"_dev_id\"].values}, verbose=0)\n",
    "else:\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# inverse-scale predictions\n",
    "y_pred = np.zeros_like(y_pred_scaled)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_pred[:, i] = Yscalers[c].inverse_transform(y_pred_scaled[:, [i]]).ravel()\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=Y_cols, index=y_test.index)\n",
    "\n",
    "# ---- metrics ----\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "def r2(a,b):   return float(r2_score(a, b))\n",
    "\n",
    "per_target = []\n",
    "for c in Y_cols:\n",
    "    per_target.append({\"target\": c, \"RMSE\": rmse(y_test[c], y_pred_df[c]), \"R2\": r2(y_test[c], y_pred_df[c]), \"n_test\": int(y_test[c].size)})\n",
    "metrics_df = pd.DataFrame(per_target).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Unseen device:\", UNSEEN_DEVICE, \"===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nOverall RMSE:\", rmse(y_test.values, y_pred_df.values))\n",
    "print(\"Overall R2  :\", r2(y_test.values, y_pred_df.values))\n",
    "\n",
    "# ---- save predictions ----\n",
    "OUT_PATH = os.path.join(os.path.dirname(CSV_PATH), f\"ann_unseen_{UNSEEN_DEVICE}_{'withEmb' if USE_EMBEDDING else 'noEmb'}.csv\")\n",
    "save_cols = [DEVICE_COL] + COMBO_COLS\n",
    "save_cols = [c for c in save_cols if c in df_test.columns]\n",
    "out = df_test[save_cols].copy()\n",
    "for c in Y_cols:\n",
    "    out[c+\"_true\"] = y_test[c].values\n",
    "    out[c+\"_pred\"] = y_pred_df[c].values\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved predictions: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcbec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_37900\\1208061340.py:114: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 11.0115 - mae: 0.5217 - val_loss: 2.6975 - val_mae: 0.3049 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.1330 - mae: 0.2348 - val_loss: 0.8424 - val_mae: 0.1487 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.6640 - mae: 0.1996 - val_loss: 0.6652 - val_mae: 0.1251 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.4690 - mae: 0.1829 - val_loss: 0.6180 - val_mae: 0.1207 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.3683 - mae: 0.1733 - val_loss: 0.6567 - val_mae: 0.1295 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.3007 - mae: 0.1663 - val_loss: 0.5900 - val_mae: 0.1180 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.2488 - mae: 0.1617 - val_loss: 0.5957 - val_mae: 0.1219 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.2067 - mae: 0.1576 - val_loss: 0.5312 - val_mae: 0.1089 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.1718 - mae: 0.1543 - val_loss: 0.4866 - val_mae: 0.0972 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.1421 - mae: 0.1514 - val_loss: 0.5724 - val_mae: 0.1176 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.1245 - mae: 0.1490 - val_loss: 0.5331 - val_mae: 0.1084 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0951 - mae: 0.1464 - val_loss: 0.5336 - val_mae: 0.1102 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0775 - mae: 0.1445 - val_loss: 0.5238 - val_mae: 0.1087 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.0580 - mae: 0.1426 - val_loss: 0.5111 - val_mae: 0.1062 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0525 - mae: 0.1409 - val_loss: 0.5168 - val_mae: 0.1071 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0368 - mae: 0.1394 - val_loss: 0.5040 - val_mae: 0.1045 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.0221 - mae: 0.1383 - val_loss: 0.5547 - val_mae: 0.1154 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0065 - mae: 0.1364 - val_loss: 0.4990 - val_mae: 0.1038 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9997 - mae: 0.1352 - val_loss: 0.5210 - val_mae: 0.1086 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9702 - mae: 0.1323 - val_loss: 0.4525 - val_mae: 0.0906 - learning_rate: 5.0000e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9536 - mae: 0.1308 - val_loss: 0.4367 - val_mae: 0.0868 - learning_rate: 5.0000e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9498 - mae: 0.1306 - val_loss: 0.4562 - val_mae: 0.0922 - learning_rate: 5.0000e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9420 - mae: 0.1299 - val_loss: 0.4230 - val_mae: 0.0822 - learning_rate: 5.0000e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9408 - mae: 0.1295 - val_loss: 0.4494 - val_mae: 0.0905 - learning_rate: 5.0000e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9398 - mae: 0.1290 - val_loss: 0.4464 - val_mae: 0.0898 - learning_rate: 5.0000e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9298 - mae: 0.1284 - val_loss: 0.4366 - val_mae: 0.0871 - learning_rate: 5.0000e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9283 - mae: 0.1279 - val_loss: 0.4384 - val_mae: 0.0883 - learning_rate: 5.0000e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9234 - mae: 0.1273 - val_loss: 0.4361 - val_mae: 0.0870 - learning_rate: 5.0000e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9162 - mae: 0.1266 - val_loss: 0.4249 - val_mae: 0.0833 - learning_rate: 5.0000e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9167 - mae: 0.1265 - val_loss: 0.4261 - val_mae: 0.0843 - learning_rate: 5.0000e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9153 - mae: 0.1261 - val_loss: 0.4336 - val_mae: 0.0873 - learning_rate: 5.0000e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9054 - mae: 0.1254 - val_loss: 0.4214 - val_mae: 0.0824 - learning_rate: 5.0000e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9049 - mae: 0.1251 - val_loss: 0.4371 - val_mae: 0.0882 - learning_rate: 5.0000e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9074 - mae: 0.1248 - val_loss: 0.4273 - val_mae: 0.0846 - learning_rate: 5.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8967 - mae: 0.1243 - val_loss: 0.4379 - val_mae: 0.0883 - learning_rate: 5.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8929 - mae: 0.1237 - val_loss: 0.4230 - val_mae: 0.0830 - learning_rate: 5.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8935 - mae: 0.1236 - val_loss: 0.4228 - val_mae: 0.0838 - learning_rate: 5.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8928 - mae: 0.1231 - val_loss: 0.4278 - val_mae: 0.0856 - learning_rate: 5.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8869 - mae: 0.1228 - val_loss: 0.4361 - val_mae: 0.0885 - learning_rate: 5.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8851 - mae: 0.1225 - val_loss: 0.4177 - val_mae: 0.0822 - learning_rate: 5.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8806 - mae: 0.1217 - val_loss: 0.4207 - val_mae: 0.0833 - learning_rate: 5.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8772 - mae: 0.1216 - val_loss: 0.4143 - val_mae: 0.0816 - learning_rate: 5.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8741 - mae: 0.1216 - val_loss: 0.4240 - val_mae: 0.0846 - learning_rate: 5.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8688 - mae: 0.1209 - val_loss: 0.4160 - val_mae: 0.0823 - learning_rate: 5.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8670 - mae: 0.1207 - val_loss: 0.4192 - val_mae: 0.0830 - learning_rate: 5.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8692 - mae: 0.1207 - val_loss: 0.4290 - val_mae: 0.0870 - learning_rate: 5.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8701 - mae: 0.1203 - val_loss: 0.4352 - val_mae: 0.0892 - learning_rate: 5.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8599 - mae: 0.1198 - val_loss: 0.4087 - val_mae: 0.0790 - learning_rate: 5.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8629 - mae: 0.1198 - val_loss: 0.4126 - val_mae: 0.0815 - learning_rate: 5.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8538 - mae: 0.1194 - val_loss: 0.4114 - val_mae: 0.0796 - learning_rate: 5.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8548 - mae: 0.1193 - val_loss: 0.4142 - val_mae: 0.0822 - learning_rate: 5.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8566 - mae: 0.1190 - val_loss: 0.4160 - val_mae: 0.0820 - learning_rate: 5.0000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8526 - mae: 0.1189 - val_loss: 0.4080 - val_mae: 0.0788 - learning_rate: 5.0000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8515 - mae: 0.1184 - val_loss: 0.4132 - val_mae: 0.0810 - learning_rate: 5.0000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8518 - mae: 0.1184 - val_loss: 0.4203 - val_mae: 0.0835 - learning_rate: 5.0000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8509 - mae: 0.1183 - val_loss: 0.4253 - val_mae: 0.0856 - learning_rate: 5.0000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8467 - mae: 0.1178 - val_loss: 0.4244 - val_mae: 0.0856 - learning_rate: 5.0000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8472 - mae: 0.1179 - val_loss: 0.4143 - val_mae: 0.0816 - learning_rate: 5.0000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8417 - mae: 0.1173 - val_loss: 0.4077 - val_mae: 0.0791 - learning_rate: 5.0000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8548 - mae: 0.1178 - val_loss: 0.4080 - val_mae: 0.0794 - learning_rate: 5.0000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8420 - mae: 0.1173 - val_loss: 0.4110 - val_mae: 0.0799 - learning_rate: 5.0000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8364 - mae: 0.1169 - val_loss: 0.4110 - val_mae: 0.0807 - learning_rate: 5.0000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8404 - mae: 0.1169 - val_loss: 0.4113 - val_mae: 0.0807 - learning_rate: 5.0000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8388 - mae: 0.1167 - val_loss: 0.4036 - val_mae: 0.0774 - learning_rate: 5.0000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8319 - mae: 0.1166 - val_loss: 0.4104 - val_mae: 0.0806 - learning_rate: 5.0000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8324 - mae: 0.1161 - val_loss: 0.4046 - val_mae: 0.0784 - learning_rate: 5.0000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8374 - mae: 0.1164 - val_loss: 0.4021 - val_mae: 0.0777 - learning_rate: 5.0000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8320 - mae: 0.1163 - val_loss: 0.4080 - val_mae: 0.0785 - learning_rate: 5.0000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8256 - mae: 0.1158 - val_loss: 0.4077 - val_mae: 0.0791 - learning_rate: 5.0000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8309 - mae: 0.1159 - val_loss: 0.4058 - val_mae: 0.0786 - learning_rate: 5.0000e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8285 - mae: 0.1156 - val_loss: 0.4057 - val_mae: 0.0789 - learning_rate: 5.0000e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8314 - mae: 0.1155 - val_loss: 0.4030 - val_mae: 0.0773 - learning_rate: 5.0000e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8288 - mae: 0.1154 - val_loss: 0.4109 - val_mae: 0.0811 - learning_rate: 5.0000e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8302 - mae: 0.1154 - val_loss: 0.4231 - val_mae: 0.0858 - learning_rate: 5.0000e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8273 - mae: 0.1152 - val_loss: 0.4049 - val_mae: 0.0789 - learning_rate: 5.0000e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8309 - mae: 0.1154 - val_loss: 0.4103 - val_mae: 0.0802 - learning_rate: 5.0000e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8248 - mae: 0.1149 - val_loss: 0.4043 - val_mae: 0.0785 - learning_rate: 5.0000e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8104 - mae: 0.1136 - val_loss: 0.3973 - val_mae: 0.0750 - learning_rate: 2.5000e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8030 - mae: 0.1132 - val_loss: 0.3968 - val_mae: 0.0733 - learning_rate: 2.5000e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.8034 - mae: 0.1132 - val_loss: 0.3935 - val_mae: 0.0727 - learning_rate: 2.5000e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8042 - mae: 0.1130 - val_loss: 0.3915 - val_mae: 0.0723 - learning_rate: 2.5000e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8052 - mae: 0.1130 - val_loss: 0.4006 - val_mae: 0.0771 - learning_rate: 2.5000e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7964 - mae: 0.1128 - val_loss: 0.3920 - val_mae: 0.0721 - learning_rate: 2.5000e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7997 - mae: 0.1127 - val_loss: 0.3961 - val_mae: 0.0747 - learning_rate: 2.5000e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7990 - mae: 0.1126 - val_loss: 0.3931 - val_mae: 0.0722 - learning_rate: 2.5000e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8011 - mae: 0.1127 - val_loss: 0.3915 - val_mae: 0.0725 - learning_rate: 2.5000e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7953 - mae: 0.1124 - val_loss: 0.3925 - val_mae: 0.0722 - learning_rate: 2.5000e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7967 - mae: 0.1125 - val_loss: 0.3992 - val_mae: 0.0763 - learning_rate: 2.5000e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7940 - mae: 0.1123 - val_loss: 0.3989 - val_mae: 0.0751 - learning_rate: 2.5000e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7933 - mae: 0.1122 - val_loss: 0.3920 - val_mae: 0.0727 - learning_rate: 2.5000e-04\n",
      "Epoch 91/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7936 - mae: 0.1122 - val_loss: 0.3925 - val_mae: 0.0727 - learning_rate: 2.5000e-04\n",
      "Epoch 92/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7898 - mae: 0.1116 - val_loss: 0.3862 - val_mae: 0.0696 - learning_rate: 1.2500e-04\n",
      "Epoch 93/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7915 - mae: 0.1117 - val_loss: 0.3870 - val_mae: 0.0702 - learning_rate: 1.2500e-04\n",
      "Epoch 94/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7847 - mae: 0.1115 - val_loss: 0.3884 - val_mae: 0.0709 - learning_rate: 1.2500e-04\n",
      "Epoch 95/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7905 - mae: 0.1116 - val_loss: 0.3855 - val_mae: 0.0692 - learning_rate: 1.2500e-04\n",
      "Epoch 96/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7838 - mae: 0.1110 - val_loss: 0.3866 - val_mae: 0.0701 - learning_rate: 1.2500e-04\n",
      "Epoch 97/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7882 - mae: 0.1112 - val_loss: 0.3855 - val_mae: 0.0695 - learning_rate: 1.2500e-04\n",
      "Epoch 98/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7820 - mae: 0.1111 - val_loss: 0.3863 - val_mae: 0.0705 - learning_rate: 1.2500e-04\n",
      "Epoch 99/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7868 - mae: 0.1111 - val_loss: 0.3865 - val_mae: 0.0702 - learning_rate: 1.2500e-04\n",
      "Epoch 100/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7865 - mae: 0.1111 - val_loss: 0.3853 - val_mae: 0.0693 - learning_rate: 1.2500e-04\n",
      "Epoch 101/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7839 - mae: 0.1111 - val_loss: 0.3871 - val_mae: 0.0704 - learning_rate: 1.2500e-04\n",
      "Epoch 102/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7814 - mae: 0.1107 - val_loss: 0.3904 - val_mae: 0.0721 - learning_rate: 1.2500e-04\n",
      "Epoch 103/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7851 - mae: 0.1109 - val_loss: 0.3864 - val_mae: 0.0701 - learning_rate: 1.2500e-04\n",
      "Epoch 104/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7800 - mae: 0.1108 - val_loss: 0.3851 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 105/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7823 - mae: 0.1109 - val_loss: 0.3868 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 106/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7778 - mae: 0.1108 - val_loss: 0.3865 - val_mae: 0.0703 - learning_rate: 1.2500e-04\n",
      "Epoch 107/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7847 - mae: 0.1109 - val_loss: 0.3869 - val_mae: 0.0708 - learning_rate: 1.2500e-04\n",
      "Epoch 108/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7785 - mae: 0.1109 - val_loss: 0.3858 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 109/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7835 - mae: 0.1109 - val_loss: 0.3877 - val_mae: 0.0706 - learning_rate: 1.2500e-04\n",
      "Epoch 110/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7795 - mae: 0.1107 - val_loss: 0.3859 - val_mae: 0.0701 - learning_rate: 1.2500e-04\n",
      "Epoch 111/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7762 - mae: 0.1108 - val_loss: 0.3869 - val_mae: 0.0708 - learning_rate: 1.2500e-04\n",
      "Epoch 112/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7813 - mae: 0.1107 - val_loss: 0.3853 - val_mae: 0.0699 - learning_rate: 1.2500e-04\n",
      "Epoch 113/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7803 - mae: 0.1104 - val_loss: 0.3838 - val_mae: 0.0688 - learning_rate: 1.2500e-04\n",
      "Epoch 114/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7728 - mae: 0.1103 - val_loss: 0.3884 - val_mae: 0.0712 - learning_rate: 1.2500e-04\n",
      "Epoch 115/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7768 - mae: 0.1108 - val_loss: 0.3871 - val_mae: 0.0705 - learning_rate: 1.2500e-04\n",
      "Epoch 116/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7745 - mae: 0.1106 - val_loss: 0.3865 - val_mae: 0.0709 - learning_rate: 1.2500e-04\n",
      "Epoch 117/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7818 - mae: 0.1105 - val_loss: 0.3867 - val_mae: 0.0706 - learning_rate: 1.2500e-04\n",
      "Epoch 118/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7828 - mae: 0.1107 - val_loss: 0.3842 - val_mae: 0.0690 - learning_rate: 1.2500e-04\n",
      "Epoch 119/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7772 - mae: 0.1105 - val_loss: 0.3878 - val_mae: 0.0708 - learning_rate: 1.2500e-04\n",
      "Epoch 120/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7802 - mae: 0.1104 - val_loss: 0.3845 - val_mae: 0.0693 - learning_rate: 1.2500e-04\n",
      "Epoch 121/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7777 - mae: 0.1102 - val_loss: 0.3857 - val_mae: 0.0703 - learning_rate: 1.2500e-04\n",
      "Epoch 122/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7732 - mae: 0.1103 - val_loss: 0.3865 - val_mae: 0.0705 - learning_rate: 1.2500e-04\n",
      "Epoch 123/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7727 - mae: 0.1104 - val_loss: 0.3860 - val_mae: 0.0704 - learning_rate: 1.2500e-04\n",
      "Epoch 124/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7729 - mae: 0.1100 - val_loss: 0.3829 - val_mae: 0.0690 - learning_rate: 6.2500e-05\n",
      "Epoch 125/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7717 - mae: 0.1098 - val_loss: 0.3824 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 126/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7682 - mae: 0.1097 - val_loss: 0.3818 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 127/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7707 - mae: 0.1099 - val_loss: 0.3820 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 128/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7731 - mae: 0.1101 - val_loss: 0.3831 - val_mae: 0.0689 - learning_rate: 6.2500e-05\n",
      "Epoch 129/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7673 - mae: 0.1097 - val_loss: 0.3820 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 130/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7699 - mae: 0.1097 - val_loss: 0.3825 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 131/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7696 - mae: 0.1100 - val_loss: 0.3828 - val_mae: 0.0689 - learning_rate: 6.2500e-05\n",
      "Epoch 132/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7651 - mae: 0.1096 - val_loss: 0.3821 - val_mae: 0.0686 - learning_rate: 6.2500e-05\n",
      "Epoch 133/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7673 - mae: 0.1098 - val_loss: 0.3819 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 134/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7688 - mae: 0.1096 - val_loss: 0.3815 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 135/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7691 - mae: 0.1096 - val_loss: 0.3820 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 136/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7644 - mae: 0.1098 - val_loss: 0.3817 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 137/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7638 - mae: 0.1096 - val_loss: 0.3815 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 138/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7626 - mae: 0.1095 - val_loss: 0.3814 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 139/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7682 - mae: 0.1095 - val_loss: 0.3814 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 140/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7662 - mae: 0.1097 - val_loss: 0.3817 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 141/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7621 - mae: 0.1093 - val_loss: 0.3816 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 142/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7635 - mae: 0.1093 - val_loss: 0.3813 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 143/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7627 - mae: 0.1096 - val_loss: 0.3815 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 144/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7736 - mae: 0.1096 - val_loss: 0.3810 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 145/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7708 - mae: 0.1097 - val_loss: 0.3820 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 146/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7676 - mae: 0.1095 - val_loss: 0.3815 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 147/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7667 - mae: 0.1095 - val_loss: 0.3815 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 148/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7654 - mae: 0.1093 - val_loss: 0.3817 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 149/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7659 - mae: 0.1094 - val_loss: 0.3808 - val_mae: 0.0679 - learning_rate: 6.2500e-05\n",
      "Epoch 150/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7647 - mae: 0.1094 - val_loss: 0.3814 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 151/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7665 - mae: 0.1095 - val_loss: 0.3817 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 152/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7662 - mae: 0.1094 - val_loss: 0.3818 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 153/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.7653 - mae: 0.1096 - val_loss: 0.3814 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 154/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7678 - mae: 0.1094 - val_loss: 0.3815 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 155/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7614 - mae: 0.1095 - val_loss: 0.3811 - val_mae: 0.0680 - learning_rate: 6.2500e-05\n",
      "Epoch 156/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7690 - mae: 0.1095 - val_loss: 0.3815 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 157/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7656 - mae: 0.1095 - val_loss: 0.3814 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 158/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7634 - mae: 0.1091 - val_loss: 0.3811 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 159/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7616 - mae: 0.1092 - val_loss: 0.3810 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 160/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7613 - mae: 0.1092 - val_loss: 0.3796 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 161/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7576 - mae: 0.1090 - val_loss: 0.3800 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 162/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7561 - mae: 0.1090 - val_loss: 0.3802 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 163/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7580 - mae: 0.1089 - val_loss: 0.3797 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 164/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7651 - mae: 0.1091 - val_loss: 0.3799 - val_mae: 0.0677 - learning_rate: 3.1250e-05\n",
      "Epoch 165/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7564 - mae: 0.1090 - val_loss: 0.3799 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 166/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7586 - mae: 0.1090 - val_loss: 0.3799 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 167/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7540 - mae: 0.1092 - val_loss: 0.3798 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 168/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7622 - mae: 0.1088 - val_loss: 0.3801 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 169/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7517 - mae: 0.1090 - val_loss: 0.3801 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 170/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7568 - mae: 0.1088 - val_loss: 0.3799 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 171/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7618 - mae: 0.1091 - val_loss: 0.3799 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 172/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7601 - mae: 0.1090 - val_loss: 0.3798 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 173/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7580 - mae: 0.1090 - val_loss: 0.3800 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 174/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7632 - mae: 0.1090 - val_loss: 0.3800 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 175/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7619 - mae: 0.1089 - val_loss: 0.3798 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 176/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7570 - mae: 0.1088 - val_loss: 0.3799 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 177/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7576 - mae: 0.1088 - val_loss: 0.3801 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 178/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7629 - mae: 0.1090 - val_loss: 0.3800 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 179/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7610 - mae: 0.1090 - val_loss: 0.3800 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 180/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7553 - mae: 0.1089 - val_loss: 0.3797 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 181/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7633 - mae: 0.1092 - val_loss: 0.3798 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 182/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7577 - mae: 0.1088 - val_loss: 0.3799 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 183/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7573 - mae: 0.1088 - val_loss: 0.3798 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 184/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7541 - mae: 0.1089 - val_loss: 0.3800 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 185/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7591 - mae: 0.1087 - val_loss: 0.3800 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "\n",
      "=== Unseen device: C2M0025120D ===\n",
      "                  target         RMSE         R2  n_test\n",
      "current_fall_time_pulse2 3.156590e-09   0.917862   21584\n",
      "current_fall_time_pulse1 3.368960e-09   0.906799   21584\n",
      "      undershoot_pulse_2 4.081535e+00   0.680510   21584\n",
      "      undershoot_pulse_1 4.209714e+00   0.663257   21584\n",
      "       overshoot_pulse_2 1.108331e+01   0.633410   21584\n",
      "       overshoot_pulse_1 9.242182e+00   0.621657   21584\n",
      "current_rise_time_pulse1 2.412501e-08  -0.225863   21584\n",
      "voltage_rise_time_pulse2 3.959462e-09  -0.370005   21584\n",
      "voltage_rise_time_pulse1 3.587885e-09  -2.499062   21584\n",
      "voltage_fall_time_pulse2 5.489100e-09  -2.544756   21584\n",
      "voltage_fall_time_pulse1 5.496734e-09  -2.563354   21584\n",
      "current_rise_time_pulse2 2.707032e-08  -2.928719   21584\n",
      "   ringing_frequency_MHz 1.476605e+01 -28.572152   21584\n",
      "\n",
      "Overall RMSE: 5.952856005263054\n",
      "Overall R2  : -2.7138781016450766\n",
      "\n",
      "✅ Saved predictions: C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\ann_unseen_C2M0025120D_noEmb_log1_wloss.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== ANN for unseen MOSFET (common combos + 25% per device + rich derived + optional embedding + log targets + weighted loss) =====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, callbacks, Model\n",
    "\n",
    "# ------------ USER SETTINGS ------------\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\Train_5_MOSFETs.csv\"\n",
    "DEVICE_COL = \"Part_Number\"\n",
    "UNSEEN_DEVICE = \"C2M0025120D\"\n",
    "SAMPLE_FRAC = 0.25\n",
    "RANDOM_STATE = 42\n",
    "USE_EMBEDDING = False     # True -> include device embedding with UNK for unseen\n",
    "APPLY_LOG1P = True        # log1p transform for skewed time targets + ringing\n",
    "# Per-target weights (emphasize the hard ones)\n",
    "TARGET_WEIGHTS = {\n",
    "    \"voltage_rise_time_pulse1\": 2.0,\n",
    "    \"voltage_rise_time_pulse2\": 2.0,\n",
    "    \"voltage_fall_time_pulse1\": 2.0,\n",
    "    \"voltage_fall_time_pulse2\": 2.0,\n",
    "    \"current_rise_time_pulse1\": 1.5,\n",
    "    \"current_rise_time_pulse2\": 1.5,\n",
    "    \"current_fall_time_pulse1\": 1.0,\n",
    "    \"current_fall_time_pulse2\": 1.0,\n",
    "    \"overshoot_pulse_1\": 1.0,\n",
    "    \"overshoot_pulse_2\": 1.0,\n",
    "    \"undershoot_pulse_1\": 1.0,\n",
    "    \"undershoot_pulse_2\": 1.0,\n",
    "    \"ringing_frequency_MHz\": 2.5\n",
    "}\n",
    "# --------------------------------------\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]\n",
    "\n",
    "RAW_INPUTS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    # include if present\n",
    "    \"Tp1\",\"Tp2\",\"Toff\",\"Tstart\",\"L1\",\"L2\",\"L3\",\"R1\",\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\n",
    "    \"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\n",
    "    \"Ciss\",\"Coss\",\"Crss\",\n",
    "    \"Rth_JC_typ\",\"Rth_JC_max\",\"Tj_max\",\n",
    "    DEVICE_COL\n",
    "]\n",
    "\n",
    "TARGETS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# ---- load ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert DEVICE_COL in df.columns, f\"'{DEVICE_COL}' not found.\"\n",
    "\n",
    "RAW_INPUTS = [c for c in RAW_INPUTS if c in df.columns]\n",
    "TARGETS = [c for c in TARGETS if c in df.columns]\n",
    "for c in COMBO_COLS:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing combo column: {c}\")\n",
    "\n",
    "# ---- derived features ----\n",
    "def add_derived_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "    eps = 1e-18\n",
    "    c_stray = 5e-12\n",
    "    # L_loop\n",
    "    L_pieces = [c for c in [\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\"L1\",\"L2\",\"L3\"] if c in f.columns]\n",
    "    f[\"L_loop\"] = f[L_pieces].sum(axis=1) if L_pieces else np.nan\n",
    "    # C_eq, resonance\n",
    "    f[\"C_eq_est\"] = (f[\"Coss\"] if \"Coss\" in f.columns else 0).fillna(0) + c_stray\n",
    "    f[\"f_res_est\"] = 1.0/(2.0*np.pi*np.sqrt(np.clip(f[\"L_loop\"]*f[\"C_eq_est\"], eps, None)))\n",
    "    # drive proxies\n",
    "    f[\"gate_drive_strength\"] = 1.0/((f[\"Rg\"].abs() if \"Rg\" in f.columns else 0)+1e-3)\n",
    "    f[\"dvdt_proxy\"] = f[\"gate_drive_strength\"]/((f[\"Crss\"].abs() if \"Crss\" in f.columns else 0)+1e-15)\n",
    "    f[\"miller_ratio\"] = (f[\"Crss\"].abs()/(f[\"Coss\"].abs()+1e-15)) if (\"Crss\" in f.columns and \"Coss\" in f.columns) else np.nan\n",
    "    # energy/cap ratios\n",
    "    f[\"E_total\"] = (f.get(\"Eon_typ\", 0)).fillna(0) + (f.get(\"Eoff_typ\", 0)).fillna(0)\n",
    "    f[\"Cgd_ratio\"] = (f[\"Crss\"].abs()/(f[\"Ciss\"].abs()+1e-15)) if (\"Crss\" in f.columns and \"Ciss\" in f.columns) else np.nan\n",
    "    f[\"Qrr_over_Qg\"] = (f[\"Qrr_typ\"].abs()/(f[\"Qg_total\"].abs()+1e-15)) if (\"Qrr_typ\" in f.columns and \"Qg_total\" in f.columns) else np.nan\n",
    "    # damping proxy zeta ~ 0.5*R*sqrt(C/L)\n",
    "    if \"R1\" in f.columns:\n",
    "        f[\"zeta_proxy\"] = 0.5*(f[\"R1\"].abs())*np.sqrt(np.clip(f[\"C_eq_est\"]/(f[\"L_loop\"]+eps), eps, None))\n",
    "    else:\n",
    "        f[\"zeta_proxy\"] = np.nan\n",
    "    return f\n",
    "\n",
    "def make_combo_key(frame: pd.DataFrame) -> pd.Series:\n",
    "    rounded = frame[COMBO_COLS].apply(lambda s: np.round(s.astype(float), 6))\n",
    "    return rounded.apply(lambda row: tuple(row.values.tolist()), axis=1)\n",
    "\n",
    "# ---- filter to common combos & sample 25%/device ----\n",
    "devices = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices, f\"{UNSEEN_DEVICE} not in dataset {devices}\"\n",
    "train_devices = [d for d in devices if d != UNSEEN_DEVICE]\n",
    "\n",
    "df[\"_combo_key\"] = make_combo_key(df)\n",
    "combos_unseen = set(df.loc[df[DEVICE_COL]==UNSEEN_DEVICE, \"_combo_key\"].dropna().unique())\n",
    "combos_train_sets = [set(df.loc[df[DEVICE_COL]==d, \"_combo_key\"].dropna().unique()) for d in train_devices]\n",
    "common_train = set.intersection(*combos_train_sets) if combos_train_sets else set()\n",
    "common_combos = combos_unseen.intersection(common_train)\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])].copy()\n",
    "\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ---- features + derived ----\n",
    "feature_cols = list(RAW_INPUTS)\n",
    "df_feat = add_derived_features(df_sampled)\n",
    "for c in [\"L_loop\",\"C_eq_est\",\"f_res_est\",\"gate_drive_strength\",\"dvdt_proxy\",\"miller_ratio\",\n",
    "          \"E_total\",\"Cgd_ratio\",\"Qrr_over_Qg\",\"zeta_proxy\"]:\n",
    "    if c in df_feat.columns: feature_cols.append(c)\n",
    "feature_cols = list(dict.fromkeys(feature_cols))\n",
    "\n",
    "# ---- split ----\n",
    "df_train = df_feat[df_feat[DEVICE_COL] != UNSEEN_DEVICE].copy()\n",
    "df_test  = df_feat[df_feat[DEVICE_COL] == UNSEEN_DEVICE].copy()\n",
    "\n",
    "# optional embedding\n",
    "if USE_EMBEDDING:\n",
    "    le = LabelEncoder()\n",
    "    known = df_train[DEVICE_COL].astype(str)\n",
    "    le.fit(known.unique().tolist())\n",
    "    df_train[\"_dev_id\"] = le.transform(df_train[DEVICE_COL].astype(str))\n",
    "    unk_id = len(le.classes_)\n",
    "    df_test[\"_dev_id\"] = df_test[DEVICE_COL].astype(str).map(lambda s: le.transform([s])[0] if s in le.classes_ else unk_id)\n",
    "else:\n",
    "    feature_cols = [c for c in feature_cols if c != DEVICE_COL]\n",
    "\n",
    "# assemble matrices\n",
    "X_num_train = df_train[feature_cols].copy()\n",
    "X_num_test  = df_test[feature_cols].copy()\n",
    "Y_cols = [c for c in TARGETS if c in df_train.columns]\n",
    "y_train = df_train[Y_cols].copy()\n",
    "y_test  = df_test[Y_cols].copy()\n",
    "\n",
    "# ---- optional log1p on selected targets ----\n",
    "log_targets = set()\n",
    "if APPLY_LOG1P:\n",
    "    for c in Y_cols:\n",
    "        if (\"rise_time\" in c) or (\"fall_time\" in c) or (\"ringing_frequency\" in c):\n",
    "            log_targets.add(c)\n",
    "def log1p_df(df_, cols):\n",
    "    out = df_.copy()\n",
    "    for c in cols:\n",
    "        out[c] = np.log1p(np.clip(out[c].values, a_min=0, a_max=None))\n",
    "    return out\n",
    "def expm1_df(df_, cols):\n",
    "    out = df_.copy()\n",
    "    for c in cols:\n",
    "        out[c] = np.expm1(out[c].values)\n",
    "    return out\n",
    "\n",
    "y_train_for_scale = log1p_df(y_train, log_targets) if log_targets else y_train.copy()\n",
    "y_test_for_scale  = log1p_df(y_test, log_targets)  if log_targets else y_test.copy()\n",
    "\n",
    "# ---- scale inputs & outputs ----\n",
    "Xscaler = StandardScaler()\n",
    "Yscalers = {c: StandardScaler() for c in Y_cols}\n",
    "\n",
    "X_train = Xscaler.fit_transform(X_num_train.fillna(X_num_train.median(numeric_only=True)))\n",
    "X_test  = Xscaler.transform(X_num_test.fillna(X_num_train.median(numeric_only=True)))\n",
    "\n",
    "y_train_scaled = np.zeros_like(y_train_for_scale.values, dtype=float)\n",
    "y_test_scaled  = np.zeros_like(y_test_for_scale.values, dtype=float)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_train_scaled[:, i] = Yscalers[c].fit_transform(y_train_for_scale[[c]].values).ravel()\n",
    "    y_test_scaled[:, i]  = Yscalers[c].transform(y_test_for_scale[[c]].values).ravel()\n",
    "\n",
    "# ---- weighted loss ----\n",
    "# MSE with per-output weights: loss = mean(sum(w_i * (y_i - yhat_i)^2))\n",
    "weights_vec = np.array([TARGET_WEIGHTS.get(c, 1.0) for c in Y_cols], dtype=\"float32\")\n",
    "weights_tf = tf.constant(weights_vec, dtype=tf.float32)\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    return tf.reduce_mean(tf.reduce_sum(weights_tf * tf.square(err), axis=-1))\n",
    "\n",
    "# ---- build model ----\n",
    "def build_model(n_num_features, n_targets, n_devices=None, emb_dim=4):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    if USE_EMBEDDING:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        inp_dev = layers.Input(shape=(), dtype=\"int32\", name=\"dev\")\n",
    "        emb = layers.Embedding(input_dim=n_devices, output_dim=emb_dim, name=\"dev_emb\")(inp_dev)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        x = layers.Concatenate()([inp_num, emb])\n",
    "    else:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        x = inp_num\n",
    "\n",
    "    x = layers.Dense(192, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(96, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    out = layers.Dense(n_targets, activation=\"linear\", name=\"y\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp_num, inp_dev], outputs=out) if USE_EMBEDDING else Model(inputs=inp_num, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=weighted_mse, metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "n_devices = (df_train[\"_dev_id\"].max() + 2) if USE_EMBEDDING else None\n",
    "model = build_model(X_train.shape[1], len(Y_cols), n_devices=n_devices, emb_dim=4)\n",
    "\n",
    "# ---- train ----\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-5)\n",
    "\n",
    "if USE_EMBEDDING:\n",
    "    hist = model.fit({\"num\": X_train, \"dev\": df_train[\"_dev_id\"].values}, y_train_scaled,\n",
    "                     validation_split=0.15, epochs=300, batch_size=256, callbacks=[es, rlr], verbose=1)\n",
    "else:\n",
    "    hist = model.fit(X_train, y_train_scaled,\n",
    "                     validation_split=0.15, epochs=300, batch_size=256, callbacks=[es, rlr], verbose=1)\n",
    "\n",
    "# ---- predict ----\n",
    "if USE_EMBEDDING:\n",
    "    y_pred_scaled = model.predict({\"num\": X_test, \"dev\": df_test[\"_dev_id\"].values}, verbose=0)\n",
    "else:\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# inverse scale + inverse log\n",
    "y_pred_df = pd.DataFrame(y_pred_scaled, columns=Y_cols, index=y_test.index)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_pred_df[c] = Yscalers[c].inverse_transform(y_pred_df[[c]].values).ravel()\n",
    "if log_targets:\n",
    "    y_pred_df = expm1_df(y_pred_df, log_targets)\n",
    "\n",
    "# ---- metrics ----\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "def r2(a,b):   return float(r2_score(a, b))\n",
    "\n",
    "per_target = []\n",
    "y_true_eval = y_test.copy()\n",
    "if log_targets:\n",
    "    # y_test_for_scale was log+scaled; we want to evaluate in the original space\n",
    "    y_true_eval = y_test.copy()  # already original space\n",
    "\n",
    "for c in Y_cols:\n",
    "    per_target.append({\n",
    "        \"target\": c,\n",
    "        \"RMSE\": rmse(y_true_eval[c], y_pred_df[c]),\n",
    "        \"R2\": r2(y_true_eval[c], y_pred_df[c]),\n",
    "        \"n_test\": int(y_true_eval[c].size)\n",
    "    })\n",
    "metrics_df = pd.DataFrame(per_target).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Unseen device:\", UNSEEN_DEVICE, \"===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nOverall RMSE:\", rmse(y_true_eval.values, y_pred_df.values))\n",
    "print(\"Overall R2  :\", r2(y_true_eval.values, y_pred_df.values))\n",
    "\n",
    "# ---- save predictions ----\n",
    "OUT_PATH = os.path.join(os.path.dirname(CSV_PATH), f\"ann_unseen_{UNSEEN_DEVICE}_{'withEmb' if USE_EMBEDDING else 'noEmb'}_log{int(APPLY_LOG1P)}_wloss.csv\")\n",
    "save_cols = [DEVICE_COL] + COMBO_COLS\n",
    "save_cols = [c for c in save_cols if c in df_test.columns]\n",
    "out = df_test[save_cols].copy()\n",
    "for c in Y_cols:\n",
    "    out[c+\"_true\"] = y_true_eval[c].values\n",
    "    out[c+\"_pred\"] = y_pred_df[c].values\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved predictions: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56eb21fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_37900\\283270763.py:114: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - loss: 8.0405 - mae: 0.4583 - val_loss: 1.7299 - val_mae: 0.2382 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 1.8268 - mae: 0.2232 - val_loss: 0.6465 - val_mae: 0.1267 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.4268 - mae: 0.1918 - val_loss: 0.5974 - val_mae: 0.1207 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.2547 - mae: 0.1777 - val_loss: 0.5659 - val_mae: 0.1159 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.1728 - mae: 0.1690 - val_loss: 0.5484 - val_mae: 0.1138 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.1125 - mae: 0.1630 - val_loss: 0.5063 - val_mae: 0.1034 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0669 - mae: 0.1589 - val_loss: 0.5014 - val_mae: 0.1046 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.0251 - mae: 0.1548 - val_loss: 0.5185 - val_mae: 0.1079 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0009 - mae: 0.1521 - val_loss: 0.5505 - val_mae: 0.1171 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9849 - mae: 0.1493 - val_loss: 0.5194 - val_mae: 0.1095 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9566 - mae: 0.1470 - val_loss: 0.5017 - val_mae: 0.1060 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9316 - mae: 0.1449 - val_loss: 0.5482 - val_mae: 0.1182 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9161 - mae: 0.1426 - val_loss: 0.5116 - val_mae: 0.1097 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9066 - mae: 0.1414 - val_loss: 0.4800 - val_mae: 0.1020 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8857 - mae: 0.1397 - val_loss: 0.4412 - val_mae: 0.0931 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8768 - mae: 0.1385 - val_loss: 0.4821 - val_mae: 0.1022 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8682 - mae: 0.1372 - val_loss: 0.4559 - val_mae: 0.0952 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8580 - mae: 0.1357 - val_loss: 0.4956 - val_mae: 0.1079 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8455 - mae: 0.1347 - val_loss: 0.4736 - val_mae: 0.1011 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8436 - mae: 0.1339 - val_loss: 0.4541 - val_mae: 0.0968 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8309 - mae: 0.1325 - val_loss: 0.4615 - val_mae: 0.0999 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8206 - mae: 0.1317 - val_loss: 0.4658 - val_mae: 0.0998 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.8184 - mae: 0.1309 - val_loss: 0.4933 - val_mae: 0.1070 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8022 - mae: 0.1298 - val_loss: 0.4542 - val_mae: 0.0967 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8004 - mae: 0.1290 - val_loss: 0.4484 - val_mae: 0.0948 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7685 - mae: 0.1260 - val_loss: 0.4088 - val_mae: 0.0846 - learning_rate: 5.0000e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7633 - mae: 0.1249 - val_loss: 0.4136 - val_mae: 0.0859 - learning_rate: 5.0000e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7630 - mae: 0.1247 - val_loss: 0.4097 - val_mae: 0.0849 - learning_rate: 5.0000e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7571 - mae: 0.1242 - val_loss: 0.4045 - val_mae: 0.0822 - learning_rate: 5.0000e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7535 - mae: 0.1239 - val_loss: 0.4048 - val_mae: 0.0828 - learning_rate: 5.0000e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7453 - mae: 0.1235 - val_loss: 0.4073 - val_mae: 0.0840 - learning_rate: 5.0000e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7439 - mae: 0.1229 - val_loss: 0.4088 - val_mae: 0.0849 - learning_rate: 5.0000e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7486 - mae: 0.1229 - val_loss: 0.4172 - val_mae: 0.0879 - learning_rate: 5.0000e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7379 - mae: 0.1221 - val_loss: 0.4091 - val_mae: 0.0853 - learning_rate: 5.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7384 - mae: 0.1218 - val_loss: 0.4173 - val_mae: 0.0878 - learning_rate: 5.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7321 - mae: 0.1212 - val_loss: 0.4097 - val_mae: 0.0857 - learning_rate: 5.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7309 - mae: 0.1209 - val_loss: 0.4031 - val_mae: 0.0830 - learning_rate: 5.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7270 - mae: 0.1205 - val_loss: 0.4062 - val_mae: 0.0843 - learning_rate: 5.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7213 - mae: 0.1202 - val_loss: 0.4207 - val_mae: 0.0894 - learning_rate: 5.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7246 - mae: 0.1199 - val_loss: 0.4222 - val_mae: 0.0899 - learning_rate: 5.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7202 - mae: 0.1200 - val_loss: 0.3979 - val_mae: 0.0814 - learning_rate: 5.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7215 - mae: 0.1197 - val_loss: 0.4177 - val_mae: 0.0884 - learning_rate: 5.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7180 - mae: 0.1193 - val_loss: 0.4123 - val_mae: 0.0865 - learning_rate: 5.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7094 - mae: 0.1189 - val_loss: 0.3991 - val_mae: 0.0820 - learning_rate: 5.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7156 - mae: 0.1187 - val_loss: 0.4040 - val_mae: 0.0839 - learning_rate: 5.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7150 - mae: 0.1186 - val_loss: 0.4098 - val_mae: 0.0857 - learning_rate: 5.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7082 - mae: 0.1183 - val_loss: 0.4292 - val_mae: 0.0929 - learning_rate: 5.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.7077 - mae: 0.1179 - val_loss: 0.4100 - val_mae: 0.0865 - learning_rate: 5.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7065 - mae: 0.1178 - val_loss: 0.3994 - val_mae: 0.0820 - learning_rate: 5.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.7047 - mae: 0.1178 - val_loss: 0.4082 - val_mae: 0.0857 - learning_rate: 5.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.7037 - mae: 0.1173 - val_loss: 0.4061 - val_mae: 0.0848 - learning_rate: 5.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6936 - mae: 0.1162 - val_loss: 0.3783 - val_mae: 0.0732 - learning_rate: 2.5000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6914 - mae: 0.1158 - val_loss: 0.3812 - val_mae: 0.0752 - learning_rate: 2.5000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6869 - mae: 0.1155 - val_loss: 0.3797 - val_mae: 0.0745 - learning_rate: 2.5000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6839 - mae: 0.1153 - val_loss: 0.3905 - val_mae: 0.0786 - learning_rate: 2.5000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6830 - mae: 0.1154 - val_loss: 0.3787 - val_mae: 0.0740 - learning_rate: 2.5000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6876 - mae: 0.1153 - val_loss: 0.3856 - val_mae: 0.0772 - learning_rate: 2.5000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6860 - mae: 0.1150 - val_loss: 0.3754 - val_mae: 0.0726 - learning_rate: 2.5000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6817 - mae: 0.1149 - val_loss: 0.3844 - val_mae: 0.0770 - learning_rate: 2.5000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6800 - mae: 0.1147 - val_loss: 0.3798 - val_mae: 0.0745 - learning_rate: 2.5000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6792 - mae: 0.1149 - val_loss: 0.3764 - val_mae: 0.0727 - learning_rate: 2.5000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6807 - mae: 0.1146 - val_loss: 0.3791 - val_mae: 0.0738 - learning_rate: 2.5000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6780 - mae: 0.1144 - val_loss: 0.3753 - val_mae: 0.0726 - learning_rate: 2.5000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6786 - mae: 0.1143 - val_loss: 0.3786 - val_mae: 0.0741 - learning_rate: 2.5000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6732 - mae: 0.1143 - val_loss: 0.3764 - val_mae: 0.0733 - learning_rate: 2.5000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6759 - mae: 0.1141 - val_loss: 0.3817 - val_mae: 0.0753 - learning_rate: 2.5000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6781 - mae: 0.1141 - val_loss: 0.3778 - val_mae: 0.0743 - learning_rate: 2.5000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6810 - mae: 0.1140 - val_loss: 0.3790 - val_mae: 0.0749 - learning_rate: 2.5000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6713 - mae: 0.1134 - val_loss: 0.3699 - val_mae: 0.0698 - learning_rate: 1.2500e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6714 - mae: 0.1133 - val_loss: 0.3706 - val_mae: 0.0697 - learning_rate: 1.2500e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6702 - mae: 0.1132 - val_loss: 0.3705 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6696 - mae: 0.1130 - val_loss: 0.3713 - val_mae: 0.0704 - learning_rate: 1.2500e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6673 - mae: 0.1131 - val_loss: 0.3695 - val_mae: 0.0696 - learning_rate: 1.2500e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6613 - mae: 0.1128 - val_loss: 0.3700 - val_mae: 0.0695 - learning_rate: 1.2500e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6598 - mae: 0.1126 - val_loss: 0.3701 - val_mae: 0.0697 - learning_rate: 1.2500e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6671 - mae: 0.1129 - val_loss: 0.3705 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6630 - mae: 0.1127 - val_loss: 0.3697 - val_mae: 0.0698 - learning_rate: 1.2500e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6614 - mae: 0.1127 - val_loss: 0.3694 - val_mae: 0.0699 - learning_rate: 1.2500e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6666 - mae: 0.1129 - val_loss: 0.3685 - val_mae: 0.0689 - learning_rate: 1.2500e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6605 - mae: 0.1126 - val_loss: 0.3686 - val_mae: 0.0696 - learning_rate: 1.2500e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6552 - mae: 0.1123 - val_loss: 0.3695 - val_mae: 0.0701 - learning_rate: 1.2500e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6624 - mae: 0.1125 - val_loss: 0.3700 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6577 - mae: 0.1123 - val_loss: 0.3696 - val_mae: 0.0695 - learning_rate: 1.2500e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6611 - mae: 0.1125 - val_loss: 0.3688 - val_mae: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6592 - mae: 0.1124 - val_loss: 0.3692 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6635 - mae: 0.1126 - val_loss: 0.3688 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6593 - mae: 0.1123 - val_loss: 0.3699 - val_mae: 0.0702 - learning_rate: 1.2500e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6615 - mae: 0.1122 - val_loss: 0.3685 - val_mae: 0.0690 - learning_rate: 1.2500e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6562 - mae: 0.1121 - val_loss: 0.3690 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6570 - mae: 0.1120 - val_loss: 0.3678 - val_mae: 0.0688 - learning_rate: 6.2500e-05\n",
      "Epoch 91/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6513 - mae: 0.1117 - val_loss: 0.3664 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 92/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6519 - mae: 0.1117 - val_loss: 0.3663 - val_mae: 0.0686 - learning_rate: 6.2500e-05\n",
      "Epoch 93/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6516 - mae: 0.1115 - val_loss: 0.3663 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 94/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6552 - mae: 0.1117 - val_loss: 0.3664 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 95/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6538 - mae: 0.1117 - val_loss: 0.3664 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 96/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6472 - mae: 0.1115 - val_loss: 0.3664 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 97/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6606 - mae: 0.1117 - val_loss: 0.3662 - val_mae: 0.0680 - learning_rate: 6.2500e-05\n",
      "Epoch 98/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6551 - mae: 0.1116 - val_loss: 0.3666 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 99/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6514 - mae: 0.1116 - val_loss: 0.3664 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 100/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6509 - mae: 0.1115 - val_loss: 0.3663 - val_mae: 0.0686 - learning_rate: 6.2500e-05\n",
      "Epoch 101/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6475 - mae: 0.1111 - val_loss: 0.3658 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 102/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6485 - mae: 0.1113 - val_loss: 0.3659 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 103/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6515 - mae: 0.1116 - val_loss: 0.3653 - val_mae: 0.0680 - learning_rate: 6.2500e-05\n",
      "Epoch 104/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6512 - mae: 0.1113 - val_loss: 0.3656 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 105/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6496 - mae: 0.1112 - val_loss: 0.3664 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 106/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6506 - mae: 0.1112 - val_loss: 0.3661 - val_mae: 0.0687 - learning_rate: 6.2500e-05\n",
      "Epoch 107/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6505 - mae: 0.1114 - val_loss: 0.3662 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 108/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6519 - mae: 0.1112 - val_loss: 0.3658 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 109/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6478 - mae: 0.1112 - val_loss: 0.3661 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 110/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6523 - mae: 0.1114 - val_loss: 0.3658 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 111/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6508 - mae: 0.1113 - val_loss: 0.3658 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 112/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6494 - mae: 0.1111 - val_loss: 0.3659 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 113/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6459 - mae: 0.1111 - val_loss: 0.3657 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 114/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6462 - mae: 0.1108 - val_loss: 0.3649 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 115/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6478 - mae: 0.1107 - val_loss: 0.3651 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 116/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6473 - mae: 0.1110 - val_loss: 0.3649 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 117/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6505 - mae: 0.1110 - val_loss: 0.3650 - val_mae: 0.0682 - learning_rate: 3.1250e-05\n",
      "Epoch 118/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6454 - mae: 0.1109 - val_loss: 0.3648 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 119/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6491 - mae: 0.1108 - val_loss: 0.3650 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 120/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6474 - mae: 0.1108 - val_loss: 0.3649 - val_mae: 0.0682 - learning_rate: 3.1250e-05\n",
      "Epoch 121/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6428 - mae: 0.1107 - val_loss: 0.3647 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 122/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6465 - mae: 0.1109 - val_loss: 0.3646 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 123/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6418 - mae: 0.1107 - val_loss: 0.3646 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 124/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6449 - mae: 0.1106 - val_loss: 0.3649 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 125/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6488 - mae: 0.1109 - val_loss: 0.3645 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 126/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6437 - mae: 0.1107 - val_loss: 0.3646 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 127/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6446 - mae: 0.1108 - val_loss: 0.3646 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 128/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6482 - mae: 0.1110 - val_loss: 0.3649 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 129/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6441 - mae: 0.1107 - val_loss: 0.3645 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 130/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6435 - mae: 0.1105 - val_loss: 0.3642 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 131/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6469 - mae: 0.1110 - val_loss: 0.3645 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 132/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6474 - mae: 0.1109 - val_loss: 0.3646 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 133/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6454 - mae: 0.1106 - val_loss: 0.3647 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 134/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6483 - mae: 0.1107 - val_loss: 0.3644 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 135/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6415 - mae: 0.1106 - val_loss: 0.3644 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 136/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6447 - mae: 0.1106 - val_loss: 0.3645 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 137/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6391 - mae: 0.1106 - val_loss: 0.3647 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 138/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6487 - mae: 0.1108 - val_loss: 0.3646 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 139/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6433 - mae: 0.1107 - val_loss: 0.3644 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 140/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6449 - mae: 0.1106 - val_loss: 0.3646 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 141/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6481 - mae: 0.1107 - val_loss: 0.3642 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 142/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6475 - mae: 0.1106 - val_loss: 0.3643 - val_mae: 0.0681 - learning_rate: 1.5625e-05\n",
      "Epoch 143/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6397 - mae: 0.1103 - val_loss: 0.3643 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 144/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6410 - mae: 0.1106 - val_loss: 0.3645 - val_mae: 0.0681 - learning_rate: 1.5625e-05\n",
      "Epoch 145/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6414 - mae: 0.1104 - val_loss: 0.3644 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 146/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6430 - mae: 0.1103 - val_loss: 0.3640 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 147/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6397 - mae: 0.1104 - val_loss: 0.3639 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 148/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6461 - mae: 0.1105 - val_loss: 0.3639 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 149/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6397 - mae: 0.1105 - val_loss: 0.3639 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 150/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6418 - mae: 0.1103 - val_loss: 0.3640 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 151/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6427 - mae: 0.1105 - val_loss: 0.3641 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 152/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6396 - mae: 0.1103 - val_loss: 0.3639 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 153/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6441 - mae: 0.1105 - val_loss: 0.3639 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 154/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6415 - mae: 0.1105 - val_loss: 0.3638 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 155/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6442 - mae: 0.1105 - val_loss: 0.3637 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 156/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6434 - mae: 0.1104 - val_loss: 0.3642 - val_mae: 0.0681 - learning_rate: 1.5625e-05\n",
      "Epoch 157/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6401 - mae: 0.1105 - val_loss: 0.3637 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 158/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6404 - mae: 0.1104 - val_loss: 0.3639 - val_mae: 0.0680 - learning_rate: 1.5625e-05\n",
      "Epoch 159/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6415 - mae: 0.1105 - val_loss: 0.3636 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 160/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6429 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 161/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6475 - mae: 0.1106 - val_loss: 0.3638 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 162/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6411 - mae: 0.1105 - val_loss: 0.3639 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 163/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6393 - mae: 0.1102 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 164/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6434 - mae: 0.1103 - val_loss: 0.3639 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 165/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6422 - mae: 0.1105 - val_loss: 0.3638 - val_mae: 0.0679 - learning_rate: 1.5625e-05\n",
      "Epoch 166/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6408 - mae: 0.1104 - val_loss: 0.3636 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 167/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6395 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 168/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6404 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 169/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6407 - mae: 0.1100 - val_loss: 0.3640 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 170/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6394 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 171/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6403 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 172/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6413 - mae: 0.1104 - val_loss: 0.3638 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 173/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6456 - mae: 0.1106 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 174/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6408 - mae: 0.1102 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 175/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6399 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 176/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6431 - mae: 0.1103 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 177/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.6375 - mae: 0.1101 - val_loss: 0.3636 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 178/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6408 - mae: 0.1104 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 179/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6420 - mae: 0.1103 - val_loss: 0.3636 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 180/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6392 - mae: 0.1102 - val_loss: 0.3635 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 181/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6374 - mae: 0.1103 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 182/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6372 - mae: 0.1102 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 183/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6392 - mae: 0.1100 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 184/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6408 - mae: 0.1104 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 185/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.6393 - mae: 0.1102 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 186/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6422 - mae: 0.1103 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 187/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.6403 - mae: 0.1102 - val_loss: 0.3637 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 188/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.6411 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 189/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.6423 - mae: 0.1105 - val_loss: 0.3635 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 190/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6459 - mae: 0.1105 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 191/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6412 - mae: 0.1103 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 192/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6385 - mae: 0.1102 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 193/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6431 - mae: 0.1101 - val_loss: 0.3638 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 194/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6419 - mae: 0.1102 - val_loss: 0.3639 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 195/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6377 - mae: 0.1100 - val_loss: 0.3638 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 196/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6404 - mae: 0.1105 - val_loss: 0.3638 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 197/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6377 - mae: 0.1102 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 198/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6421 - mae: 0.1101 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 199/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6409 - mae: 0.1100 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 200/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6390 - mae: 0.1103 - val_loss: 0.3635 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 201/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6391 - mae: 0.1102 - val_loss: 0.3635 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 202/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6377 - mae: 0.1101 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 203/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6439 - mae: 0.1104 - val_loss: 0.3635 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 204/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6368 - mae: 0.1100 - val_loss: 0.3634 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 205/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6401 - mae: 0.1103 - val_loss: 0.3636 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 206/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6438 - mae: 0.1103 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 207/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6391 - mae: 0.1100 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 208/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6395 - mae: 0.1105 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 209/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6381 - mae: 0.1101 - val_loss: 0.3637 - val_mae: 0.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 210/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6436 - mae: 0.1102 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 211/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6416 - mae: 0.1102 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 212/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6354 - mae: 0.1100 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 213/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6383 - mae: 0.1101 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 214/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6395 - mae: 0.1103 - val_loss: 0.3637 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 215/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6375 - mae: 0.1101 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 216/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6402 - mae: 0.1101 - val_loss: 0.3635 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 217/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6359 - mae: 0.1102 - val_loss: 0.3635 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 218/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6403 - mae: 0.1105 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 219/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6381 - mae: 0.1102 - val_loss: 0.3636 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 220/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6401 - mae: 0.1100 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 221/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6405 - mae: 0.1102 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 222/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6382 - mae: 0.1102 - val_loss: 0.3635 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 223/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6367 - mae: 0.1099 - val_loss: 0.3634 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 224/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6400 - mae: 0.1101 - val_loss: 0.3633 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 225/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6367 - mae: 0.1101 - val_loss: 0.3634 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 226/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6382 - mae: 0.1103 - val_loss: 0.3635 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 227/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.6416 - mae: 0.1102 - val_loss: 0.3634 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 228/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6376 - mae: 0.1102 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 229/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6421 - mae: 0.1103 - val_loss: 0.3632 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 230/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6399 - mae: 0.1100 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 231/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6363 - mae: 0.1101 - val_loss: 0.3633 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 232/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6413 - mae: 0.1102 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 233/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6384 - mae: 0.1100 - val_loss: 0.3632 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 234/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6423 - mae: 0.1102 - val_loss: 0.3632 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 235/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6389 - mae: 0.1102 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 236/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6388 - mae: 0.1100 - val_loss: 0.3635 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 237/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6416 - mae: 0.1102 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 238/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6394 - mae: 0.1101 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 239/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6363 - mae: 0.1100 - val_loss: 0.3633 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 240/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6404 - mae: 0.1100 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 241/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6396 - mae: 0.1100 - val_loss: 0.3634 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 242/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6327 - mae: 0.1101 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 243/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6429 - mae: 0.1101 - val_loss: 0.3635 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 244/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6380 - mae: 0.1101 - val_loss: 0.3634 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 245/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6442 - mae: 0.1102 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 246/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6382 - mae: 0.1100 - val_loss: 0.3636 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 247/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6380 - mae: 0.1100 - val_loss: 0.3632 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 248/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6385 - mae: 0.1100 - val_loss: 0.3634 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 249/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6385 - mae: 0.1099 - val_loss: 0.3635 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 250/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6373 - mae: 0.1099 - val_loss: 0.3633 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 251/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6375 - mae: 0.1101 - val_loss: 0.3632 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 252/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6414 - mae: 0.1103 - val_loss: 0.3633 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 253/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6402 - mae: 0.1099 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 254/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6398 - mae: 0.1099 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 255/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6412 - mae: 0.1099 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 256/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6374 - mae: 0.1099 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 257/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6380 - mae: 0.1103 - val_loss: 0.3630 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 258/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6370 - mae: 0.1097 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 259/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6378 - mae: 0.1100 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 260/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6405 - mae: 0.1102 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 261/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6383 - mae: 0.1100 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 262/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6369 - mae: 0.1100 - val_loss: 0.3632 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 263/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6355 - mae: 0.1096 - val_loss: 0.3629 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 264/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6362 - mae: 0.1101 - val_loss: 0.3631 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 265/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6431 - mae: 0.1102 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 266/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6385 - mae: 0.1100 - val_loss: 0.3631 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 267/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6411 - mae: 0.1102 - val_loss: 0.3631 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 268/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6389 - mae: 0.1099 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 269/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.6374 - mae: 0.1099 - val_loss: 0.3630 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 270/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6359 - mae: 0.1101 - val_loss: 0.3630 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 271/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6402 - mae: 0.1099 - val_loss: 0.3631 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 272/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6355 - mae: 0.1100 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 273/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6411 - mae: 0.1100 - val_loss: 0.3633 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 274/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6385 - mae: 0.1099 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 275/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6385 - mae: 0.1098 - val_loss: 0.3633 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 276/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6365 - mae: 0.1100 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 277/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6357 - mae: 0.1098 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 278/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6355 - mae: 0.1099 - val_loss: 0.3630 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 279/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6321 - mae: 0.1099 - val_loss: 0.3629 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 280/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6358 - mae: 0.1099 - val_loss: 0.3630 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 281/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6313 - mae: 0.1097 - val_loss: 0.3630 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 282/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6378 - mae: 0.1100 - val_loss: 0.3630 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 283/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6364 - mae: 0.1098 - val_loss: 0.3630 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 284/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6353 - mae: 0.1101 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 285/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6383 - mae: 0.1098 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 286/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.6396 - mae: 0.1099 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 287/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.6389 - mae: 0.1100 - val_loss: 0.3629 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 288/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.6376 - mae: 0.1098 - val_loss: 0.3631 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 289/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6388 - mae: 0.1101 - val_loss: 0.3632 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 290/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6407 - mae: 0.1099 - val_loss: 0.3629 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 291/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6332 - mae: 0.1098 - val_loss: 0.3630 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 292/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6396 - mae: 0.1098 - val_loss: 0.3628 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 293/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6368 - mae: 0.1098 - val_loss: 0.3630 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 294/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6337 - mae: 0.1097 - val_loss: 0.3631 - val_mae: 0.0679 - learning_rate: 1.0000e-05\n",
      "Epoch 295/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6346 - mae: 0.1097 - val_loss: 0.3630 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 296/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6408 - mae: 0.1102 - val_loss: 0.3630 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 297/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6321 - mae: 0.1098 - val_loss: 0.3631 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 298/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6353 - mae: 0.1098 - val_loss: 0.3629 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 299/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6387 - mae: 0.1099 - val_loss: 0.3628 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 300/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6397 - mae: 0.1100 - val_loss: 0.3629 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "\n",
      "=== Unseen device: C2M0040120D ===\n",
      "                  target         RMSE        R2  n_test\n",
      "       overshoot_pulse_1 4.730438e+00  0.877641   21584\n",
      "current_fall_time_pulse1 4.157852e-09  0.862841   21584\n",
      "current_fall_time_pulse2 4.145298e-09  0.861774   21584\n",
      "       overshoot_pulse_2 1.115791e+01  0.709937   21584\n",
      "      undershoot_pulse_2 6.746994e+00  0.520432   21584\n",
      "      undershoot_pulse_1 6.965133e+00  0.495825   21584\n",
      "voltage_rise_time_pulse2 3.489839e-09  0.440442   21584\n",
      "current_rise_time_pulse2 1.493844e-08  0.206031   21584\n",
      "voltage_fall_time_pulse1 2.588262e-09 -0.235978   21584\n",
      "voltage_fall_time_pulse2 2.623899e-09 -0.268873   21584\n",
      "voltage_rise_time_pulse1 1.941654e-09 -0.271451   21584\n",
      "current_rise_time_pulse1 1.817600e-08 -0.373161   21584\n",
      "   ringing_frequency_MHz 6.540227e+00 -1.934895   21584\n",
      "\n",
      "Overall RMSE: 4.671399529092997\n",
      "Overall R2  : 0.14542812188488632\n",
      "\n",
      "✅ Saved predictions: C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\ann_unseen_C2M0040120D_noEmb_log1_wloss.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== ANN for unseen MOSFET (common combos + 25% per device + rich derived + optional embedding + log targets + weighted loss) =====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, callbacks, Model\n",
    "\n",
    "# ------------ USER SETTINGS ------------\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\MERGED_ORIGINAL\\Train_5_MOSFETs.csv\"\n",
    "DEVICE_COL = \"Part_Number\"\n",
    "UNSEEN_DEVICE = \"C2M0040120D\"\n",
    "SAMPLE_FRAC = 0.25\n",
    "RANDOM_STATE = 42\n",
    "USE_EMBEDDING = False     # True -> include device embedding with UNK for unseen\n",
    "APPLY_LOG1P = True        # log1p transform for skewed time targets + ringing\n",
    "# Per-target weights (emphasize the hard ones)\n",
    "TARGET_WEIGHTS = {\n",
    "    \"voltage_rise_time_pulse1\": 2.0,\n",
    "    \"voltage_rise_time_pulse2\": 2.0,\n",
    "    \"voltage_fall_time_pulse1\": 2.0,\n",
    "    \"voltage_fall_time_pulse2\": 2.0,\n",
    "    \"current_rise_time_pulse1\": 1.5,\n",
    "    \"current_rise_time_pulse2\": 1.5,\n",
    "    \"current_fall_time_pulse1\": 1.0,\n",
    "    \"current_fall_time_pulse2\": 1.0,\n",
    "    \"overshoot_pulse_1\": 1.0,\n",
    "    \"overshoot_pulse_2\": 1.0,\n",
    "    \"undershoot_pulse_1\": 1.0,\n",
    "    \"undershoot_pulse_2\": 1.0,\n",
    "    \"ringing_frequency_MHz\": 2.5\n",
    "}\n",
    "# --------------------------------------\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]\n",
    "\n",
    "RAW_INPUTS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    # include if present\n",
    "    \"Tp1\",\"Tp2\",\"Toff\",\"Tstart\",\"L1\",\"L2\",\"L3\",\"R1\",\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\n",
    "    \"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\n",
    "    \"Ciss\",\"Coss\",\"Crss\",\n",
    "    \"Rth_JC_typ\",\"Rth_JC_max\",\"Tj_max\",\n",
    "    DEVICE_COL\n",
    "]\n",
    "\n",
    "TARGETS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# ---- load ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert DEVICE_COL in df.columns, f\"'{DEVICE_COL}' not found.\"\n",
    "\n",
    "RAW_INPUTS = [c for c in RAW_INPUTS if c in df.columns]\n",
    "TARGETS = [c for c in TARGETS if c in df.columns]\n",
    "for c in COMBO_COLS:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing combo column: {c}\")\n",
    "\n",
    "# ---- derived features ----\n",
    "def add_derived_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "    eps = 1e-18\n",
    "    c_stray = 5e-12\n",
    "    # L_loop\n",
    "    L_pieces = [c for c in [\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\"L1\",\"L2\",\"L3\"] if c in f.columns]\n",
    "    f[\"L_loop\"] = f[L_pieces].sum(axis=1) if L_pieces else np.nan\n",
    "    # C_eq, resonance\n",
    "    f[\"C_eq_est\"] = (f[\"Coss\"] if \"Coss\" in f.columns else 0).fillna(0) + c_stray\n",
    "    f[\"f_res_est\"] = 1.0/(2.0*np.pi*np.sqrt(np.clip(f[\"L_loop\"]*f[\"C_eq_est\"], eps, None)))\n",
    "    # drive proxies\n",
    "    f[\"gate_drive_strength\"] = 1.0/((f[\"Rg\"].abs() if \"Rg\" in f.columns else 0)+1e-3)\n",
    "    f[\"dvdt_proxy\"] = f[\"gate_drive_strength\"]/((f[\"Crss\"].abs() if \"Crss\" in f.columns else 0)+1e-15)\n",
    "    f[\"miller_ratio\"] = (f[\"Crss\"].abs()/(f[\"Coss\"].abs()+1e-15)) if (\"Crss\" in f.columns and \"Coss\" in f.columns) else np.nan\n",
    "    # energy/cap ratios\n",
    "    f[\"E_total\"] = (f.get(\"Eon_typ\", 0)).fillna(0) + (f.get(\"Eoff_typ\", 0)).fillna(0)\n",
    "    f[\"Cgd_ratio\"] = (f[\"Crss\"].abs()/(f[\"Ciss\"].abs()+1e-15)) if (\"Crss\" in f.columns and \"Ciss\" in f.columns) else np.nan\n",
    "    f[\"Qrr_over_Qg\"] = (f[\"Qrr_typ\"].abs()/(f[\"Qg_total\"].abs()+1e-15)) if (\"Qrr_typ\" in f.columns and \"Qg_total\" in f.columns) else np.nan\n",
    "    # damping proxy zeta ~ 0.5*R*sqrt(C/L)\n",
    "    if \"R1\" in f.columns:\n",
    "        f[\"zeta_proxy\"] = 0.5*(f[\"R1\"].abs())*np.sqrt(np.clip(f[\"C_eq_est\"]/(f[\"L_loop\"]+eps), eps, None))\n",
    "    else:\n",
    "        f[\"zeta_proxy\"] = np.nan\n",
    "    return f\n",
    "\n",
    "def make_combo_key(frame: pd.DataFrame) -> pd.Series:\n",
    "    rounded = frame[COMBO_COLS].apply(lambda s: np.round(s.astype(float), 6))\n",
    "    return rounded.apply(lambda row: tuple(row.values.tolist()), axis=1)\n",
    "\n",
    "# ---- filter to common combos & sample 25%/device ----\n",
    "devices = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices, f\"{UNSEEN_DEVICE} not in dataset {devices}\"\n",
    "train_devices = [d for d in devices if d != UNSEEN_DEVICE]\n",
    "\n",
    "df[\"_combo_key\"] = make_combo_key(df)\n",
    "combos_unseen = set(df.loc[df[DEVICE_COL]==UNSEEN_DEVICE, \"_combo_key\"].dropna().unique())\n",
    "combos_train_sets = [set(df.loc[df[DEVICE_COL]==d, \"_combo_key\"].dropna().unique()) for d in train_devices]\n",
    "common_train = set.intersection(*combos_train_sets) if combos_train_sets else set()\n",
    "common_combos = combos_unseen.intersection(common_train)\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])].copy()\n",
    "\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ---- features + derived ----\n",
    "feature_cols = list(RAW_INPUTS)\n",
    "df_feat = add_derived_features(df_sampled)\n",
    "for c in [\"L_loop\",\"C_eq_est\",\"f_res_est\",\"gate_drive_strength\",\"dvdt_proxy\",\"miller_ratio\",\n",
    "          \"E_total\",\"Cgd_ratio\",\"Qrr_over_Qg\",\"zeta_proxy\"]:\n",
    "    if c in df_feat.columns: feature_cols.append(c)\n",
    "feature_cols = list(dict.fromkeys(feature_cols))\n",
    "\n",
    "# ---- split ----\n",
    "df_train = df_feat[df_feat[DEVICE_COL] != UNSEEN_DEVICE].copy()\n",
    "df_test  = df_feat[df_feat[DEVICE_COL] == UNSEEN_DEVICE].copy()\n",
    "\n",
    "# optional embedding\n",
    "if USE_EMBEDDING:\n",
    "    le = LabelEncoder()\n",
    "    known = df_train[DEVICE_COL].astype(str)\n",
    "    le.fit(known.unique().tolist())\n",
    "    df_train[\"_dev_id\"] = le.transform(df_train[DEVICE_COL].astype(str))\n",
    "    unk_id = len(le.classes_)\n",
    "    df_test[\"_dev_id\"] = df_test[DEVICE_COL].astype(str).map(lambda s: le.transform([s])[0] if s in le.classes_ else unk_id)\n",
    "else:\n",
    "    feature_cols = [c for c in feature_cols if c != DEVICE_COL]\n",
    "\n",
    "# assemble matrices\n",
    "X_num_train = df_train[feature_cols].copy()\n",
    "X_num_test  = df_test[feature_cols].copy()\n",
    "Y_cols = [c for c in TARGETS if c in df_train.columns]\n",
    "y_train = df_train[Y_cols].copy()\n",
    "y_test  = df_test[Y_cols].copy()\n",
    "\n",
    "# ---- optional log1p on selected targets ----\n",
    "log_targets = set()\n",
    "if APPLY_LOG1P:\n",
    "    for c in Y_cols:\n",
    "        if (\"rise_time\" in c) or (\"fall_time\" in c) or (\"ringing_frequency\" in c):\n",
    "            log_targets.add(c)\n",
    "def log1p_df(df_, cols):\n",
    "    out = df_.copy()\n",
    "    for c in cols:\n",
    "        out[c] = np.log1p(np.clip(out[c].values, a_min=0, a_max=None))\n",
    "    return out\n",
    "def expm1_df(df_, cols):\n",
    "    out = df_.copy()\n",
    "    for c in cols:\n",
    "        out[c] = np.expm1(out[c].values)\n",
    "    return out\n",
    "\n",
    "y_train_for_scale = log1p_df(y_train, log_targets) if log_targets else y_train.copy()\n",
    "y_test_for_scale  = log1p_df(y_test, log_targets)  if log_targets else y_test.copy()\n",
    "\n",
    "# ---- scale inputs & outputs ----\n",
    "Xscaler = StandardScaler()\n",
    "Yscalers = {c: StandardScaler() for c in Y_cols}\n",
    "\n",
    "X_train = Xscaler.fit_transform(X_num_train.fillna(X_num_train.median(numeric_only=True)))\n",
    "X_test  = Xscaler.transform(X_num_test.fillna(X_num_train.median(numeric_only=True)))\n",
    "\n",
    "y_train_scaled = np.zeros_like(y_train_for_scale.values, dtype=float)\n",
    "y_test_scaled  = np.zeros_like(y_test_for_scale.values, dtype=float)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_train_scaled[:, i] = Yscalers[c].fit_transform(y_train_for_scale[[c]].values).ravel()\n",
    "    y_test_scaled[:, i]  = Yscalers[c].transform(y_test_for_scale[[c]].values).ravel()\n",
    "\n",
    "# ---- weighted loss ----\n",
    "# MSE with per-output weights: loss = mean(sum(w_i * (y_i - yhat_i)^2))\n",
    "weights_vec = np.array([TARGET_WEIGHTS.get(c, 1.0) for c in Y_cols], dtype=\"float32\")\n",
    "weights_tf = tf.constant(weights_vec, dtype=tf.float32)\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    return tf.reduce_mean(tf.reduce_sum(weights_tf * tf.square(err), axis=-1))\n",
    "\n",
    "# ---- build model ----\n",
    "def build_model(n_num_features, n_targets, n_devices=None, emb_dim=4):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    if USE_EMBEDDING:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        inp_dev = layers.Input(shape=(), dtype=\"int32\", name=\"dev\")\n",
    "        emb = layers.Embedding(input_dim=n_devices, output_dim=emb_dim, name=\"dev_emb\")(inp_dev)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        x = layers.Concatenate()([inp_num, emb])\n",
    "    else:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        x = inp_num\n",
    "\n",
    "    x = layers.Dense(192, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(96, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    out = layers.Dense(n_targets, activation=\"linear\", name=\"y\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp_num, inp_dev], outputs=out) if USE_EMBEDDING else Model(inputs=inp_num, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=weighted_mse, metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "n_devices = (df_train[\"_dev_id\"].max() + 2) if USE_EMBEDDING else None\n",
    "model = build_model(X_train.shape[1], len(Y_cols), n_devices=n_devices, emb_dim=4)\n",
    "\n",
    "# ---- train ----\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-5)\n",
    "\n",
    "if USE_EMBEDDING:\n",
    "    hist = model.fit({\"num\": X_train, \"dev\": df_train[\"_dev_id\"].values}, y_train_scaled,\n",
    "                     validation_split=0.15, epochs=300, batch_size=256, callbacks=[es, rlr], verbose=1)\n",
    "else:\n",
    "    hist = model.fit(X_train, y_train_scaled,\n",
    "                     validation_split=0.15, epochs=300, batch_size=256, callbacks=[es, rlr], verbose=1)\n",
    "\n",
    "# ---- predict ----\n",
    "if USE_EMBEDDING:\n",
    "    y_pred_scaled = model.predict({\"num\": X_test, \"dev\": df_test[\"_dev_id\"].values}, verbose=0)\n",
    "else:\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# inverse scale + inverse log\n",
    "y_pred_df = pd.DataFrame(y_pred_scaled, columns=Y_cols, index=y_test.index)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_pred_df[c] = Yscalers[c].inverse_transform(y_pred_df[[c]].values).ravel()\n",
    "if log_targets:\n",
    "    y_pred_df = expm1_df(y_pred_df, log_targets)\n",
    "\n",
    "# ---- metrics ----\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "def r2(a,b):   return float(r2_score(a, b))\n",
    "\n",
    "per_target = []\n",
    "y_true_eval = y_test.copy()\n",
    "if log_targets:\n",
    "    # y_test_for_scale was log+scaled; we want to evaluate in the original space\n",
    "    y_true_eval = y_test.copy()  # already original space\n",
    "\n",
    "for c in Y_cols:\n",
    "    per_target.append({\n",
    "        \"target\": c,\n",
    "        \"RMSE\": rmse(y_true_eval[c], y_pred_df[c]),\n",
    "        \"R2\": r2(y_true_eval[c], y_pred_df[c]),\n",
    "        \"n_test\": int(y_true_eval[c].size)\n",
    "    })\n",
    "metrics_df = pd.DataFrame(per_target).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Unseen device:\", UNSEEN_DEVICE, \"===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nOverall RMSE:\", rmse(y_true_eval.values, y_pred_df.values))\n",
    "print(\"Overall R2  :\", r2(y_true_eval.values, y_pred_df.values))\n",
    "\n",
    "# ---- save predictions ----\n",
    "OUT_PATH = os.path.join(os.path.dirname(CSV_PATH), f\"ann_unseen_{UNSEEN_DEVICE}_{'withEmb' if USE_EMBEDDING else 'noEmb'}_log{int(APPLY_LOG1P)}_wloss.csv\")\n",
    "save_cols = [DEVICE_COL] + COMBO_COLS\n",
    "save_cols = [c for c in save_cols if c in df_test.columns]\n",
    "out = df_test[save_cols].copy()\n",
    "for c in Y_cols:\n",
    "    out[c+\"_true\"] = y_true_eval[c].values\n",
    "    out[c+\"_pred\"] = y_pred_df[c].values\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved predictions: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a522138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_25060\\1552104927.py:114: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 7.8233 - mae: 0.4598 - val_loss: 2.0370 - val_mae: 0.2715 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8095 - mae: 0.2237 - val_loss: 0.7437 - val_mae: 0.1405 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4149 - mae: 0.1929 - val_loss: 0.5781 - val_mae: 0.1147 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2480 - mae: 0.1781 - val_loss: 0.5683 - val_mae: 0.1123 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1541 - mae: 0.1684 - val_loss: 0.5322 - val_mae: 0.1064 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0862 - mae: 0.1619 - val_loss: 0.5291 - val_mae: 0.1067 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.0371 - mae: 0.1572 - val_loss: 0.5279 - val_mae: 0.1084 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9996 - mae: 0.1528 - val_loss: 0.5142 - val_mae: 0.1055 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9849 - mae: 0.1502 - val_loss: 0.4967 - val_mae: 0.1017 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9512 - mae: 0.1468 - val_loss: 0.4803 - val_mae: 0.0988 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9159 - mae: 0.1440 - val_loss: 0.4927 - val_mae: 0.1015 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9055 - mae: 0.1421 - val_loss: 0.4945 - val_mae: 0.1022 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8969 - mae: 0.1407 - val_loss: 0.5249 - val_mae: 0.1107 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8789 - mae: 0.1390 - val_loss: 0.5483 - val_mae: 0.1178 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8641 - mae: 0.1371 - val_loss: 0.4518 - val_mae: 0.0927 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8535 - mae: 0.1357 - val_loss: 0.4911 - val_mae: 0.1025 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8431 - mae: 0.1344 - val_loss: 0.4609 - val_mae: 0.0954 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8330 - mae: 0.1333 - val_loss: 0.4620 - val_mae: 0.0962 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.8242 - mae: 0.1319 - val_loss: 0.4812 - val_mae: 0.1022 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.8182 - mae: 0.1313 - val_loss: 0.4658 - val_mae: 0.0956 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8084 - mae: 0.1301 - val_loss: 0.4567 - val_mae: 0.0957 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7952 - mae: 0.1293 - val_loss: 0.4557 - val_mae: 0.0950 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7954 - mae: 0.1281 - val_loss: 0.4315 - val_mae: 0.0883 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7884 - mae: 0.1272 - val_loss: 0.4314 - val_mae: 0.0889 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7843 - mae: 0.1265 - val_loss: 0.4816 - val_mae: 0.1037 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7797 - mae: 0.1259 - val_loss: 0.4918 - val_mae: 0.1064 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7652 - mae: 0.1249 - val_loss: 0.4646 - val_mae: 0.0977 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7599 - mae: 0.1243 - val_loss: 0.4585 - val_mae: 0.0968 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7563 - mae: 0.1236 - val_loss: 0.4524 - val_mae: 0.0981 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7525 - mae: 0.1229 - val_loss: 0.4643 - val_mae: 0.0997 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.7547 - mae: 0.1224 - val_loss: 0.4519 - val_mae: 0.0964 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7424 - mae: 0.1214 - val_loss: 0.4714 - val_mae: 0.1011 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.7428 - mae: 0.1215 - val_loss: 0.4370 - val_mae: 0.0921 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7226 - mae: 0.1187 - val_loss: 0.3999 - val_mae: 0.0801 - learning_rate: 5.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7195 - mae: 0.1183 - val_loss: 0.3881 - val_mae: 0.0760 - learning_rate: 5.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7083 - mae: 0.1177 - val_loss: 0.4027 - val_mae: 0.0828 - learning_rate: 5.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7077 - mae: 0.1176 - val_loss: 0.4005 - val_mae: 0.0812 - learning_rate: 5.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7133 - mae: 0.1174 - val_loss: 0.3912 - val_mae: 0.0782 - learning_rate: 5.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7068 - mae: 0.1172 - val_loss: 0.3943 - val_mae: 0.0787 - learning_rate: 5.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.7052 - mae: 0.1169 - val_loss: 0.3947 - val_mae: 0.0793 - learning_rate: 5.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7021 - mae: 0.1165 - val_loss: 0.3902 - val_mae: 0.0780 - learning_rate: 5.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.6997 - mae: 0.1165 - val_loss: 0.4065 - val_mae: 0.0852 - learning_rate: 5.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6932 - mae: 0.1159 - val_loss: 0.3942 - val_mae: 0.0798 - learning_rate: 5.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6945 - mae: 0.1159 - val_loss: 0.4002 - val_mae: 0.0822 - learning_rate: 5.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.7012 - mae: 0.1161 - val_loss: 0.3948 - val_mae: 0.0802 - learning_rate: 5.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6902 - mae: 0.1147 - val_loss: 0.3796 - val_mae: 0.0733 - learning_rate: 2.5000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6796 - mae: 0.1140 - val_loss: 0.3817 - val_mae: 0.0747 - learning_rate: 2.5000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6818 - mae: 0.1140 - val_loss: 0.3803 - val_mae: 0.0742 - learning_rate: 2.5000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6747 - mae: 0.1141 - val_loss: 0.3779 - val_mae: 0.0732 - learning_rate: 2.5000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6808 - mae: 0.1139 - val_loss: 0.3794 - val_mae: 0.0744 - learning_rate: 2.5000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6765 - mae: 0.1136 - val_loss: 0.3851 - val_mae: 0.0765 - learning_rate: 2.5000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6735 - mae: 0.1134 - val_loss: 0.3784 - val_mae: 0.0733 - learning_rate: 2.5000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6765 - mae: 0.1133 - val_loss: 0.3808 - val_mae: 0.0744 - learning_rate: 2.5000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6757 - mae: 0.1133 - val_loss: 0.3814 - val_mae: 0.0752 - learning_rate: 2.5000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6709 - mae: 0.1132 - val_loss: 0.3806 - val_mae: 0.0745 - learning_rate: 2.5000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6686 - mae: 0.1129 - val_loss: 0.3819 - val_mae: 0.0760 - learning_rate: 2.5000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6743 - mae: 0.1132 - val_loss: 0.3780 - val_mae: 0.0732 - learning_rate: 2.5000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6722 - mae: 0.1130 - val_loss: 0.3792 - val_mae: 0.0740 - learning_rate: 2.5000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6678 - mae: 0.1128 - val_loss: 0.3763 - val_mae: 0.0725 - learning_rate: 2.5000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6726 - mae: 0.1129 - val_loss: 0.3829 - val_mae: 0.0762 - learning_rate: 2.5000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6682 - mae: 0.1124 - val_loss: 0.3792 - val_mae: 0.0739 - learning_rate: 2.5000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6668 - mae: 0.1124 - val_loss: 0.3809 - val_mae: 0.0756 - learning_rate: 2.5000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6679 - mae: 0.1125 - val_loss: 0.3767 - val_mae: 0.0740 - learning_rate: 2.5000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.6664 - mae: 0.1124 - val_loss: 0.3824 - val_mae: 0.0762 - learning_rate: 2.5000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6610 - mae: 0.1122 - val_loss: 0.3761 - val_mae: 0.0737 - learning_rate: 2.5000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6653 - mae: 0.1124 - val_loss: 0.3794 - val_mae: 0.0751 - learning_rate: 2.5000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6628 - mae: 0.1121 - val_loss: 0.3761 - val_mae: 0.0735 - learning_rate: 2.5000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6618 - mae: 0.1119 - val_loss: 0.3789 - val_mae: 0.0749 - learning_rate: 2.5000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6610 - mae: 0.1120 - val_loss: 0.3760 - val_mae: 0.0734 - learning_rate: 2.5000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6600 - mae: 0.1118 - val_loss: 0.3779 - val_mae: 0.0743 - learning_rate: 2.5000e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6591 - mae: 0.1117 - val_loss: 0.3747 - val_mae: 0.0730 - learning_rate: 2.5000e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6620 - mae: 0.1119 - val_loss: 0.3751 - val_mae: 0.0729 - learning_rate: 2.5000e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6584 - mae: 0.1116 - val_loss: 0.3765 - val_mae: 0.0740 - learning_rate: 2.5000e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6568 - mae: 0.1115 - val_loss: 0.3775 - val_mae: 0.0747 - learning_rate: 2.5000e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6559 - mae: 0.1114 - val_loss: 0.3746 - val_mae: 0.0735 - learning_rate: 2.5000e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6580 - mae: 0.1113 - val_loss: 0.3781 - val_mae: 0.0749 - learning_rate: 2.5000e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6568 - mae: 0.1112 - val_loss: 0.3740 - val_mae: 0.0730 - learning_rate: 2.5000e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6494 - mae: 0.1111 - val_loss: 0.3756 - val_mae: 0.0735 - learning_rate: 2.5000e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6584 - mae: 0.1111 - val_loss: 0.3758 - val_mae: 0.0741 - learning_rate: 2.5000e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.6593 - mae: 0.1112 - val_loss: 0.3760 - val_mae: 0.0743 - learning_rate: 2.5000e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6515 - mae: 0.1111 - val_loss: 0.3750 - val_mae: 0.0737 - learning_rate: 2.5000e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6509 - mae: 0.1109 - val_loss: 0.3739 - val_mae: 0.0726 - learning_rate: 2.5000e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6507 - mae: 0.1108 - val_loss: 0.3770 - val_mae: 0.0744 - learning_rate: 2.5000e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6501 - mae: 0.1106 - val_loss: 0.3748 - val_mae: 0.0740 - learning_rate: 2.5000e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6510 - mae: 0.1110 - val_loss: 0.3748 - val_mae: 0.0738 - learning_rate: 2.5000e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6508 - mae: 0.1107 - val_loss: 0.3739 - val_mae: 0.0730 - learning_rate: 2.5000e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6533 - mae: 0.1108 - val_loss: 0.3732 - val_mae: 0.0724 - learning_rate: 2.5000e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6491 - mae: 0.1105 - val_loss: 0.3719 - val_mae: 0.0720 - learning_rate: 2.5000e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6486 - mae: 0.1105 - val_loss: 0.3753 - val_mae: 0.0735 - learning_rate: 2.5000e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6476 - mae: 0.1105 - val_loss: 0.3742 - val_mae: 0.0733 - learning_rate: 2.5000e-04\n",
      "Epoch 91/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6511 - mae: 0.1105 - val_loss: 0.3746 - val_mae: 0.0734 - learning_rate: 2.5000e-04\n",
      "Epoch 92/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6491 - mae: 0.1102 - val_loss: 0.3733 - val_mae: 0.0728 - learning_rate: 2.5000e-04\n",
      "Epoch 93/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6515 - mae: 0.1103 - val_loss: 0.3738 - val_mae: 0.0737 - learning_rate: 2.5000e-04\n",
      "Epoch 94/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6488 - mae: 0.1104 - val_loss: 0.3724 - val_mae: 0.0722 - learning_rate: 2.5000e-04\n",
      "Epoch 95/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6477 - mae: 0.1101 - val_loss: 0.3741 - val_mae: 0.0735 - learning_rate: 2.5000e-04\n",
      "Epoch 96/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6496 - mae: 0.1102 - val_loss: 0.3735 - val_mae: 0.0728 - learning_rate: 2.5000e-04\n",
      "Epoch 97/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6464 - mae: 0.1101 - val_loss: 0.3753 - val_mae: 0.0742 - learning_rate: 2.5000e-04\n",
      "Epoch 98/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6443 - mae: 0.1098 - val_loss: 0.3737 - val_mae: 0.0737 - learning_rate: 2.5000e-04\n",
      "Epoch 99/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6436 - mae: 0.1093 - val_loss: 0.3690 - val_mae: 0.0704 - learning_rate: 1.2500e-04\n",
      "Epoch 100/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6375 - mae: 0.1091 - val_loss: 0.3672 - val_mae: 0.0693 - learning_rate: 1.2500e-04\n",
      "Epoch 101/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6352 - mae: 0.1089 - val_loss: 0.3683 - val_mae: 0.0697 - learning_rate: 1.2500e-04\n",
      "Epoch 102/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6409 - mae: 0.1089 - val_loss: 0.3678 - val_mae: 0.0701 - learning_rate: 1.2500e-04\n",
      "Epoch 103/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6335 - mae: 0.1090 - val_loss: 0.3671 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 104/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6390 - mae: 0.1089 - val_loss: 0.3674 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 105/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6392 - mae: 0.1091 - val_loss: 0.3672 - val_mae: 0.0698 - learning_rate: 1.2500e-04\n",
      "Epoch 106/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6343 - mae: 0.1089 - val_loss: 0.3684 - val_mae: 0.0706 - learning_rate: 1.2500e-04\n",
      "Epoch 107/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6357 - mae: 0.1088 - val_loss: 0.3674 - val_mae: 0.0697 - learning_rate: 1.2500e-04\n",
      "Epoch 108/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6327 - mae: 0.1086 - val_loss: 0.3659 - val_mae: 0.0690 - learning_rate: 1.2500e-04\n",
      "Epoch 109/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6345 - mae: 0.1088 - val_loss: 0.3666 - val_mae: 0.0695 - learning_rate: 1.2500e-04\n",
      "Epoch 110/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6366 - mae: 0.1089 - val_loss: 0.3687 - val_mae: 0.0709 - learning_rate: 1.2500e-04\n",
      "Epoch 111/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6352 - mae: 0.1088 - val_loss: 0.3657 - val_mae: 0.0688 - learning_rate: 1.2500e-04\n",
      "Epoch 112/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6339 - mae: 0.1087 - val_loss: 0.3672 - val_mae: 0.0698 - learning_rate: 1.2500e-04\n",
      "Epoch 113/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6340 - mae: 0.1086 - val_loss: 0.3671 - val_mae: 0.0698 - learning_rate: 1.2500e-04\n",
      "Epoch 114/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6290 - mae: 0.1085 - val_loss: 0.3667 - val_mae: 0.0696 - learning_rate: 1.2500e-04\n",
      "Epoch 115/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6321 - mae: 0.1086 - val_loss: 0.3663 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 116/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6337 - mae: 0.1086 - val_loss: 0.3670 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 117/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6275 - mae: 0.1083 - val_loss: 0.3665 - val_mae: 0.0697 - learning_rate: 1.2500e-04\n",
      "Epoch 118/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6289 - mae: 0.1085 - val_loss: 0.3660 - val_mae: 0.0695 - learning_rate: 1.2500e-04\n",
      "Epoch 119/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6359 - mae: 0.1085 - val_loss: 0.3657 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 120/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6310 - mae: 0.1083 - val_loss: 0.3654 - val_mae: 0.0692 - learning_rate: 1.2500e-04\n",
      "Epoch 121/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6318 - mae: 0.1084 - val_loss: 0.3657 - val_mae: 0.0693 - learning_rate: 1.2500e-04\n",
      "Epoch 122/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6315 - mae: 0.1084 - val_loss: 0.3662 - val_mae: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 123/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6297 - mae: 0.1081 - val_loss: 0.3662 - val_mae: 0.0696 - learning_rate: 1.2500e-04\n",
      "Epoch 124/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6287 - mae: 0.1081 - val_loss: 0.3671 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 125/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6351 - mae: 0.1082 - val_loss: 0.3659 - val_mae: 0.0693 - learning_rate: 1.2500e-04\n",
      "Epoch 126/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6355 - mae: 0.1084 - val_loss: 0.3673 - val_mae: 0.0698 - learning_rate: 1.2500e-04\n",
      "Epoch 127/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6331 - mae: 0.1084 - val_loss: 0.3656 - val_mae: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 128/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6288 - mae: 0.1084 - val_loss: 0.3660 - val_mae: 0.0694 - learning_rate: 1.2500e-04\n",
      "Epoch 129/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6322 - mae: 0.1081 - val_loss: 0.3672 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 130/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6319 - mae: 0.1082 - val_loss: 0.3652 - val_mae: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 131/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6334 - mae: 0.1080 - val_loss: 0.3658 - val_mae: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 132/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6245 - mae: 0.1079 - val_loss: 0.3674 - val_mae: 0.0699 - learning_rate: 1.2500e-04\n",
      "Epoch 133/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6310 - mae: 0.1078 - val_loss: 0.3672 - val_mae: 0.0704 - learning_rate: 1.2500e-04\n",
      "Epoch 134/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6319 - mae: 0.1081 - val_loss: 0.3666 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 135/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6233 - mae: 0.1076 - val_loss: 0.3664 - val_mae: 0.0692 - learning_rate: 1.2500e-04\n",
      "Epoch 136/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6264 - mae: 0.1078 - val_loss: 0.3653 - val_mae: 0.0692 - learning_rate: 1.2500e-04\n",
      "Epoch 137/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6315 - mae: 0.1080 - val_loss: 0.3646 - val_mae: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 138/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6269 - mae: 0.1078 - val_loss: 0.3660 - val_mae: 0.0692 - learning_rate: 1.2500e-04\n",
      "Epoch 139/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6272 - mae: 0.1077 - val_loss: 0.3662 - val_mae: 0.0700 - learning_rate: 1.2500e-04\n",
      "Epoch 140/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6266 - mae: 0.1077 - val_loss: 0.3649 - val_mae: 0.0689 - learning_rate: 1.2500e-04\n",
      "Epoch 141/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6272 - mae: 0.1078 - val_loss: 0.3646 - val_mae: 0.0689 - learning_rate: 1.2500e-04\n",
      "Epoch 142/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6235 - mae: 0.1078 - val_loss: 0.3650 - val_mae: 0.0691 - learning_rate: 1.2500e-04\n",
      "Epoch 143/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6276 - mae: 0.1077 - val_loss: 0.3644 - val_mae: 0.0689 - learning_rate: 1.2500e-04\n",
      "Epoch 144/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6275 - mae: 0.1076 - val_loss: 0.3665 - val_mae: 0.0704 - learning_rate: 1.2500e-04\n",
      "Epoch 145/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6269 - mae: 0.1078 - val_loss: 0.3660 - val_mae: 0.0695 - learning_rate: 1.2500e-04\n",
      "Epoch 146/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6227 - mae: 0.1076 - val_loss: 0.3644 - val_mae: 0.0687 - learning_rate: 1.2500e-04\n",
      "Epoch 147/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6248 - mae: 0.1075 - val_loss: 0.3643 - val_mae: 0.0690 - learning_rate: 1.2500e-04\n",
      "Epoch 148/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6223 - mae: 0.1074 - val_loss: 0.3642 - val_mae: 0.0687 - learning_rate: 1.2500e-04\n",
      "Epoch 149/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.6238 - mae: 0.1073 - val_loss: 0.3633 - val_mae: 0.0688 - learning_rate: 1.2500e-04\n",
      "Epoch 150/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6274 - mae: 0.1077 - val_loss: 0.3636 - val_mae: 0.0688 - learning_rate: 1.2500e-04\n",
      "Epoch 151/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.6207 - mae: 0.1073 - val_loss: 0.3643 - val_mae: 0.0687 - learning_rate: 1.2500e-04\n",
      "Epoch 152/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6264 - mae: 0.1075 - val_loss: 0.3648 - val_mae: 0.0690 - learning_rate: 1.2500e-04\n",
      "Epoch 153/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6216 - mae: 0.1075 - val_loss: 0.3642 - val_mae: 0.0686 - learning_rate: 1.2500e-04\n",
      "Epoch 154/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6233 - mae: 0.1074 - val_loss: 0.3653 - val_mae: 0.0696 - learning_rate: 1.2500e-04\n",
      "Epoch 155/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6241 - mae: 0.1077 - val_loss: 0.3643 - val_mae: 0.0690 - learning_rate: 1.2500e-04\n",
      "Epoch 156/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6317 - mae: 0.1075 - val_loss: 0.3642 - val_mae: 0.0688 - learning_rate: 1.2500e-04\n",
      "Epoch 157/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.6229 - mae: 0.1075 - val_loss: 0.3633 - val_mae: 0.0682 - learning_rate: 1.2500e-04\n",
      "Epoch 158/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6230 - mae: 0.1075 - val_loss: 0.3663 - val_mae: 0.0697 - learning_rate: 1.2500e-04\n",
      "Epoch 159/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6206 - mae: 0.1073 - val_loss: 0.3641 - val_mae: 0.0687 - learning_rate: 1.2500e-04\n",
      "Epoch 160/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6223 - mae: 0.1070 - val_loss: 0.3631 - val_mae: 0.0688 - learning_rate: 6.2500e-05\n",
      "Epoch 161/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6212 - mae: 0.1069 - val_loss: 0.3635 - val_mae: 0.0687 - learning_rate: 6.2500e-05\n",
      "Epoch 162/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6206 - mae: 0.1068 - val_loss: 0.3625 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 163/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6193 - mae: 0.1066 - val_loss: 0.3629 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 164/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6192 - mae: 0.1068 - val_loss: 0.3625 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 165/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6174 - mae: 0.1067 - val_loss: 0.3629 - val_mae: 0.0680 - learning_rate: 6.2500e-05\n",
      "Epoch 166/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6181 - mae: 0.1068 - val_loss: 0.3623 - val_mae: 0.0679 - learning_rate: 6.2500e-05\n",
      "Epoch 167/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6192 - mae: 0.1070 - val_loss: 0.3632 - val_mae: 0.0686 - learning_rate: 6.2500e-05\n",
      "Epoch 168/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6149 - mae: 0.1066 - val_loss: 0.3627 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 169/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6138 - mae: 0.1067 - val_loss: 0.3630 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 170/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6142 - mae: 0.1065 - val_loss: 0.3626 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 171/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6188 - mae: 0.1068 - val_loss: 0.3627 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 172/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.6145 - mae: 0.1066 - val_loss: 0.3629 - val_mae: 0.0685 - learning_rate: 6.2500e-05\n",
      "Epoch 173/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6165 - mae: 0.1067 - val_loss: 0.3617 - val_mae: 0.0678 - learning_rate: 6.2500e-05\n",
      "Epoch 174/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6158 - mae: 0.1066 - val_loss: 0.3629 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 175/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6198 - mae: 0.1067 - val_loss: 0.3629 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 176/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6179 - mae: 0.1065 - val_loss: 0.3624 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 177/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6138 - mae: 0.1065 - val_loss: 0.3631 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 178/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6171 - mae: 0.1065 - val_loss: 0.3621 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 179/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6191 - mae: 0.1065 - val_loss: 0.3628 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 180/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6175 - mae: 0.1066 - val_loss: 0.3626 - val_mae: 0.0684 - learning_rate: 6.2500e-05\n",
      "Epoch 181/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6167 - mae: 0.1067 - val_loss: 0.3623 - val_mae: 0.0680 - learning_rate: 6.2500e-05\n",
      "Epoch 182/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6163 - mae: 0.1065 - val_loss: 0.3621 - val_mae: 0.0681 - learning_rate: 6.2500e-05\n",
      "Epoch 183/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6150 - mae: 0.1064 - val_loss: 0.3624 - val_mae: 0.0683 - learning_rate: 6.2500e-05\n",
      "Epoch 184/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6152 - mae: 0.1064 - val_loss: 0.3623 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 185/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6131 - mae: 0.1062 - val_loss: 0.3617 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 186/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6137 - mae: 0.1065 - val_loss: 0.3618 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 187/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6161 - mae: 0.1065 - val_loss: 0.3615 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 188/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6137 - mae: 0.1063 - val_loss: 0.3615 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 189/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6152 - mae: 0.1064 - val_loss: 0.3619 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 190/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6152 - mae: 0.1061 - val_loss: 0.3616 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 191/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6128 - mae: 0.1064 - val_loss: 0.3620 - val_mae: 0.0680 - learning_rate: 3.1250e-05\n",
      "Epoch 192/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6137 - mae: 0.1060 - val_loss: 0.3615 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 193/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6138 - mae: 0.1063 - val_loss: 0.3614 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 194/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.6110 - mae: 0.1059 - val_loss: 0.3613 - val_mae: 0.0677 - learning_rate: 3.1250e-05\n",
      "Epoch 195/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6147 - mae: 0.1062 - val_loss: 0.3614 - val_mae: 0.0677 - learning_rate: 3.1250e-05\n",
      "Epoch 196/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6101 - mae: 0.1061 - val_loss: 0.3611 - val_mae: 0.0676 - learning_rate: 3.1250e-05\n",
      "Epoch 197/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6108 - mae: 0.1061 - val_loss: 0.3613 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 198/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6150 - mae: 0.1064 - val_loss: 0.3612 - val_mae: 0.0676 - learning_rate: 3.1250e-05\n",
      "Epoch 199/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6135 - mae: 0.1062 - val_loss: 0.3619 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 200/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6107 - mae: 0.1061 - val_loss: 0.3616 - val_mae: 0.0681 - learning_rate: 3.1250e-05\n",
      "Epoch 201/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6142 - mae: 0.1062 - val_loss: 0.3613 - val_mae: 0.0677 - learning_rate: 3.1250e-05\n",
      "Epoch 202/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6100 - mae: 0.1062 - val_loss: 0.3611 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 203/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6077 - mae: 0.1060 - val_loss: 0.3614 - val_mae: 0.0679 - learning_rate: 3.1250e-05\n",
      "Epoch 204/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6127 - mae: 0.1061 - val_loss: 0.3611 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 205/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6113 - mae: 0.1061 - val_loss: 0.3614 - val_mae: 0.0678 - learning_rate: 3.1250e-05\n",
      "Epoch 206/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6155 - mae: 0.1061 - val_loss: 0.3612 - val_mae: 0.0676 - learning_rate: 3.1250e-05\n",
      "Epoch 207/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6133 - mae: 0.1062 - val_loss: 0.3612 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 208/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6158 - mae: 0.1060 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.5625e-05\n",
      "Epoch 209/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6064 - mae: 0.1059 - val_loss: 0.3609 - val_mae: 0.0676 - learning_rate: 1.5625e-05\n",
      "Epoch 210/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6138 - mae: 0.1061 - val_loss: 0.3611 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 211/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6088 - mae: 0.1061 - val_loss: 0.3610 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 212/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6125 - mae: 0.1062 - val_loss: 0.3607 - val_mae: 0.0677 - learning_rate: 1.5625e-05\n",
      "Epoch 213/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6089 - mae: 0.1059 - val_loss: 0.3608 - val_mae: 0.0677 - learning_rate: 1.5625e-05\n",
      "Epoch 214/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6097 - mae: 0.1058 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.5625e-05\n",
      "Epoch 215/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6064 - mae: 0.1061 - val_loss: 0.3608 - val_mae: 0.0677 - learning_rate: 1.5625e-05\n",
      "Epoch 216/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6140 - mae: 0.1059 - val_loss: 0.3610 - val_mae: 0.0676 - learning_rate: 1.5625e-05\n",
      "Epoch 217/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6090 - mae: 0.1063 - val_loss: 0.3609 - val_mae: 0.0678 - learning_rate: 1.5625e-05\n",
      "Epoch 218/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6084 - mae: 0.1058 - val_loss: 0.3606 - val_mae: 0.0675 - learning_rate: 1.5625e-05\n",
      "Epoch 219/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6147 - mae: 0.1060 - val_loss: 0.3609 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 220/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6105 - mae: 0.1060 - val_loss: 0.3608 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 221/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6121 - mae: 0.1061 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 222/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6075 - mae: 0.1061 - val_loss: 0.3608 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 223/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6124 - mae: 0.1059 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 224/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6114 - mae: 0.1060 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 225/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6115 - mae: 0.1060 - val_loss: 0.3608 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 226/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6098 - mae: 0.1058 - val_loss: 0.3608 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 227/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6112 - mae: 0.1058 - val_loss: 0.3607 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 228/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6122 - mae: 0.1060 - val_loss: 0.3608 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 229/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6106 - mae: 0.1058 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 230/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6076 - mae: 0.1058 - val_loss: 0.3609 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 231/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6091 - mae: 0.1059 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 232/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6079 - mae: 0.1058 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 233/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6106 - mae: 0.1060 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 234/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6099 - mae: 0.1059 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 235/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6127 - mae: 0.1060 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 236/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6089 - mae: 0.1060 - val_loss: 0.3608 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 237/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6096 - mae: 0.1061 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 238/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6091 - mae: 0.1057 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 239/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6115 - mae: 0.1060 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 240/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6080 - mae: 0.1059 - val_loss: 0.3605 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 241/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6109 - mae: 0.1059 - val_loss: 0.3609 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 242/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6094 - mae: 0.1059 - val_loss: 0.3608 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 243/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6116 - mae: 0.1061 - val_loss: 0.3609 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 244/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6094 - mae: 0.1061 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 245/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6093 - mae: 0.1060 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 246/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6083 - mae: 0.1059 - val_loss: 0.3609 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 247/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6092 - mae: 0.1060 - val_loss: 0.3608 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 248/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6130 - mae: 0.1059 - val_loss: 0.3611 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 249/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6076 - mae: 0.1060 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 250/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6096 - mae: 0.1059 - val_loss: 0.3608 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 251/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6080 - mae: 0.1058 - val_loss: 0.3604 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 252/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6094 - mae: 0.1059 - val_loss: 0.3604 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 253/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6088 - mae: 0.1059 - val_loss: 0.3605 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 254/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6080 - mae: 0.1059 - val_loss: 0.3606 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 255/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6111 - mae: 0.1059 - val_loss: 0.3609 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 256/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6082 - mae: 0.1060 - val_loss: 0.3605 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 257/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6099 - mae: 0.1059 - val_loss: 0.3605 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 258/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6110 - mae: 0.1059 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 259/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6060 - mae: 0.1056 - val_loss: 0.3604 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 260/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6107 - mae: 0.1059 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 261/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6099 - mae: 0.1058 - val_loss: 0.3605 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 262/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6061 - mae: 0.1054 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 263/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6077 - mae: 0.1058 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 264/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6107 - mae: 0.1058 - val_loss: 0.3608 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 265/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6088 - mae: 0.1057 - val_loss: 0.3605 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 266/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6122 - mae: 0.1059 - val_loss: 0.3609 - val_mae: 0.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 267/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6061 - mae: 0.1057 - val_loss: 0.3607 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 268/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6068 - mae: 0.1058 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 269/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6108 - mae: 0.1058 - val_loss: 0.3601 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 270/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6076 - mae: 0.1059 - val_loss: 0.3604 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 271/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6114 - mae: 0.1059 - val_loss: 0.3604 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 272/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6076 - mae: 0.1058 - val_loss: 0.3604 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 273/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6078 - mae: 0.1058 - val_loss: 0.3607 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 274/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6082 - mae: 0.1058 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 275/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6117 - mae: 0.1060 - val_loss: 0.3605 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 276/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6082 - mae: 0.1059 - val_loss: 0.3603 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 277/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6098 - mae: 0.1059 - val_loss: 0.3604 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 278/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6083 - mae: 0.1059 - val_loss: 0.3606 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 279/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6085 - mae: 0.1059 - val_loss: 0.3604 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 280/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6092 - mae: 0.1059 - val_loss: 0.3606 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 281/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6059 - mae: 0.1058 - val_loss: 0.3603 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 282/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6080 - mae: 0.1059 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 283/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6068 - mae: 0.1056 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 284/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6070 - mae: 0.1056 - val_loss: 0.3608 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 285/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6114 - mae: 0.1059 - val_loss: 0.3608 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 286/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6096 - mae: 0.1059 - val_loss: 0.3605 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 287/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6068 - mae: 0.1057 - val_loss: 0.3604 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 288/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6086 - mae: 0.1057 - val_loss: 0.3604 - val_mae: 0.0675 - learning_rate: 1.0000e-05\n",
      "Epoch 289/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.6094 - mae: 0.1056 - val_loss: 0.3604 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 290/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6067 - mae: 0.1057 - val_loss: 0.3605 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 291/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.6071 - mae: 0.1057 - val_loss: 0.3602 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "Epoch 292/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6093 - mae: 0.1058 - val_loss: 0.3604 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 293/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6036 - mae: 0.1058 - val_loss: 0.3603 - val_mae: 0.0676 - learning_rate: 1.0000e-05\n",
      "Epoch 294/300\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6060 - mae: 0.1057 - val_loss: 0.3606 - val_mae: 0.0677 - learning_rate: 1.0000e-05\n",
      "\n",
      "=== Unseen device: C2M0040120D ===\n",
      "                  target         RMSE        R2  n_test\n",
      "current_fall_time_pulse1 4.583996e-09  0.833285   21584\n",
      "current_fall_time_pulse2 4.664679e-09  0.824967   21584\n",
      "       overshoot_pulse_2 8.825012e+00  0.818550   21584\n",
      "      undershoot_pulse_2 5.768914e+00  0.649395   21584\n",
      "       overshoot_pulse_1 8.035090e+00  0.646966   21584\n",
      "      undershoot_pulse_1 5.889967e+00  0.639465   21584\n",
      "voltage_rise_time_pulse2 3.492543e-09  0.439575   21584\n",
      "current_rise_time_pulse2 1.608235e-08  0.079779   21584\n",
      "current_rise_time_pulse1 1.816099e-08 -0.370893   21584\n",
      "voltage_fall_time_pulse2 2.920995e-09 -0.572482   21584\n",
      "voltage_fall_time_pulse1 2.926071e-09 -0.579662   21584\n",
      "voltage_rise_time_pulse1 2.295423e-09 -0.776976   21584\n",
      "   ringing_frequency_MHz 6.609391e+00 -1.997298   21584\n",
      "\n",
      "Overall RMSE: 4.421100154325863\n",
      "Overall R2  : 0.04882090012275597\n",
      "\n",
      "✅ Saved predictions: C:\\Users\\pc\\Desktop\\Classical_Models\\ann_unseen_C2M0040120D_noEmb_log1_wloss.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== ANN for unseen MOSFET (common combos + 25% per device + rich derived + optional embedding + log targets + weighted loss) =====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, callbacks, Model\n",
    "\n",
    "# ------------ USER SETTINGS ------------\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\Classical_Models\\Train_5_MOSFETs_engineered.csv\"\n",
    "DEVICE_COL = \"Part_Number\"\n",
    "UNSEEN_DEVICE = \"C2M0040120D\"\n",
    "SAMPLE_FRAC = 0.25\n",
    "RANDOM_STATE = 42\n",
    "USE_EMBEDDING = False     # True -> include device embedding with UNK for unseen\n",
    "APPLY_LOG1P = True        # log1p transform for skewed time targets + ringing\n",
    "# Per-target weights (emphasize the hard ones)\n",
    "TARGET_WEIGHTS = {\n",
    "    \"voltage_rise_time_pulse1\": 2.0,\n",
    "    \"voltage_rise_time_pulse2\": 2.0,\n",
    "    \"voltage_fall_time_pulse1\": 2.0,\n",
    "    \"voltage_fall_time_pulse2\": 2.0,\n",
    "    \"current_rise_time_pulse1\": 1.5,\n",
    "    \"current_rise_time_pulse2\": 1.5,\n",
    "    \"current_fall_time_pulse1\": 1.0,\n",
    "    \"current_fall_time_pulse2\": 1.0,\n",
    "    \"overshoot_pulse_1\": 1.0,\n",
    "    \"overshoot_pulse_2\": 1.0,\n",
    "    \"undershoot_pulse_1\": 1.0,\n",
    "    \"undershoot_pulse_2\": 1.0,\n",
    "    \"ringing_frequency_MHz\": 2.5\n",
    "}\n",
    "# --------------------------------------\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "COMBO_COLS = [\"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\"]\n",
    "\n",
    "RAW_INPUTS = [\n",
    "    \"Vbus\",\"Rg\",\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\n",
    "    # include if present\n",
    "    \"Tp1\",\"Tp2\",\"Toff\",\"Tstart\",\"L1\",\"L2\",\"L3\",\"R1\",\n",
    "    \"VDS_max\",\"ID_max_25C\",\"RDS_on_typ\",\"RDS_on_max\",\n",
    "    \"VGS_th_min\",\"VGS_th_typ\",\"VGS_th_max\",\n",
    "    \"Qg_total\",\"Qrr_typ\",\"Irrm_typ\",\"Eon_typ\",\"Eoff_typ\",\n",
    "    \"Ciss\",\"Coss\",\"Crss\",\n",
    "    \"Rth_JC_typ\",\"Rth_JC_max\",\"Tj_max\",\n",
    "    DEVICE_COL\n",
    "]\n",
    "\n",
    "TARGETS = [\n",
    "    \"voltage_rise_time_pulse1\",\"voltage_rise_time_pulse2\",\n",
    "    \"voltage_fall_time_pulse1\",\"voltage_fall_time_pulse2\",\n",
    "    \"current_rise_time_pulse1\",\"current_rise_time_pulse2\",\n",
    "    \"current_fall_time_pulse1\",\"current_fall_time_pulse2\",\n",
    "    \"overshoot_pulse_1\",\"overshoot_pulse_2\",\n",
    "    \"undershoot_pulse_1\",\"undershoot_pulse_2\",\n",
    "    \"ringing_frequency_MHz\"\n",
    "]\n",
    "\n",
    "# ---- load ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert DEVICE_COL in df.columns, f\"'{DEVICE_COL}' not found.\"\n",
    "\n",
    "RAW_INPUTS = [c for c in RAW_INPUTS if c in df.columns]\n",
    "TARGETS = [c for c in TARGETS if c in df.columns]\n",
    "for c in COMBO_COLS:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing combo column: {c}\")\n",
    "\n",
    "# ---- derived features ----\n",
    "def add_derived_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "    eps = 1e-18\n",
    "    c_stray = 5e-12\n",
    "    # L_loop\n",
    "    L_pieces = [c for c in [\"Ls4\",\"Ls5\",\"Ls6\",\"Ls7\",\"Ls8\",\"Ls9\",\"Ls10\",\"Ls11\",\"L1\",\"L2\",\"L3\"] if c in f.columns]\n",
    "    f[\"L_loop\"] = f[L_pieces].sum(axis=1) if L_pieces else np.nan\n",
    "    # C_eq, resonance\n",
    "    f[\"C_eq_est\"] = (f[\"Coss\"] if \"Coss\" in f.columns else 0).fillna(0) + c_stray\n",
    "    f[\"f_res_est\"] = 1.0/(2.0*np.pi*np.sqrt(np.clip(f[\"L_loop\"]*f[\"C_eq_est\"], eps, None)))\n",
    "    # drive proxies\n",
    "    f[\"gate_drive_strength\"] = 1.0/((f[\"Rg\"].abs() if \"Rg\" in f.columns else 0)+1e-3)\n",
    "    f[\"dvdt_proxy\"] = f[\"gate_drive_strength\"]/((f[\"Crss\"].abs() if \"Crss\" in f.columns else 0)+1e-15)\n",
    "    f[\"miller_ratio\"] = (f[\"Crss\"].abs()/(f[\"Coss\"].abs()+1e-15)) if (\"Crss\" in f.columns and \"Coss\" in f.columns) else np.nan\n",
    "    # energy/cap ratios\n",
    "    f[\"E_total\"] = (f.get(\"Eon_typ\", 0)).fillna(0) + (f.get(\"Eoff_typ\", 0)).fillna(0)\n",
    "    f[\"Cgd_ratio\"] = (f[\"Crss\"].abs()/(f[\"Ciss\"].abs()+1e-15)) if (\"Crss\" in f.columns and \"Ciss\" in f.columns) else np.nan\n",
    "    f[\"Qrr_over_Qg\"] = (f[\"Qrr_typ\"].abs()/(f[\"Qg_total\"].abs()+1e-15)) if (\"Qrr_typ\" in f.columns and \"Qg_total\" in f.columns) else np.nan\n",
    "    # damping proxy zeta ~ 0.5*R*sqrt(C/L)\n",
    "    if \"R1\" in f.columns:\n",
    "        f[\"zeta_proxy\"] = 0.5*(f[\"R1\"].abs())*np.sqrt(np.clip(f[\"C_eq_est\"]/(f[\"L_loop\"]+eps), eps, None))\n",
    "    else:\n",
    "        f[\"zeta_proxy\"] = np.nan\n",
    "    return f\n",
    "\n",
    "def make_combo_key(frame: pd.DataFrame) -> pd.Series:\n",
    "    rounded = frame[COMBO_COLS].apply(lambda s: np.round(s.astype(float), 6))\n",
    "    return rounded.apply(lambda row: tuple(row.values.tolist()), axis=1)\n",
    "\n",
    "# ---- filter to common combos & sample 25%/device ----\n",
    "devices = df[DEVICE_COL].dropna().unique().tolist()\n",
    "assert UNSEEN_DEVICE in devices, f\"{UNSEEN_DEVICE} not in dataset {devices}\"\n",
    "train_devices = [d for d in devices if d != UNSEEN_DEVICE]\n",
    "\n",
    "df[\"_combo_key\"] = make_combo_key(df)\n",
    "combos_unseen = set(df.loc[df[DEVICE_COL]==UNSEEN_DEVICE, \"_combo_key\"].dropna().unique())\n",
    "combos_train_sets = [set(df.loc[df[DEVICE_COL]==d, \"_combo_key\"].dropna().unique()) for d in train_devices]\n",
    "common_train = set.intersection(*combos_train_sets) if combos_train_sets else set()\n",
    "common_combos = combos_unseen.intersection(common_train)\n",
    "df_common = df[df[\"_combo_key\"].isin(common_combos) & df[DEVICE_COL].isin(train_devices + [UNSEEN_DEVICE])].copy()\n",
    "\n",
    "df_sampled = df_common.groupby(DEVICE_COL, group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ---- features + derived ----\n",
    "feature_cols = list(RAW_INPUTS)\n",
    "df_feat = add_derived_features(df_sampled)\n",
    "for c in [\"L_loop\",\"C_eq_est\",\"f_res_est\",\"gate_drive_strength\",\"dvdt_proxy\",\"miller_ratio\",\n",
    "          \"E_total\",\"Cgd_ratio\",\"Qrr_over_Qg\",\"zeta_proxy\"]:\n",
    "    if c in df_feat.columns: feature_cols.append(c)\n",
    "feature_cols = list(dict.fromkeys(feature_cols))\n",
    "\n",
    "# ---- split ----\n",
    "df_train = df_feat[df_feat[DEVICE_COL] != UNSEEN_DEVICE].copy()\n",
    "df_test  = df_feat[df_feat[DEVICE_COL] == UNSEEN_DEVICE].copy()\n",
    "\n",
    "# optional embedding\n",
    "if USE_EMBEDDING:\n",
    "    le = LabelEncoder()\n",
    "    known = df_train[DEVICE_COL].astype(str)\n",
    "    le.fit(known.unique().tolist())\n",
    "    df_train[\"_dev_id\"] = le.transform(df_train[DEVICE_COL].astype(str))\n",
    "    unk_id = len(le.classes_)\n",
    "    df_test[\"_dev_id\"] = df_test[DEVICE_COL].astype(str).map(lambda s: le.transform([s])[0] if s in le.classes_ else unk_id)\n",
    "else:\n",
    "    feature_cols = [c for c in feature_cols if c != DEVICE_COL]\n",
    "\n",
    "# assemble matrices\n",
    "X_num_train = df_train[feature_cols].copy()\n",
    "X_num_test  = df_test[feature_cols].copy()\n",
    "Y_cols = [c for c in TARGETS if c in df_train.columns]\n",
    "y_train = df_train[Y_cols].copy()\n",
    "y_test  = df_test[Y_cols].copy()\n",
    "\n",
    "# ---- optional log1p on selected targets ----\n",
    "log_targets = set()\n",
    "if APPLY_LOG1P:\n",
    "    for c in Y_cols:\n",
    "        if (\"rise_time\" in c) or (\"fall_time\" in c) or (\"ringing_frequency\" in c):\n",
    "            log_targets.add(c)\n",
    "def log1p_df(df_, cols):\n",
    "    out = df_.copy()\n",
    "    for c in cols:\n",
    "        out[c] = np.log1p(np.clip(out[c].values, a_min=0, a_max=None))\n",
    "    return out\n",
    "def expm1_df(df_, cols):\n",
    "    out = df_.copy()\n",
    "    for c in cols:\n",
    "        out[c] = np.expm1(out[c].values)\n",
    "    return out\n",
    "\n",
    "y_train_for_scale = log1p_df(y_train, log_targets) if log_targets else y_train.copy()\n",
    "y_test_for_scale  = log1p_df(y_test, log_targets)  if log_targets else y_test.copy()\n",
    "\n",
    "# ---- scale inputs & outputs ----\n",
    "Xscaler = StandardScaler()\n",
    "Yscalers = {c: StandardScaler() for c in Y_cols}\n",
    "\n",
    "X_train = Xscaler.fit_transform(X_num_train.fillna(X_num_train.median(numeric_only=True)))\n",
    "X_test  = Xscaler.transform(X_num_test.fillna(X_num_train.median(numeric_only=True)))\n",
    "\n",
    "y_train_scaled = np.zeros_like(y_train_for_scale.values, dtype=float)\n",
    "y_test_scaled  = np.zeros_like(y_test_for_scale.values, dtype=float)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_train_scaled[:, i] = Yscalers[c].fit_transform(y_train_for_scale[[c]].values).ravel()\n",
    "    y_test_scaled[:, i]  = Yscalers[c].transform(y_test_for_scale[[c]].values).ravel()\n",
    "\n",
    "# ---- weighted loss ----\n",
    "# MSE with per-output weights: loss = mean(sum(w_i * (y_i - yhat_i)^2))\n",
    "weights_vec = np.array([TARGET_WEIGHTS.get(c, 1.0) for c in Y_cols], dtype=\"float32\")\n",
    "weights_tf = tf.constant(weights_vec, dtype=tf.float32)\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    return tf.reduce_mean(tf.reduce_sum(weights_tf * tf.square(err), axis=-1))\n",
    "\n",
    "# ---- build model ----\n",
    "def build_model(n_num_features, n_targets, n_devices=None, emb_dim=4):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    if USE_EMBEDDING:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        inp_dev = layers.Input(shape=(), dtype=\"int32\", name=\"dev\")\n",
    "        emb = layers.Embedding(input_dim=n_devices, output_dim=emb_dim, name=\"dev_emb\")(inp_dev)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        x = layers.Concatenate()([inp_num, emb])\n",
    "    else:\n",
    "        inp_num = layers.Input(shape=(n_num_features,), name=\"num\")\n",
    "        x = inp_num\n",
    "\n",
    "    x = layers.Dense(192, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(96, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    out = layers.Dense(n_targets, activation=\"linear\", name=\"y\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp_num, inp_dev], outputs=out) if USE_EMBEDDING else Model(inputs=inp_num, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=weighted_mse, metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "n_devices = (df_train[\"_dev_id\"].max() + 2) if USE_EMBEDDING else None\n",
    "model = build_model(X_train.shape[1], len(Y_cols), n_devices=n_devices, emb_dim=4)\n",
    "\n",
    "# ---- train ----\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-5)\n",
    "\n",
    "if USE_EMBEDDING:\n",
    "    hist = model.fit({\"num\": X_train, \"dev\": df_train[\"_dev_id\"].values}, y_train_scaled,\n",
    "                     validation_split=0.15, epochs=300, batch_size=256, callbacks=[es, rlr], verbose=1)\n",
    "else:\n",
    "    hist = model.fit(X_train, y_train_scaled,\n",
    "                     validation_split=0.15, epochs=300, batch_size=256, callbacks=[es, rlr], verbose=1)\n",
    "\n",
    "# ---- predict ----\n",
    "if USE_EMBEDDING:\n",
    "    y_pred_scaled = model.predict({\"num\": X_test, \"dev\": df_test[\"_dev_id\"].values}, verbose=0)\n",
    "else:\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# inverse scale + inverse log\n",
    "y_pred_df = pd.DataFrame(y_pred_scaled, columns=Y_cols, index=y_test.index)\n",
    "for i, c in enumerate(Y_cols):\n",
    "    y_pred_df[c] = Yscalers[c].inverse_transform(y_pred_df[[c]].values).ravel()\n",
    "if log_targets:\n",
    "    y_pred_df = expm1_df(y_pred_df, log_targets)\n",
    "\n",
    "# ---- metrics ----\n",
    "def rmse(a,b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "def r2(a,b):   return float(r2_score(a, b))\n",
    "\n",
    "per_target = []\n",
    "y_true_eval = y_test.copy()\n",
    "if log_targets:\n",
    "    # y_test_for_scale was log+scaled; we want to evaluate in the original space\n",
    "    y_true_eval = y_test.copy()  # already original space\n",
    "\n",
    "for c in Y_cols:\n",
    "    per_target.append({\n",
    "        \"target\": c,\n",
    "        \"RMSE\": rmse(y_true_eval[c], y_pred_df[c]),\n",
    "        \"R2\": r2(y_true_eval[c], y_pred_df[c]),\n",
    "        \"n_test\": int(y_true_eval[c].size)\n",
    "    })\n",
    "metrics_df = pd.DataFrame(per_target).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Unseen device:\", UNSEEN_DEVICE, \"===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nOverall RMSE:\", rmse(y_true_eval.values, y_pred_df.values))\n",
    "print(\"Overall R2  :\", r2(y_true_eval.values, y_pred_df.values))\n",
    "\n",
    "# ---- save predictions ----\n",
    "OUT_PATH = os.path.join(os.path.dirname(CSV_PATH), f\"ann_unseen_{UNSEEN_DEVICE}_{'withEmb' if USE_EMBEDDING else 'noEmb'}_log{int(APPLY_LOG1P)}_wloss.csv\")\n",
    "save_cols = [DEVICE_COL] + COMBO_COLS\n",
    "save_cols = [c for c in save_cols if c in df_test.columns]\n",
    "out = df_test[save_cols].copy()\n",
    "for c in Y_cols:\n",
    "    out[c+\"_true\"] = y_true_eval[c].values\n",
    "    out[c+\"_pred\"] = y_pred_df[c].values\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved predictions: {OUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
