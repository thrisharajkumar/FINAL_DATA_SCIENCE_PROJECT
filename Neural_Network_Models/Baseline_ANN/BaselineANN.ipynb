{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b767650",
   "metadata": {},
   "source": [
    "# Baseline ANN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259d3a8",
   "metadata": {},
   "source": [
    "Here we are Sampling from the complete dataset to 25% from each unique simulation combination of:\n",
    "\n",
    "\n",
    "['Vbus', 'Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3842296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_34884\\1285976940.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(frac=0.25, random_state=SEED))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25% sampled file to: merged_train_5_MOSFETs_25percent.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs.csv\")\n",
    "\n",
    "# Select simulation columns that define unique simulation setups\n",
    "sim_cols = ['Vbus', 'Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']\n",
    "\n",
    "# Sample 25% from each unique simulation config\n",
    "df_25 = (\n",
    "    df.groupby(sim_cols, group_keys=False)\n",
    "      .apply(lambda x: x.sample(frac=0.25, random_state=SEED))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save to disk for reuse\n",
    "df_25.to_csv(\"merged_train_5_MOSFETs_25percent.csv\", index=False)\n",
    "print(\"Saved 25% sampled file to: merged_train_5_MOSFETs_25percent.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f915d40",
   "metadata": {},
   "source": [
    "## ITERATION - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9fd618",
   "metadata": {},
   "source": [
    "| **Technique**                            | **In this Iteration**                                                               |\n",
    "| ---------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **Dropout**                            | Two `Dropout(0.2)` layers after Dense                                                        |\n",
    "| **L2 Regularization**                  | `kernel_regularizer=regularizers.l2(1e-4)` in Dense layers                                   |\n",
    "| **Early Stopping**                     | `callbacks.EarlyStopping(monitor='val_loss', patience=10)`                                   |\n",
    "| **Batch Normalization**                | —                                                                                            |\n",
    "| **Learning Rate Scheduling**           | Fixed learning rate (`Adam`) without decay/scheduler                                         |\n",
    "| **Scaling (Inputs)**                   | Inputs scaled using `StandardScaler()`                                                       |\n",
    "| **Scaling (Outputs)**                  | All targets scaled using `StandardScaler`, but `ringing_frequency_MHz` uses `MinMaxScaler()` |\n",
    "| **Hyperparameter Tuning**              | Manual hyperparameters set in `build_ann()`                                                  |\n",
    "| **Evaluation (Train/Val/Test/Unseen)** | Evaluates on 4 splits: `train`, `val`, `test`, `unseen MOSFET`                               |\n",
    "| **Generalization to Unseen Device**    | Uses `Part_Number` encoding, and unseen part (`C2M0280120D`) is evaluated separately         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "006cd6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2397 - mae: 0.3097 - val_loss: 0.0843 - val_mae: 0.1451\n",
      "Epoch 2/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1087 - mae: 0.1966 - val_loss: 0.0721 - val_mae: 0.1366\n",
      "Epoch 3/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0944 - mae: 0.1830 - val_loss: 0.0668 - val_mae: 0.1301\n",
      "Epoch 4/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0899 - mae: 0.1773 - val_loss: 0.0663 - val_mae: 0.1288\n",
      "Epoch 5/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0872 - mae: 0.1741 - val_loss: 0.0640 - val_mae: 0.1259\n",
      "Epoch 6/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0861 - mae: 0.1727 - val_loss: 0.0633 - val_mae: 0.1267\n",
      "Epoch 7/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0860 - mae: 0.1725 - val_loss: 0.0616 - val_mae: 0.1222\n",
      "Epoch 8/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0843 - mae: 0.1711 - val_loss: 0.0633 - val_mae: 0.1261\n",
      "Epoch 9/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0838 - mae: 0.1703 - val_loss: 0.0613 - val_mae: 0.1225\n",
      "Epoch 10/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0839 - mae: 0.1703 - val_loss: 0.0634 - val_mae: 0.1259\n",
      "Epoch 11/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0832 - mae: 0.1697 - val_loss: 0.0630 - val_mae: 0.1268\n",
      "Epoch 12/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0828 - mae: 0.1691 - val_loss: 0.0595 - val_mae: 0.1209\n",
      "Epoch 13/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0830 - mae: 0.1691 - val_loss: 0.0613 - val_mae: 0.1243\n",
      "Epoch 14/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0824 - mae: 0.1684 - val_loss: 0.0591 - val_mae: 0.1188\n",
      "Epoch 15/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0816 - mae: 0.1681 - val_loss: 0.0600 - val_mae: 0.1216\n",
      "Epoch 16/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0821 - mae: 0.1679 - val_loss: 0.0608 - val_mae: 0.1248\n",
      "Epoch 17/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0814 - mae: 0.1673 - val_loss: 0.0586 - val_mae: 0.1198\n",
      "Epoch 18/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0818 - mae: 0.1673 - val_loss: 0.0587 - val_mae: 0.1204\n",
      "Epoch 19/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0809 - mae: 0.1670 - val_loss: 0.0598 - val_mae: 0.1214\n",
      "Epoch 20/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0808 - mae: 0.1664 - val_loss: 0.0617 - val_mae: 0.1225\n",
      "Epoch 21/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0811 - mae: 0.1669 - val_loss: 0.0605 - val_mae: 0.1247\n",
      "Epoch 22/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0815 - mae: 0.1670 - val_loss: 0.0578 - val_mae: 0.1195\n",
      "Epoch 23/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0807 - mae: 0.1665 - val_loss: 0.0604 - val_mae: 0.1239\n",
      "Epoch 24/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0805 - mae: 0.1659 - val_loss: 0.0586 - val_mae: 0.1223\n",
      "Epoch 25/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0810 - mae: 0.1665 - val_loss: 0.0593 - val_mae: 0.1233\n",
      "Epoch 26/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0802 - mae: 0.1657 - val_loss: 0.0645 - val_mae: 0.1267\n",
      "Epoch 27/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0805 - mae: 0.1659 - val_loss: 0.0584 - val_mae: 0.1202\n",
      "Epoch 28/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0808 - mae: 0.1661 - val_loss: 0.0609 - val_mae: 0.1264\n",
      "Epoch 29/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0803 - mae: 0.1661 - val_loss: 0.0628 - val_mae: 0.1261\n",
      "Epoch 30/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0803 - mae: 0.1656 - val_loss: 0.0598 - val_mae: 0.1248\n",
      "Epoch 31/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0801 - mae: 0.1660 - val_loss: 0.0595 - val_mae: 0.1225\n",
      "Epoch 32/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0805 - mae: 0.1659 - val_loss: 0.0581 - val_mae: 0.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998224 2.482687e-10\n",
      "voltage_fall_time_pulse1 0.998206 2.498879e-10\n",
      "current_fall_time_pulse2 0.993574 8.118418e-10\n",
      "current_fall_time_pulse1 0.993474 8.199246e-10\n",
      "voltage_rise_time_pulse1 0.981386 5.419773e-10\n",
      "   ringing_frequency_MHz 0.977980 2.963775e-06\n",
      "       overshoot_pulse_1 0.973717 2.503170e+00\n",
      "      undershoot_pulse_2 0.964102 2.172334e+00\n",
      "      undershoot_pulse_1 0.962148 2.260896e+00\n",
      "current_rise_time_pulse2 0.949044 7.116870e-09\n",
      "       overshoot_pulse_2 0.915609 6.622726e+00\n",
      "voltage_rise_time_pulse2 0.885372 1.645811e-09\n",
      "current_rise_time_pulse1 0.712010 1.443695e-08\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse1 0.998275 2.447454e-10\n",
      "voltage_fall_time_pulse2 0.998255 2.457510e-10\n",
      "current_fall_time_pulse1 0.993729 8.022779e-10\n",
      "current_fall_time_pulse2 0.993687 8.034622e-10\n",
      "voltage_rise_time_pulse1 0.980540 5.506081e-10\n",
      "   ringing_frequency_MHz 0.977639 2.987550e-06\n",
      "       overshoot_pulse_1 0.974232 2.466488e+00\n",
      "      undershoot_pulse_2 0.962306 2.227088e+00\n",
      "      undershoot_pulse_1 0.961511 2.269878e+00\n",
      "current_rise_time_pulse2 0.948913 7.089207e-09\n",
      "       overshoot_pulse_2 0.913395 6.686610e+00\n",
      "voltage_rise_time_pulse2 0.888108 1.595568e-09\n",
      "current_rise_time_pulse1 0.702120 1.457058e-08\n",
      "\u001b[1m2633/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 870us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998229 2.478926e-10\n",
      "voltage_fall_time_pulse1 0.998216 2.491233e-10\n",
      "current_fall_time_pulse2 0.993591 8.105904e-10\n",
      "current_fall_time_pulse1 0.993512 8.173018e-10\n",
      "voltage_rise_time_pulse1 0.981261 5.432807e-10\n",
      "   ringing_frequency_MHz 0.977930 2.967353e-06\n",
      "       overshoot_pulse_1 0.973795 2.497702e+00\n",
      "      undershoot_pulse_2 0.963832 2.180635e+00\n",
      "      undershoot_pulse_1 0.962053 2.262246e+00\n",
      "current_rise_time_pulse2 0.949026 7.112727e-09\n",
      "       overshoot_pulse_2 0.915281 6.632348e+00\n",
      "voltage_rise_time_pulse2 0.885770 1.638373e-09\n",
      "current_rise_time_pulse1 0.710547 1.445707e-08\n",
      "\u001b[1m918/918\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 826us/step\n",
      "                  Target       R2         RMSE\n",
      "      undershoot_pulse_1 0.824860 5.638523e+00\n",
      "      undershoot_pulse_2 0.822619 5.685007e+00\n",
      "current_fall_time_pulse1 0.748429 3.150780e-09\n",
      "current_fall_time_pulse2 0.689709 3.445720e-09\n",
      "current_rise_time_pulse2 0.560550 7.657961e-09\n",
      "voltage_rise_time_pulse2 0.237996 1.849320e-09\n",
      "\u001b[1m2633/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 895us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 1: BASELINE ANN WITH PART_NUMBER GENERALIZATION ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0280120D'\n",
    "\n",
    "BASE_DIR = \"first_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")  # Add encoded device as feature\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE BASELINE ANN ==============\n",
    "def build_ann(input_dim, output_dim, dropout=0.2, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/baseline_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL (INTERNAL TEST) SCATTER PLOTS ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='lightblue')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f7221",
   "metadata": {},
   "source": [
    "## ITERATION - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68c1afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1528 - mae: 0.2224 - val_loss: 0.0620 - val_mae: 0.1333\n",
      "Epoch 2/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0593 - mae: 0.1273 - val_loss: 0.0555 - val_mae: 0.1231\n",
      "Epoch 3/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0533 - mae: 0.1175 - val_loss: 0.0530 - val_mae: 0.1171\n",
      "Epoch 4/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0509 - mae: 0.1130 - val_loss: 0.0508 - val_mae: 0.1119\n",
      "Epoch 5/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - mae: 0.1101 - val_loss: 0.0499 - val_mae: 0.1095\n",
      "Epoch 6/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0486 - mae: 0.1081 - val_loss: 0.0491 - val_mae: 0.1079\n",
      "Epoch 7/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0480 - mae: 0.1068 - val_loss: 0.0484 - val_mae: 0.1064\n",
      "Epoch 8/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0475 - mae: 0.1057 - val_loss: 0.0479 - val_mae: 0.1056\n",
      "Epoch 9/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0471 - mae: 0.1047 - val_loss: 0.0478 - val_mae: 0.1055\n",
      "Epoch 10/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0468 - mae: 0.1040 - val_loss: 0.0473 - val_mae: 0.1046\n",
      "Epoch 11/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0465 - mae: 0.1033 - val_loss: 0.0470 - val_mae: 0.1037\n",
      "Epoch 12/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0462 - mae: 0.1026 - val_loss: 0.0467 - val_mae: 0.1030\n",
      "Epoch 13/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0460 - mae: 0.1020 - val_loss: 0.0464 - val_mae: 0.1024\n",
      "Epoch 14/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0457 - mae: 0.1014 - val_loss: 0.0461 - val_mae: 0.1018\n",
      "Epoch 15/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0455 - mae: 0.1010 - val_loss: 0.0460 - val_mae: 0.1012\n",
      "Epoch 16/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0453 - mae: 0.1005 - val_loss: 0.0458 - val_mae: 0.1009\n",
      "Epoch 17/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0452 - mae: 0.1001 - val_loss: 0.0458 - val_mae: 0.1006\n",
      "Epoch 18/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0450 - mae: 0.0997 - val_loss: 0.0455 - val_mae: 0.1003\n",
      "Epoch 19/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0448 - mae: 0.0995 - val_loss: 0.0455 - val_mae: 0.1002\n",
      "Epoch 20/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0447 - mae: 0.0992 - val_loss: 0.0455 - val_mae: 0.1002\n",
      "Epoch 21/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0446 - mae: 0.0989 - val_loss: 0.0453 - val_mae: 0.0998\n",
      "Epoch 22/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.0986 - val_loss: 0.0452 - val_mae: 0.0997\n",
      "Epoch 23/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0444 - mae: 0.0984 - val_loss: 0.0452 - val_mae: 0.0993\n",
      "Epoch 24/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.0982 - val_loss: 0.0454 - val_mae: 0.0997\n",
      "Epoch 25/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.0980 - val_loss: 0.0450 - val_mae: 0.0988\n",
      "Epoch 26/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.0978 - val_loss: 0.0449 - val_mae: 0.0986\n",
      "Epoch 27/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0440 - mae: 0.0976 - val_loss: 0.0448 - val_mae: 0.0986\n",
      "Epoch 28/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0439 - mae: 0.0975 - val_loss: 0.0448 - val_mae: 0.0984\n",
      "Epoch 29/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0439 - mae: 0.0973 - val_loss: 0.0448 - val_mae: 0.0983\n",
      "Epoch 30/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0438 - mae: 0.0972 - val_loss: 0.0448 - val_mae: 0.0983\n",
      "Epoch 31/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0437 - mae: 0.0969 - val_loss: 0.0447 - val_mae: 0.0981\n",
      "Epoch 32/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0437 - mae: 0.0968 - val_loss: 0.0447 - val_mae: 0.0981\n",
      "Epoch 33/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0436 - mae: 0.0966 - val_loss: 0.0447 - val_mae: 0.0983\n",
      "Epoch 34/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0435 - mae: 0.0965 - val_loss: 0.0446 - val_mae: 0.0980\n",
      "Epoch 35/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0434 - mae: 0.0964 - val_loss: 0.0445 - val_mae: 0.0976\n",
      "Epoch 36/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0434 - mae: 0.0962 - val_loss: 0.0446 - val_mae: 0.0977\n",
      "Epoch 37/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.0960 - val_loss: 0.0445 - val_mae: 0.0975\n",
      "Epoch 38/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.0960 - val_loss: 0.0443 - val_mae: 0.0969\n",
      "Epoch 39/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0432 - mae: 0.0958 - val_loss: 0.0443 - val_mae: 0.0970\n",
      "Epoch 40/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0431 - mae: 0.0957 - val_loss: 0.0442 - val_mae: 0.0968\n",
      "Epoch 41/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0431 - mae: 0.0955 - val_loss: 0.0443 - val_mae: 0.0967\n",
      "Epoch 42/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0430 - mae: 0.0955 - val_loss: 0.0445 - val_mae: 0.0972\n",
      "Epoch 43/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0430 - mae: 0.0954 - val_loss: 0.0443 - val_mae: 0.0967\n",
      "Epoch 44/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0430 - mae: 0.0953 - val_loss: 0.0443 - val_mae: 0.0967\n",
      "Epoch 45/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0429 - mae: 0.0952 - val_loss: 0.0443 - val_mae: 0.0966\n",
      "Epoch 46/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.0952 - val_loss: 0.0443 - val_mae: 0.0967\n",
      "Epoch 47/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0428 - mae: 0.0951 - val_loss: 0.0442 - val_mae: 0.0965\n",
      "Epoch 48/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0428 - mae: 0.0950 - val_loss: 0.0443 - val_mae: 0.0967\n",
      "Epoch 49/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.0949 - val_loss: 0.0440 - val_mae: 0.0960\n",
      "Epoch 50/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.0948 - val_loss: 0.0441 - val_mae: 0.0962\n",
      "Epoch 51/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0426 - mae: 0.0948 - val_loss: 0.0441 - val_mae: 0.0964\n",
      "Epoch 52/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0426 - mae: 0.0948 - val_loss: 0.0441 - val_mae: 0.0962\n",
      "Epoch 53/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0426 - mae: 0.0947 - val_loss: 0.0440 - val_mae: 0.0959\n",
      "Epoch 54/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0425 - mae: 0.0946 - val_loss: 0.0440 - val_mae: 0.0961\n",
      "Epoch 55/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0424 - mae: 0.0945 - val_loss: 0.0442 - val_mae: 0.0965\n",
      "Epoch 56/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0424 - mae: 0.0945 - val_loss: 0.0440 - val_mae: 0.0960\n",
      "Epoch 57/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0424 - mae: 0.0944 - val_loss: 0.0441 - val_mae: 0.0960\n",
      "Epoch 58/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0423 - mae: 0.0944 - val_loss: 0.0440 - val_mae: 0.0961\n",
      "Epoch 59/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0423 - mae: 0.0943 - val_loss: 0.0440 - val_mae: 0.0961\n",
      "Epoch 60/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0423 - mae: 0.0942 - val_loss: 0.0442 - val_mae: 0.0966\n",
      "Epoch 61/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0422 - mae: 0.0942 - val_loss: 0.0441 - val_mae: 0.0966\n",
      "Epoch 62/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0423 - mae: 0.0942 - val_loss: 0.0439 - val_mae: 0.0958\n",
      "Epoch 63/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0422 - mae: 0.0941 - val_loss: 0.0439 - val_mae: 0.0961\n",
      "Epoch 64/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0422 - mae: 0.0940 - val_loss: 0.0438 - val_mae: 0.0961\n",
      "Epoch 65/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0421 - mae: 0.0940 - val_loss: 0.0439 - val_mae: 0.0960\n",
      "Epoch 66/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0421 - mae: 0.0940 - val_loss: 0.0439 - val_mae: 0.0958\n",
      "Epoch 67/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0421 - mae: 0.0939 - val_loss: 0.0441 - val_mae: 0.0967\n",
      "Epoch 68/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0421 - mae: 0.0940 - val_loss: 0.0438 - val_mae: 0.0957\n",
      "Epoch 69/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0420 - mae: 0.0938 - val_loss: 0.0438 - val_mae: 0.0957\n",
      "Epoch 70/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0420 - mae: 0.0938 - val_loss: 0.0438 - val_mae: 0.0956\n",
      "Epoch 71/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0420 - mae: 0.0938 - val_loss: 0.0438 - val_mae: 0.0954\n",
      "Epoch 72/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0420 - mae: 0.0937 - val_loss: 0.0438 - val_mae: 0.0954\n",
      "Epoch 73/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0419 - mae: 0.0936 - val_loss: 0.0438 - val_mae: 0.0955\n",
      "Epoch 74/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0419 - mae: 0.0936 - val_loss: 0.0437 - val_mae: 0.0954\n",
      "Epoch 75/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.0936 - val_loss: 0.0437 - val_mae: 0.0954\n",
      "Epoch 76/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.0936 - val_loss: 0.0436 - val_mae: 0.0954\n",
      "Epoch 77/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.0935 - val_loss: 0.0436 - val_mae: 0.0950\n",
      "Epoch 78/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.0935 - val_loss: 0.0437 - val_mae: 0.0951\n",
      "Epoch 79/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.0935 - val_loss: 0.0437 - val_mae: 0.0951\n",
      "Epoch 80/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0417 - mae: 0.0934 - val_loss: 0.0436 - val_mae: 0.0951\n",
      "Epoch 81/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0417 - mae: 0.0934 - val_loss: 0.0437 - val_mae: 0.0951\n",
      "Epoch 82/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0417 - mae: 0.0933 - val_loss: 0.0436 - val_mae: 0.0949\n",
      "Epoch 83/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0417 - mae: 0.0933 - val_loss: 0.0436 - val_mae: 0.0949\n",
      "Epoch 84/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0417 - mae: 0.0933 - val_loss: 0.0436 - val_mae: 0.0948\n",
      "Epoch 85/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0416 - mae: 0.0932 - val_loss: 0.0435 - val_mae: 0.0947\n",
      "Epoch 86/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0416 - mae: 0.0932 - val_loss: 0.0436 - val_mae: 0.0949\n",
      "Epoch 87/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0416 - mae: 0.0932 - val_loss: 0.0436 - val_mae: 0.0949\n",
      "Epoch 88/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.0932 - val_loss: 0.0436 - val_mae: 0.0948\n",
      "Epoch 89/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.0930 - val_loss: 0.0437 - val_mae: 0.0949\n",
      "Epoch 90/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.0931 - val_loss: 0.0438 - val_mae: 0.0950\n",
      "Epoch 91/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.0930 - val_loss: 0.0437 - val_mae: 0.0951\n",
      "Epoch 92/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.0930 - val_loss: 0.0437 - val_mae: 0.0949\n",
      "Epoch 93/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0414 - mae: 0.0929 - val_loss: 0.0437 - val_mae: 0.0949\n",
      "Epoch 94/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0414 - mae: 0.0929 - val_loss: 0.0437 - val_mae: 0.0947\n",
      "Epoch 95/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0413 - mae: 0.0928 - val_loss: 0.0435 - val_mae: 0.0945\n",
      "Epoch 96/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0414 - mae: 0.0929 - val_loss: 0.0436 - val_mae: 0.0946\n",
      "Epoch 97/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0413 - mae: 0.0928 - val_loss: 0.0435 - val_mae: 0.0947\n",
      "Epoch 98/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0413 - mae: 0.0929 - val_loss: 0.0435 - val_mae: 0.0945\n",
      "Epoch 99/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0413 - mae: 0.0929 - val_loss: 0.0435 - val_mae: 0.0943\n",
      "Epoch 100/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0412 - mae: 0.0927 - val_loss: 0.0437 - val_mae: 0.0947\n",
      "Epoch 101/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0412 - mae: 0.0928 - val_loss: 0.0435 - val_mae: 0.0946\n",
      "Epoch 102/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0412 - mae: 0.0927 - val_loss: 0.0436 - val_mae: 0.0946\n",
      "Epoch 103/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0412 - mae: 0.0927 - val_loss: 0.0436 - val_mae: 0.0944\n",
      "Epoch 104/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0412 - mae: 0.0927 - val_loss: 0.0438 - val_mae: 0.0948\n",
      "Epoch 105/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0412 - mae: 0.0928 - val_loss: 0.0437 - val_mae: 0.0946\n",
      "Epoch 106/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0411 - mae: 0.0927 - val_loss: 0.0437 - val_mae: 0.0947\n",
      "Epoch 107/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0412 - mae: 0.0927 - val_loss: 0.0437 - val_mae: 0.0948\n",
      "Epoch 108/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0412 - mae: 0.0927 - val_loss: 0.0437 - val_mae: 0.0948\n",
      "Epoch 109/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0411 - mae: 0.0927 - val_loss: 0.0439 - val_mae: 0.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 899us/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.999214 1.651387e-10\n",
      "voltage_fall_time_pulse1 0.999211 1.656969e-10\n",
      "current_fall_time_pulse2 0.998619 3.763129e-10\n",
      "current_fall_time_pulse1 0.998474 3.964882e-10\n",
      "   ringing_frequency_MHz 0.998188 8.502798e-07\n",
      "voltage_rise_time_pulse1 0.991424 3.678801e-10\n",
      "       overshoot_pulse_1 0.987109 1.753050e+00\n",
      "      undershoot_pulse_2 0.976192 1.769095e+00\n",
      "      undershoot_pulse_1 0.973869 1.878530e+00\n",
      "current_rise_time_pulse2 0.967089 5.719536e-09\n",
      "       overshoot_pulse_2 0.925020 6.242556e+00\n",
      "voltage_rise_time_pulse2 0.907220 1.480680e-09\n",
      "current_rise_time_pulse1 0.746301 1.355021e-08\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.999245 1.616906e-10\n",
      "voltage_fall_time_pulse1 0.999237 1.627953e-10\n",
      "current_fall_time_pulse2 0.998548 3.853105e-10\n",
      "current_fall_time_pulse1 0.998423 4.022763e-10\n",
      "   ringing_frequency_MHz 0.998191 8.498277e-07\n",
      "voltage_rise_time_pulse1 0.990980 3.748669e-10\n",
      "       overshoot_pulse_1 0.986817 1.764174e+00\n",
      "      undershoot_pulse_2 0.974912 1.816928e+00\n",
      "      undershoot_pulse_1 0.972392 1.922461e+00\n",
      "current_rise_time_pulse2 0.964822 5.882717e-09\n",
      "       overshoot_pulse_2 0.920833 6.392995e+00\n",
      "voltage_rise_time_pulse2 0.900946 1.501248e-09\n",
      "current_rise_time_pulse1 0.720105 1.412386e-08\n",
      "\u001b[1m2633/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 900us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.999219 1.646261e-10\n",
      "voltage_fall_time_pulse1 0.999215 1.652649e-10\n",
      "current_fall_time_pulse2 0.998609 3.776762e-10\n",
      "current_fall_time_pulse1 0.998466 3.973618e-10\n",
      "   ringing_frequency_MHz 0.998188 8.502120e-07\n",
      "voltage_rise_time_pulse1 0.991358 3.689365e-10\n",
      "       overshoot_pulse_1 0.987066 1.754723e+00\n",
      "      undershoot_pulse_2 0.976000 1.776352e+00\n",
      "      undershoot_pulse_1 0.973649 1.885185e+00\n",
      "current_rise_time_pulse2 0.966753 5.744309e-09\n",
      "       overshoot_pulse_2 0.924398 6.265352e+00\n",
      "voltage_rise_time_pulse2 0.906310 1.483784e-09\n",
      "current_rise_time_pulse1 0.742424 1.363779e-08\n",
      "\u001b[1m918/918\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse2 0.799569 2.769351e-09\n",
      "current_fall_time_pulse1 0.781483 2.936502e-09\n",
      "      undershoot_pulse_2 0.086894 1.289846e+01\n",
      "      undershoot_pulse_1 0.040793 1.319559e+01\n",
      "voltage_rise_time_pulse2 0.034633 2.081516e-09\n",
      "\u001b[1m2633/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 947us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 2: NO REGULARIZATION ANN ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0280120D'\n",
    "\n",
    "BASE_DIR = \"second_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")  # Add encoded device as feature\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE NO REGULARIZATION ANN ==============\n",
    "def build_ann(input_dim, output_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/no_regularization_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL (INTERNAL TEST) SCATTER PLOTS ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='orange')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281b869",
   "metadata": {},
   "source": [
    "## ITERATION - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cd31de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.2020 - mae: 0.2713 - val_loss: 0.0858 - val_mae: 0.1573\n",
      "Epoch 2/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0942 - mae: 0.1784 - val_loss: 0.0779 - val_mae: 0.1507\n",
      "Epoch 3/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0834 - mae: 0.1650 - val_loss: 0.0747 - val_mae: 0.1509\n",
      "Epoch 4/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0777 - mae: 0.1572 - val_loss: 0.0713 - val_mae: 0.1467\n",
      "Epoch 5/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0744 - mae: 0.1529 - val_loss: 0.0693 - val_mae: 0.1434\n",
      "Epoch 6/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0723 - mae: 0.1501 - val_loss: 0.0664 - val_mae: 0.1381\n",
      "Epoch 7/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0707 - mae: 0.1481 - val_loss: 0.0648 - val_mae: 0.1356\n",
      "Epoch 8/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0694 - mae: 0.1463 - val_loss: 0.0634 - val_mae: 0.1333\n",
      "Epoch 9/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0684 - mae: 0.1449 - val_loss: 0.0632 - val_mae: 0.1335\n",
      "Epoch 10/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0677 - mae: 0.1437 - val_loss: 0.0616 - val_mae: 0.1301\n",
      "Epoch 11/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0670 - mae: 0.1427 - val_loss: 0.0605 - val_mae: 0.1275\n",
      "Epoch 12/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0664 - mae: 0.1418 - val_loss: 0.0599 - val_mae: 0.1259\n",
      "Epoch 13/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0660 - mae: 0.1410 - val_loss: 0.0596 - val_mae: 0.1252\n",
      "Epoch 14/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0655 - mae: 0.1401 - val_loss: 0.0589 - val_mae: 0.1244\n",
      "Epoch 15/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0651 - mae: 0.1393 - val_loss: 0.0582 - val_mae: 0.1226\n",
      "Epoch 16/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0647 - mae: 0.1384 - val_loss: 0.0581 - val_mae: 0.1222\n",
      "Epoch 17/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0643 - mae: 0.1378 - val_loss: 0.0582 - val_mae: 0.1216\n",
      "Epoch 18/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0640 - mae: 0.1371 - val_loss: 0.0580 - val_mae: 0.1215\n",
      "Epoch 19/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0637 - mae: 0.1364 - val_loss: 0.0577 - val_mae: 0.1217\n",
      "Epoch 20/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0634 - mae: 0.1358 - val_loss: 0.0583 - val_mae: 0.1218\n",
      "Epoch 21/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0631 - mae: 0.1352 - val_loss: 0.0575 - val_mae: 0.1207\n",
      "Epoch 22/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0627 - mae: 0.1346 - val_loss: 0.0574 - val_mae: 0.1214\n",
      "Epoch 23/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0624 - mae: 0.1339 - val_loss: 0.0591 - val_mae: 0.1248\n",
      "Epoch 24/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0622 - mae: 0.1333 - val_loss: 0.0578 - val_mae: 0.1224\n",
      "Epoch 25/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0620 - mae: 0.1328 - val_loss: 0.0577 - val_mae: 0.1222\n",
      "Epoch 26/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0617 - mae: 0.1322 - val_loss: 0.0571 - val_mae: 0.1222\n",
      "Epoch 27/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0614 - mae: 0.1315 - val_loss: 0.0573 - val_mae: 0.1224\n",
      "Epoch 28/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0612 - mae: 0.1309 - val_loss: 0.0570 - val_mae: 0.1219\n",
      "Epoch 29/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0610 - mae: 0.1304 - val_loss: 0.0571 - val_mae: 0.1228\n",
      "Epoch 30/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0608 - mae: 0.1298 - val_loss: 0.0570 - val_mae: 0.1221\n",
      "Epoch 31/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0605 - mae: 0.1292 - val_loss: 0.0566 - val_mae: 0.1213\n",
      "Epoch 32/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0604 - mae: 0.1288 - val_loss: 0.0563 - val_mae: 0.1205\n",
      "Epoch 33/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0602 - mae: 0.1284 - val_loss: 0.0567 - val_mae: 0.1223\n",
      "Epoch 34/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0600 - mae: 0.1280 - val_loss: 0.0567 - val_mae: 0.1226\n",
      "Epoch 35/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0598 - mae: 0.1276 - val_loss: 0.0561 - val_mae: 0.1208\n",
      "Epoch 36/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0597 - mae: 0.1273 - val_loss: 0.0559 - val_mae: 0.1208\n",
      "Epoch 37/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0595 - mae: 0.1270 - val_loss: 0.0558 - val_mae: 0.1207\n",
      "Epoch 38/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0594 - mae: 0.1266 - val_loss: 0.0554 - val_mae: 0.1198\n",
      "Epoch 39/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0592 - mae: 0.1261 - val_loss: 0.0555 - val_mae: 0.1195\n",
      "Epoch 40/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0591 - mae: 0.1258 - val_loss: 0.0559 - val_mae: 0.1197\n",
      "Epoch 41/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0589 - mae: 0.1254 - val_loss: 0.0551 - val_mae: 0.1185\n",
      "Epoch 42/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0589 - mae: 0.1252 - val_loss: 0.0552 - val_mae: 0.1192\n",
      "Epoch 43/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0588 - mae: 0.1249 - val_loss: 0.0549 - val_mae: 0.1184\n",
      "Epoch 44/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0587 - mae: 0.1248 - val_loss: 0.0548 - val_mae: 0.1178\n",
      "Epoch 45/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0585 - mae: 0.1244 - val_loss: 0.0552 - val_mae: 0.1191\n",
      "Epoch 46/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0585 - mae: 0.1243 - val_loss: 0.0553 - val_mae: 0.1194\n",
      "Epoch 47/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0584 - mae: 0.1241 - val_loss: 0.0552 - val_mae: 0.1190\n",
      "Epoch 48/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0583 - mae: 0.1239 - val_loss: 0.0544 - val_mae: 0.1176\n",
      "Epoch 49/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0583 - mae: 0.1237 - val_loss: 0.0547 - val_mae: 0.1175\n",
      "Epoch 50/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0581 - mae: 0.1234 - val_loss: 0.0546 - val_mae: 0.1173\n",
      "Epoch 51/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0581 - mae: 0.1234 - val_loss: 0.0546 - val_mae: 0.1178\n",
      "Epoch 52/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0580 - mae: 0.1231 - val_loss: 0.0543 - val_mae: 0.1173\n",
      "Epoch 53/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0580 - mae: 0.1230 - val_loss: 0.0546 - val_mae: 0.1170\n",
      "Epoch 54/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0579 - mae: 0.1227 - val_loss: 0.0542 - val_mae: 0.1164\n",
      "Epoch 55/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0578 - mae: 0.1226 - val_loss: 0.0541 - val_mae: 0.1160\n",
      "Epoch 56/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0578 - mae: 0.1224 - val_loss: 0.0540 - val_mae: 0.1158\n",
      "Epoch 57/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0577 - mae: 0.1222 - val_loss: 0.0540 - val_mae: 0.1169\n",
      "Epoch 58/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0576 - mae: 0.1220 - val_loss: 0.0533 - val_mae: 0.1148\n",
      "Epoch 59/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0576 - mae: 0.1218 - val_loss: 0.0536 - val_mae: 0.1154\n",
      "Epoch 60/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0575 - mae: 0.1217 - val_loss: 0.0535 - val_mae: 0.1151\n",
      "Epoch 61/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0574 - mae: 0.1215 - val_loss: 0.0539 - val_mae: 0.1149\n",
      "Epoch 62/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0573 - mae: 0.1213 - val_loss: 0.0538 - val_mae: 0.1141\n",
      "Epoch 63/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0573 - mae: 0.1213 - val_loss: 0.0534 - val_mae: 0.1149\n",
      "Epoch 64/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0572 - mae: 0.1211 - val_loss: 0.0535 - val_mae: 0.1145\n",
      "Epoch 65/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0572 - mae: 0.1211 - val_loss: 0.0534 - val_mae: 0.1137\n",
      "Epoch 66/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0573 - mae: 0.1211 - val_loss: 0.0533 - val_mae: 0.1124\n",
      "Epoch 67/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0572 - mae: 0.1209 - val_loss: 0.0530 - val_mae: 0.1119\n",
      "Epoch 68/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0571 - mae: 0.1208 - val_loss: 0.0531 - val_mae: 0.1130\n",
      "Epoch 69/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0570 - mae: 0.1207 - val_loss: 0.0526 - val_mae: 0.1113\n",
      "Epoch 70/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0570 - mae: 0.1205 - val_loss: 0.0531 - val_mae: 0.1121\n",
      "Epoch 71/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0569 - mae: 0.1204 - val_loss: 0.0533 - val_mae: 0.1130\n",
      "Epoch 72/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0569 - mae: 0.1204 - val_loss: 0.0536 - val_mae: 0.1133\n",
      "Epoch 73/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0568 - mae: 0.1202 - val_loss: 0.0537 - val_mae: 0.1141\n",
      "Epoch 74/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0569 - mae: 0.1202 - val_loss: 0.0536 - val_mae: 0.1137\n",
      "Epoch 75/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0568 - mae: 0.1202 - val_loss: 0.0534 - val_mae: 0.1127\n",
      "Epoch 76/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0567 - mae: 0.1200 - val_loss: 0.0544 - val_mae: 0.1148\n",
      "Epoch 77/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0567 - mae: 0.1199 - val_loss: 0.0540 - val_mae: 0.1146\n",
      "Epoch 78/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0567 - mae: 0.1199 - val_loss: 0.0538 - val_mae: 0.1142\n",
      "Epoch 79/200\n",
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0566 - mae: 0.1197 - val_loss: 0.0539 - val_mae: 0.1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2238/2238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998576 2.223389e-10\n",
      "voltage_fall_time_pulse1 0.998481 2.299636e-10\n",
      "current_fall_time_pulse2 0.997048 5.502547e-10\n",
      "current_fall_time_pulse1 0.996792 5.748204e-10\n",
      "   ringing_frequency_MHz 0.990809 1.914749e-06\n",
      "       overshoot_pulse_1 0.980332 2.165375e+00\n",
      "voltage_rise_time_pulse1 0.976791 6.051882e-10\n",
      "      undershoot_pulse_2 0.969839 1.991184e+00\n",
      "      undershoot_pulse_1 0.965770 2.150019e+00\n",
      "current_rise_time_pulse2 0.958849 6.395643e-09\n",
      "       overshoot_pulse_2 0.918994 6.488559e+00\n",
      "voltage_rise_time_pulse2 0.881075 1.676374e-09\n",
      "current_rise_time_pulse1 0.724313 1.412521e-08\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998593 2.206554e-10\n",
      "voltage_fall_time_pulse1 0.998452 2.318317e-10\n",
      "current_fall_time_pulse2 0.996996 5.542328e-10\n",
      "current_fall_time_pulse1 0.996791 5.739069e-10\n",
      "   ringing_frequency_MHz 0.990783 1.918033e-06\n",
      "       overshoot_pulse_1 0.980389 2.151738e+00\n",
      "voltage_rise_time_pulse1 0.975299 6.203383e-10\n",
      "      undershoot_pulse_2 0.968280 2.043002e+00\n",
      "      undershoot_pulse_1 0.964821 2.170087e+00\n",
      "current_rise_time_pulse2 0.958908 6.358003e-09\n",
      "       overshoot_pulse_2 0.916800 6.553838e+00\n",
      "voltage_rise_time_pulse2 0.885141 1.616584e-09\n",
      "current_rise_time_pulse1 0.712121 1.432389e-08\n",
      "\u001b[1m2633/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998578 2.220872e-10\n",
      "voltage_fall_time_pulse1 0.998476 2.302448e-10\n",
      "current_fall_time_pulse2 0.997040 5.508533e-10\n",
      "current_fall_time_pulse1 0.996792 5.746835e-10\n",
      "   ringing_frequency_MHz 0.990806 1.915242e-06\n",
      "       overshoot_pulse_1 0.980341 2.163335e+00\n",
      "voltage_rise_time_pulse1 0.976570 6.074848e-10\n",
      "      undershoot_pulse_2 0.969605 1.999042e+00\n",
      "      undershoot_pulse_1 0.965629 2.153041e+00\n",
      "current_rise_time_pulse2 0.958858 6.390011e-09\n",
      "       overshoot_pulse_2 0.918669 6.498393e+00\n",
      "voltage_rise_time_pulse2 0.881667 1.667542e-09\n",
      "current_rise_time_pulse1 0.722509 1.415519e-08\n",
      "\u001b[1m918/918\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "                  Target       R2         RMSE\n",
      "       overshoot_pulse_1 0.814110 2.941310e+00\n",
      "current_fall_time_pulse1 0.731268 3.256475e-09\n",
      "      undershoot_pulse_1 0.711573 7.235867e+00\n",
      "current_fall_time_pulse2 0.690943 3.438862e-09\n",
      "      undershoot_pulse_2 0.677359 7.667199e+00\n",
      "current_rise_time_pulse2 0.518084 8.019442e-09\n",
      "\u001b[1m2633/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 3: BATCHNORM + L2, NO DROPOUT ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0280120D'\n",
    "\n",
    "BASE_DIR = \"third_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE ANN with BATCHNORM + L2 =================\n",
    "def build_ann(input_dim, output_dim, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/bn_l2_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL SCATTER PLOTS (TEST) ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='purple')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5b0ca",
   "metadata": {},
   "source": [
    "## ITERATION - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.3939 - mae: 0.4335 - val_loss: 0.0870 - val_mae: 0.1555 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1338 - mae: 0.2378 - val_loss: 0.0813 - val_mae: 0.1540 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1207 - mae: 0.2251 - val_loss: 0.0759 - val_mae: 0.1512 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1140 - mae: 0.2199 - val_loss: 0.0716 - val_mae: 0.1456 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.1101 - mae: 0.2167 - val_loss: 0.0659 - val_mae: 0.1325 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.1086 - mae: 0.2148 - val_loss: 0.0682 - val_mae: 0.1402 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.1071 - mae: 0.2133 - val_loss: 0.0680 - val_mae: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.1059 - mae: 0.2119 - val_loss: 0.0677 - val_mae: 0.1395 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1051 - mae: 0.2106 - val_loss: 0.0649 - val_mae: 0.1314 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1046 - mae: 0.2100 - val_loss: 0.0646 - val_mae: 0.1299 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1042 - mae: 0.2090 - val_loss: 0.0687 - val_mae: 0.1410 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1029 - mae: 0.2079 - val_loss: 0.0665 - val_mae: 0.1350 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1029 - mae: 0.2073 - val_loss: 0.0633 - val_mae: 0.1315 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1021 - mae: 0.2065 - val_loss: 0.0664 - val_mae: 0.1402 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1017 - mae: 0.2058 - val_loss: 0.0644 - val_mae: 0.1321 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.1014 - mae: 0.2049 - val_loss: 0.0684 - val_mae: 0.1397 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1014 - mae: 0.2053 - val_loss: 0.0652 - val_mae: 0.1352 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m1113/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1010 - mae: 0.2044\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1010 - mae: 0.2044 - val_loss: 0.0674 - val_mae: 0.1427 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0984 - mae: 0.2010 - val_loss: 0.0600 - val_mae: 0.1232 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0971 - mae: 0.1997 - val_loss: 0.0601 - val_mae: 0.1229 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0963 - mae: 0.1993 - val_loss: 0.0595 - val_mae: 0.1211 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0960 - mae: 0.1992 - val_loss: 0.0601 - val_mae: 0.1268 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0960 - mae: 0.1993 - val_loss: 0.0616 - val_mae: 0.1273 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0955 - mae: 0.1986 - val_loss: 0.0583 - val_mae: 0.1216 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0962 - mae: 0.1995 - val_loss: 0.0588 - val_mae: 0.1220 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0950 - mae: 0.1980 - val_loss: 0.0602 - val_mae: 0.1207 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0956 - mae: 0.1988 - val_loss: 0.0607 - val_mae: 0.1240 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0959 - mae: 0.1986 - val_loss: 0.0627 - val_mae: 0.1260 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m1098/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0950 - mae: 0.1983\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0950 - mae: 0.1983 - val_loss: 0.0618 - val_mae: 0.1259 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0937 - mae: 0.1956 - val_loss: 0.0573 - val_mae: 0.1175 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0932 - mae: 0.1955 - val_loss: 0.0586 - val_mae: 0.1215 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0926 - mae: 0.1952 - val_loss: 0.0569 - val_mae: 0.1165 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0923 - mae: 0.1943 - val_loss: 0.0572 - val_mae: 0.1152 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0923 - mae: 0.1951 - val_loss: 0.0572 - val_mae: 0.1182 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0923 - mae: 0.1946 - val_loss: 0.0559 - val_mae: 0.1152 - learning_rate: 2.5000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0922 - mae: 0.1948 - val_loss: 0.0560 - val_mae: 0.1172 - learning_rate: 2.5000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0916 - mae: 0.1941 - val_loss: 0.0558 - val_mae: 0.1180 - learning_rate: 2.5000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0916 - mae: 0.1941 - val_loss: 0.0566 - val_mae: 0.1150 - learning_rate: 2.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0915 - mae: 0.1941 - val_loss: 0.0571 - val_mae: 0.1165 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0913 - mae: 0.1938 - val_loss: 0.0557 - val_mae: 0.1168 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0910 - mae: 0.1936 - val_loss: 0.0566 - val_mae: 0.1196 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0915 - mae: 0.1939 - val_loss: 0.0548 - val_mae: 0.1133 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0913 - mae: 0.1936 - val_loss: 0.0555 - val_mae: 0.1176 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0912 - mae: 0.1935 - val_loss: 0.0548 - val_mae: 0.1150 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0911 - mae: 0.1936 - val_loss: 0.0556 - val_mae: 0.1156 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0912 - mae: 0.1938 - val_loss: 0.0562 - val_mae: 0.1193 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m1116/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0903 - mae: 0.1926\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0903 - mae: 0.1926 - val_loss: 0.0556 - val_mae: 0.1171 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0902 - mae: 0.1924 - val_loss: 0.0546 - val_mae: 0.1152 - learning_rate: 1.2500e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0902 - mae: 0.1917 - val_loss: 0.0538 - val_mae: 0.1117 - learning_rate: 1.2500e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0895 - mae: 0.1914 - val_loss: 0.0544 - val_mae: 0.1130 - learning_rate: 1.2500e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0901 - mae: 0.1918 - val_loss: 0.0543 - val_mae: 0.1115 - learning_rate: 1.2500e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0901 - mae: 0.1916 - val_loss: 0.0549 - val_mae: 0.1148 - learning_rate: 1.2500e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0892 - mae: 0.1913 - val_loss: 0.0543 - val_mae: 0.1139 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m1107/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0892 - mae: 0.1914\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0893 - mae: 0.1914 - val_loss: 0.0543 - val_mae: 0.1128 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0893 - mae: 0.1908 - val_loss: 0.0537 - val_mae: 0.1120 - learning_rate: 6.2500e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0887 - mae: 0.1906 - val_loss: 0.0538 - val_mae: 0.1128 - learning_rate: 6.2500e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0887 - mae: 0.1901 - val_loss: 0.0530 - val_mae: 0.1109 - learning_rate: 6.2500e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0885 - mae: 0.1904 - val_loss: 0.0534 - val_mae: 0.1120 - learning_rate: 6.2500e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0886 - mae: 0.1902 - val_loss: 0.0538 - val_mae: 0.1123 - learning_rate: 6.2500e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0883 - mae: 0.1899 - val_loss: 0.0544 - val_mae: 0.1147 - learning_rate: 6.2500e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0880 - mae: 0.1897 - val_loss: 0.0532 - val_mae: 0.1113 - learning_rate: 6.2500e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m1102/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0873 - mae: 0.1892\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0873 - mae: 0.1892 - val_loss: 0.0537 - val_mae: 0.1116 - learning_rate: 6.2500e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0871 - mae: 0.1891 - val_loss: 0.0537 - val_mae: 0.1115 - learning_rate: 3.1250e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0873 - mae: 0.1891 - val_loss: 0.0531 - val_mae: 0.1112 - learning_rate: 3.1250e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0879 - mae: 0.1890 - val_loss: 0.0536 - val_mae: 0.1119 - learning_rate: 3.1250e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 89ms/step - loss: 0.0874 - mae: 0.1888 - val_loss: 0.0530 - val_mae: 0.1102 - learning_rate: 3.1250e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m1106/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0876 - mae: 0.1890\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0876 - mae: 0.1890 - val_loss: 0.0534 - val_mae: 0.1116 - learning_rate: 3.1250e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0872 - mae: 0.1884 - val_loss: 0.0532 - val_mae: 0.1115 - learning_rate: 1.5625e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0872 - mae: 0.1887 - val_loss: 0.0531 - val_mae: 0.1111 - learning_rate: 1.5625e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0872 - mae: 0.1885 - val_loss: 0.0529 - val_mae: 0.1111 - learning_rate: 1.5625e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0872 - mae: 0.1884 - val_loss: 0.0531 - val_mae: 0.1110 - learning_rate: 1.5625e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m1111/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0873 - mae: 0.1885\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0873 - mae: 0.1885 - val_loss: 0.0532 - val_mae: 0.1110 - learning_rate: 1.5625e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0871 - mae: 0.1883 - val_loss: 0.0531 - val_mae: 0.1112 - learning_rate: 7.8125e-06\n",
      "Epoch 74/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0870 - mae: 0.1881 - val_loss: 0.0530 - val_mae: 0.1109 - learning_rate: 7.8125e-06\n",
      "Epoch 75/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0871 - mae: 0.1883 - val_loss: 0.0530 - val_mae: 0.1111 - learning_rate: 7.8125e-06\n",
      "Epoch 76/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0865 - mae: 0.1880 - val_loss: 0.0532 - val_mae: 0.1114 - learning_rate: 7.8125e-06\n",
      "Epoch 77/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0871 - mae: 0.1885\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0871 - mae: 0.1885 - val_loss: 0.0531 - val_mae: 0.1114 - learning_rate: 7.8125e-06\n",
      "Epoch 78/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0864 - mae: 0.1878 - val_loss: 0.0530 - val_mae: 0.1111 - learning_rate: 3.9063e-06\n",
      "Epoch 79/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.0866 - mae: 0.1877 - val_loss: 0.0529 - val_mae: 0.1110 - learning_rate: 3.9063e-06\n",
      "Epoch 80/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0871 - mae: 0.1880 - val_loss: 0.0530 - val_mae: 0.1111 - learning_rate: 3.9063e-06\n",
      "Epoch 81/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - loss: 0.0870 - mae: 0.1884 - val_loss: 0.0530 - val_mae: 0.1108 - learning_rate: 3.9063e-06\n",
      "Epoch 82/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0871 - mae: 0.1880 - val_loss: 0.0530 - val_mae: 0.1109 - learning_rate: 3.9063e-06\n",
      "Epoch 83/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0868 - mae: 0.1881 - val_loss: 0.0531 - val_mae: 0.1107 - learning_rate: 3.9063e-06\n",
      "Epoch 84/200\n",
      "\u001b[1m1111/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0870 - mae: 0.1885\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0870 - mae: 0.1885 - val_loss: 0.0531 - val_mae: 0.1111 - learning_rate: 3.9063e-06\n",
      "Epoch 85/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0870 - mae: 0.1882 - val_loss: 0.0530 - val_mae: 0.1110 - learning_rate: 1.9531e-06\n",
      "Epoch 86/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0867 - mae: 0.1879 - val_loss: 0.0531 - val_mae: 0.1112 - learning_rate: 1.9531e-06\n",
      "Epoch 87/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0862 - mae: 0.1874 - val_loss: 0.0531 - val_mae: 0.1112 - learning_rate: 1.9531e-06\n",
      "Epoch 88/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0867 - mae: 0.1880 - val_loss: 0.0530 - val_mae: 0.1112 - learning_rate: 1.9531e-06\n",
      "Epoch 89/200\n",
      "\u001b[1m1106/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0863 - mae: 0.1882\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0864 - mae: 0.1882 - val_loss: 0.0530 - val_mae: 0.1108 - learning_rate: 1.9531e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse1 0.998024 3.403266e-10\n",
      "current_fall_time_pulse2 0.997805 3.562515e-10\n",
      "voltage_fall_time_pulse1 0.995632 1.181826e-10\n",
      "voltage_fall_time_pulse2 0.995628 1.182210e-10\n",
      "      undershoot_pulse_2 0.990147 1.210193e+00\n",
      "      undershoot_pulse_1 0.990016 1.214965e+00\n",
      "voltage_rise_time_pulse1 0.987355 1.593718e-10\n",
      "   ringing_frequency_MHz 0.970072 4.113790e-06\n",
      "       overshoot_pulse_1 0.962650 1.809418e+00\n",
      "current_rise_time_pulse1 0.947605 9.359465e-09\n",
      "       overshoot_pulse_2 0.925005 5.859627e+00\n",
      "current_rise_time_pulse2 0.813601 5.189486e-09\n",
      "voltage_rise_time_pulse2 0.727201 1.065312e-09\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse1 0.997900 3.517334e-10\n",
      "current_fall_time_pulse2 0.997690 3.662144e-10\n",
      "voltage_fall_time_pulse1 0.995834 1.158558e-10\n",
      "voltage_fall_time_pulse2 0.995759 1.168882e-10\n",
      "      undershoot_pulse_2 0.990119 1.203824e+00\n",
      "      undershoot_pulse_1 0.989702 1.225990e+00\n",
      "voltage_rise_time_pulse1 0.987226 1.608682e-10\n",
      "   ringing_frequency_MHz 0.969832 4.122966e-06\n",
      "       overshoot_pulse_1 0.962386 1.785566e+00\n",
      "current_rise_time_pulse1 0.950423 9.063365e-09\n",
      "       overshoot_pulse_2 0.925225 5.833544e+00\n",
      "current_rise_time_pulse2 0.814788 5.114235e-09\n",
      "voltage_rise_time_pulse2 0.721023 1.075430e-09\n",
      "\u001b[1m2627/2627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse1 0.998005 3.420620e-10\n",
      "current_fall_time_pulse2 0.997787 3.577637e-10\n",
      "voltage_fall_time_pulse1 0.995663 1.178365e-10\n",
      "voltage_fall_time_pulse2 0.995648 1.180220e-10\n",
      "      undershoot_pulse_2 0.990143 1.209239e+00\n",
      "      undershoot_pulse_1 0.989970 1.216625e+00\n",
      "voltage_rise_time_pulse1 0.987336 1.595971e-10\n",
      "   ringing_frequency_MHz 0.970036 4.115168e-06\n",
      "       overshoot_pulse_1 0.962612 1.805860e+00\n",
      "current_rise_time_pulse1 0.948026 9.315647e-09\n",
      "       overshoot_pulse_2 0.925039 5.855721e+00\n",
      "current_rise_time_pulse2 0.813777 5.178267e-09\n",
      "voltage_rise_time_pulse2 0.726277 1.066836e-09\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 964us/step\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse2 0.762690 4.896871e-09\n",
      "current_fall_time_pulse1 0.753562 4.960260e-09\n",
      "voltage_fall_time_pulse2 0.637226 1.286940e-09\n",
      "voltage_fall_time_pulse1 0.631934 1.294850e-09\n",
      "       overshoot_pulse_1 0.494030 1.016762e+01\n",
      "      undershoot_pulse_1 0.455131 6.327204e+00\n",
      "      undershoot_pulse_2 0.402003 6.476134e+00\n",
      "\u001b[1m2627/2627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 918us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 4: DROPOUT + L2 + LR SCHEDULER ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0025120D'\n",
    "\n",
    "BASE_DIR = \"fourth_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE ANN with DROPOUT + BN + L2 =================\n",
    "def build_ann(input_dim, output_dim, dropout=0.2, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop, lr_schedule], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/dropout_l2_scheduler_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL SCATTER PLOTS (TEST) ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='teal')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42388d67",
   "metadata": {},
   "source": [
    "## ITERATION - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5ff22b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.3994 - mae: 0.4354 - val_loss: 0.0878 - val_mae: 0.1569 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1347 - mae: 0.2395 - val_loss: 0.0784 - val_mae: 0.1466 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1203 - mae: 0.2246 - val_loss: 0.0721 - val_mae: 0.1408 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1137 - mae: 0.2193 - val_loss: 0.0706 - val_mae: 0.1430 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1101 - mae: 0.2163 - val_loss: 0.0688 - val_mae: 0.1393 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1081 - mae: 0.2144 - val_loss: 0.0679 - val_mae: 0.1361 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1070 - mae: 0.2129 - val_loss: 0.0668 - val_mae: 0.1372 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1055 - mae: 0.2109 - val_loss: 0.0711 - val_mae: 0.1486 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1044 - mae: 0.2094 - val_loss: 0.0657 - val_mae: 0.1348 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1042 - mae: 0.2089 - val_loss: 0.0651 - val_mae: 0.1322 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1037 - mae: 0.2079 - val_loss: 0.0671 - val_mae: 0.1349 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1030 - mae: 0.2073 - val_loss: 0.0643 - val_mae: 0.1337 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1027 - mae: 0.2063 - val_loss: 0.0663 - val_mae: 0.1340 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1016 - mae: 0.2057 - val_loss: 0.0682 - val_mae: 0.1430 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1024 - mae: 0.2059 - val_loss: 0.0642 - val_mae: 0.1329 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1017 - mae: 0.2047 - val_loss: 0.0652 - val_mae: 0.1360 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1013 - mae: 0.2044 - val_loss: 0.0642 - val_mae: 0.1312 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1013 - mae: 0.2042 - val_loss: 0.0625 - val_mae: 0.1267 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1006 - mae: 0.2036 - val_loss: 0.0645 - val_mae: 0.1338 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1004 - mae: 0.2032 - val_loss: 0.0647 - val_mae: 0.1355 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1005 - mae: 0.2033 - val_loss: 0.0629 - val_mae: 0.1303 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1001 - mae: 0.2030 - val_loss: 0.0646 - val_mae: 0.1353 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0998 - mae: 0.2022 - val_loss: 0.0624 - val_mae: 0.1289 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1001 - mae: 0.2024 - val_loss: 0.0626 - val_mae: 0.1301 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0997 - mae: 0.2020 - val_loss: 0.0649 - val_mae: 0.1358 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1000 - mae: 0.2025 - val_loss: 0.0623 - val_mae: 0.1273 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0989 - mae: 0.2013 - val_loss: 0.0627 - val_mae: 0.1282 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0993 - mae: 0.2016 - val_loss: 0.0631 - val_mae: 0.1340 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0992 - mae: 0.2017 - val_loss: 0.0617 - val_mae: 0.1281 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0993 - mae: 0.2015 - val_loss: 0.0659 - val_mae: 0.1352 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0990 - mae: 0.2009 - val_loss: 0.0628 - val_mae: 0.1304 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0992 - mae: 0.2013 - val_loss: 0.0648 - val_mae: 0.1360 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0994 - mae: 0.2012 - val_loss: 0.0645 - val_mae: 0.1364 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m1110/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0991 - mae: 0.2010\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0991 - mae: 0.2010 - val_loss: 0.0644 - val_mae: 0.1351 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0960 - mae: 0.1971 - val_loss: 0.0636 - val_mae: 0.1299 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0951 - mae: 0.1962 - val_loss: 0.0591 - val_mae: 0.1187 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0950 - mae: 0.1963 - val_loss: 0.0611 - val_mae: 0.1240 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0944 - mae: 0.1960 - val_loss: 0.0600 - val_mae: 0.1253 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0936 - mae: 0.1953 - val_loss: 0.0594 - val_mae: 0.1249 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 25ms/step - loss: 0.0948 - mae: 0.1966 - val_loss: 0.0599 - val_mae: 0.1241 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m1111/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0935 - mae: 0.1955\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.0935 - mae: 0.1955 - val_loss: 0.0626 - val_mae: 0.1259 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0928 - mae: 0.1936 - val_loss: 0.0564 - val_mae: 0.1158 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0921 - mae: 0.1930 - val_loss: 0.0575 - val_mae: 0.1186 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0915 - mae: 0.1929 - val_loss: 0.0564 - val_mae: 0.1158 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0909 - mae: 0.1923 - val_loss: 0.0567 - val_mae: 0.1164 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0914 - mae: 0.1927 - val_loss: 0.0579 - val_mae: 0.1208 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0909 - mae: 0.1923 - val_loss: 0.0558 - val_mae: 0.1169 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0909 - mae: 0.1923 - val_loss: 0.0570 - val_mae: 0.1194 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0908 - mae: 0.1921 - val_loss: 0.0559 - val_mae: 0.1170 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0907 - mae: 0.1918 - val_loss: 0.0562 - val_mae: 0.1150 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0906 - mae: 0.1920 - val_loss: 0.0558 - val_mae: 0.1161 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m1112/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0904 - mae: 0.1918\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0904 - mae: 0.1918 - val_loss: 0.0568 - val_mae: 0.1167 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0895 - mae: 0.1905 - val_loss: 0.0548 - val_mae: 0.1114 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0894 - mae: 0.1903 - val_loss: 0.0541 - val_mae: 0.1118 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0891 - mae: 0.1894 - val_loss: 0.0544 - val_mae: 0.1111 - learning_rate: 1.2500e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0883 - mae: 0.1896 - val_loss: 0.0547 - val_mae: 0.1124 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0887 - mae: 0.1894 - val_loss: 0.0542 - val_mae: 0.1121 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0888 - mae: 0.1899 - val_loss: 0.0543 - val_mae: 0.1122 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m1104/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0888 - mae: 0.1892\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0888 - mae: 0.1892 - val_loss: 0.0542 - val_mae: 0.1126 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0881 - mae: 0.1887 - val_loss: 0.0539 - val_mae: 0.1109 - learning_rate: 6.2500e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0880 - mae: 0.1884 - val_loss: 0.0543 - val_mae: 0.1112 - learning_rate: 6.2500e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0875 - mae: 0.1883 - val_loss: 0.0541 - val_mae: 0.1116 - learning_rate: 6.2500e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0878 - mae: 0.1881 - val_loss: 0.0540 - val_mae: 0.1119 - learning_rate: 6.2500e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0876 - mae: 0.1883 - val_loss: 0.0536 - val_mae: 0.1093 - learning_rate: 6.2500e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0873 - mae: 0.1879 - val_loss: 0.0542 - val_mae: 0.1126 - learning_rate: 6.2500e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0874 - mae: 0.1882 - val_loss: 0.0537 - val_mae: 0.1112 - learning_rate: 6.2500e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0873 - mae: 0.1878 - val_loss: 0.0543 - val_mae: 0.1119 - learning_rate: 6.2500e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0871 - mae: 0.1877 - val_loss: 0.0542 - val_mae: 0.1128 - learning_rate: 6.2500e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m1100/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0873 - mae: 0.1878\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0873 - mae: 0.1878 - val_loss: 0.0537 - val_mae: 0.1115 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0871 - mae: 0.1873 - val_loss: 0.0539 - val_mae: 0.1112 - learning_rate: 3.1250e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0867 - mae: 0.1872 - val_loss: 0.0538 - val_mae: 0.1117 - learning_rate: 3.1250e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0869 - mae: 0.1875 - val_loss: 0.0534 - val_mae: 0.1112 - learning_rate: 3.1250e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0870 - mae: 0.1872 - val_loss: 0.0531 - val_mae: 0.1098 - learning_rate: 3.1250e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0867 - mae: 0.1871 - val_loss: 0.0532 - val_mae: 0.1101 - learning_rate: 3.1250e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0869 - mae: 0.1872 - val_loss: 0.0532 - val_mae: 0.1105 - learning_rate: 3.1250e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0869 - mae: 0.1874 - val_loss: 0.0532 - val_mae: 0.1098 - learning_rate: 3.1250e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0869 - mae: 0.1874 - val_loss: 0.0533 - val_mae: 0.1096 - learning_rate: 3.1250e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m1104/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0868 - mae: 0.1875\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0868 - mae: 0.1875 - val_loss: 0.0535 - val_mae: 0.1122 - learning_rate: 3.1250e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0869 - mae: 0.1870 - val_loss: 0.0533 - val_mae: 0.1111 - learning_rate: 1.5625e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0866 - mae: 0.1867 - val_loss: 0.0532 - val_mae: 0.1104 - learning_rate: 1.5625e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0861 - mae: 0.1864 - val_loss: 0.0534 - val_mae: 0.1102 - learning_rate: 1.5625e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0864 - mae: 0.1867 - val_loss: 0.0533 - val_mae: 0.1110 - learning_rate: 1.5625e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0861 - mae: 0.1862\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1117/1117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0861 - mae: 0.1862 - val_loss: 0.0533 - val_mae: 0.1114 - learning_rate: 1.5625e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step  \n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse1 0.998211 3.238791e-10\n",
      "current_fall_time_pulse2 0.998126 3.291592e-10\n",
      "voltage_fall_time_pulse1 0.995686 1.174541e-10\n",
      "voltage_fall_time_pulse2 0.995679 1.175339e-10\n",
      "      undershoot_pulse_2 0.990060 1.215536e+00\n",
      "      undershoot_pulse_1 0.989934 1.219964e+00\n",
      "   ringing_frequency_MHz 0.989704 2.231612e+00\n",
      "voltage_rise_time_pulse1 0.987426 1.589193e-10\n",
      "       overshoot_pulse_1 0.962610 1.810392e+00\n",
      "current_rise_time_pulse1 0.947821 9.340172e-09\n",
      "       overshoot_pulse_2 0.925218 5.851290e+00\n",
      "current_rise_time_pulse2 0.812061 5.210873e-09\n",
      "voltage_rise_time_pulse2 0.726974 1.065753e-09\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse1 0.998138 3.312043e-10\n",
      "current_fall_time_pulse2 0.998070 3.347482e-10\n",
      "voltage_fall_time_pulse1 0.996052 1.127827e-10\n",
      "voltage_fall_time_pulse2 0.995996 1.135717e-10\n",
      "      undershoot_pulse_2 0.990034 1.208969e+00\n",
      "   ringing_frequency_MHz 0.990024 2.193980e+00\n",
      "      undershoot_pulse_1 0.989650 1.229119e+00\n",
      "voltage_rise_time_pulse1 0.987144 1.613858e-10\n",
      "       overshoot_pulse_1 0.962255 1.788672e+00\n",
      "current_rise_time_pulse1 0.950730 9.035233e-09\n",
      "       overshoot_pulse_2 0.925277 5.831542e+00\n",
      "current_rise_time_pulse2 0.814533 5.117755e-09\n",
      "voltage_rise_time_pulse2 0.720623 1.076202e-09\n",
      "\u001b[1m2627/2627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 990us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse1 0.998200 3.249885e-10\n",
      "current_fall_time_pulse2 0.998117 3.300036e-10\n",
      "voltage_fall_time_pulse1 0.995741 1.167653e-10\n",
      "voltage_fall_time_pulse2 0.995727 1.169481e-10\n",
      "      undershoot_pulse_2 0.990056 1.214553e+00\n",
      "      undershoot_pulse_1 0.989892 1.221341e+00\n",
      "   ringing_frequency_MHz 0.989752 2.226008e+00\n",
      "voltage_rise_time_pulse1 0.987384 1.592917e-10\n",
      "       overshoot_pulse_1 0.962558 1.807151e+00\n",
      "current_rise_time_pulse1 0.948255 9.295066e-09\n",
      "       overshoot_pulse_2 0.925228 5.848332e+00\n",
      "current_rise_time_pulse2 0.812426 5.197010e-09\n",
      "voltage_rise_time_pulse2 0.726025 1.067327e-09\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 958us/step\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse2 0.933830 2.585779e-09\n",
      "current_fall_time_pulse1 0.926310 2.712414e-09\n",
      "       overshoot_pulse_1 0.597691 9.066449e+00\n",
      "      undershoot_pulse_1 0.470738 6.235931e+00\n",
      "      undershoot_pulse_2 0.406574 6.451337e+00\n",
      "\u001b[1m2627/2627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 936us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 5: PHYSICS FEATURES + ALL REGULARIZATION ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0025120D'\n",
    "BASE_DIR = \"fifth_iteration\"\n",
    "\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "seen_parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in seen_parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED PHYSICS FEATURES ==============\n",
    "def compute_physics_features(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    f_resonance = 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6 if L_eq > 0 and C_eq > 0 else 0\n",
    "    overshoot_est = row[\"VDS_max\"] - row[\"Vbus\"]\n",
    "    undershoot_est = 0 - row[\"VGS_th_min\"]\n",
    "    dVdt_est = row[\"VDS_max\"] / row[\"Tp1\"] if row[\"Tp1\"] != 0 else 0\n",
    "    dIdt_est = row[\"ID_max_25C\"] / row[\"Tp1\"] if row[\"Tp1\"] != 0 else 0\n",
    "    return pd.Series([f_resonance, overshoot_est, undershoot_est, dVdt_est, dIdt_est])\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_[['f_resonance', 'overshoot_est', 'undershoot_est', 'dVdt_est', 'dIdt_est']] = df_.apply(compute_physics_features, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE ==============\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(pd.concat([train_df['Part_Number'], test_df['Part_Number']]))\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "] + ['f_resonance', 'overshoot_est', 'undershoot_est', 'dVdt_est', 'dIdt_est', 'Part_encoded']\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE ANN =================\n",
    "def build_ann(input_dim, output_dim, dropout=0.2, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(128, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAINING ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop, lr_schedule], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/iteration5_final_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL SCATTER PLOTS ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='darkblue')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
