{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b767650",
   "metadata": {},
   "source": [
    "# Baseline ANN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259d3a8",
   "metadata": {},
   "source": [
    "Here we are Sampling from the complete dataset to 25% from each unique simulation combination of:\n",
    "\n",
    "\n",
    "['Vbus', 'Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9fd618",
   "metadata": {},
   "source": [
    "| Iteration | Dropout | L2 Reg. | BatchNorm | EarlyStopping | LR Scheduler      | Scaling                              | Physics Features | Notes                               |\n",
    "| --------- | ------- | ------- | --------- | ------------- | ----------------- | ------------------------------------ | ---------------- | ----------------------------------- |\n",
    "| **1**     | —       | —       | —         | —             | —                 | StdScaler (X,y)                      | No               | Pure vanilla ANN                    |\n",
    "| **2**     | 0.2     | —       | —         | Yes           | —                 | StdScaler (X,y)                      | No               | Added dropout + early stop          |\n",
    "| **3**     | 0.2     | 1e-4    | —         | Yes           | —                 | StdScaler (X,y)                      | No               | Best baseline (good generalisation) |\n",
    "| **4**     | 0.2     | 1e-4    | —         | Yes           | ReduceLROnPlateau | StdScaler (X) + Per-target y scaling | No               | LR scheduling added                 |\n",
    "| **5**     | 0.2     | 1e-4    | Yes       | Yes           | ReduceLROnPlateau | StdScaler (X) + Ringing→MinMax       | Yes              | Physics features added              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f915d40",
   "metadata": {},
   "source": [
    "## ITERATION - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006cd6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.2624 - mae: 0.3301 - val_loss: 0.0934 - val_mae: 0.1544\n",
      "Epoch 2/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1239 - mae: 0.2124 - val_loss: 0.0853 - val_mae: 0.1490\n",
      "Epoch 3/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1146 - mae: 0.2013 - val_loss: 0.0805 - val_mae: 0.1399\n",
      "Epoch 4/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1089 - mae: 0.1962 - val_loss: 0.0780 - val_mae: 0.1407\n",
      "Epoch 5/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1050 - mae: 0.1932 - val_loss: 0.0757 - val_mae: 0.1340\n",
      "Epoch 6/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1036 - mae: 0.1910 - val_loss: 0.0752 - val_mae: 0.1312\n",
      "Epoch 7/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1026 - mae: 0.1898 - val_loss: 0.0751 - val_mae: 0.1402\n",
      "Epoch 8/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1015 - mae: 0.1886 - val_loss: 0.0760 - val_mae: 0.1350\n",
      "Epoch 9/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1008 - mae: 0.1878 - val_loss: 0.0737 - val_mae: 0.1311\n",
      "Epoch 10/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1011 - mae: 0.1873 - val_loss: 0.0765 - val_mae: 0.1375\n",
      "Epoch 11/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1001 - mae: 0.1869 - val_loss: 0.0724 - val_mae: 0.1310\n",
      "Epoch 12/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0993 - mae: 0.1868 - val_loss: 0.0760 - val_mae: 0.1363\n",
      "Epoch 13/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1000 - mae: 0.1867 - val_loss: 0.0721 - val_mae: 0.1309\n",
      "Epoch 14/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0998 - mae: 0.1867 - val_loss: 0.0724 - val_mae: 0.1324\n",
      "Epoch 15/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0998 - mae: 0.1864 - val_loss: 0.0754 - val_mae: 0.1364\n",
      "Epoch 16/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1004 - mae: 0.1868 - val_loss: 0.0725 - val_mae: 0.1318\n",
      "Epoch 17/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0985 - mae: 0.1855 - val_loss: 0.0724 - val_mae: 0.1337\n",
      "Epoch 18/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0983 - mae: 0.1854 - val_loss: 0.0739 - val_mae: 0.1379\n",
      "Epoch 19/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0996 - mae: 0.1859 - val_loss: 0.0727 - val_mae: 0.1317\n",
      "Epoch 20/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0987 - mae: 0.1856 - val_loss: 0.0729 - val_mae: 0.1325\n",
      "Epoch 21/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0978 - mae: 0.1850 - val_loss: 0.0718 - val_mae: 0.1303\n",
      "Epoch 22/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0977 - mae: 0.1846 - val_loss: 0.0713 - val_mae: 0.1314\n",
      "Epoch 23/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0987 - mae: 0.1853 - val_loss: 0.0730 - val_mae: 0.1317\n",
      "Epoch 24/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0982 - mae: 0.1852 - val_loss: 0.0739 - val_mae: 0.1393\n",
      "Epoch 25/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0984 - mae: 0.1850 - val_loss: 0.0703 - val_mae: 0.1267\n",
      "Epoch 26/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0982 - mae: 0.1843 - val_loss: 0.0722 - val_mae: 0.1299\n",
      "Epoch 27/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0974 - mae: 0.1843 - val_loss: 0.0713 - val_mae: 0.1306\n",
      "Epoch 28/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0982 - mae: 0.1853 - val_loss: 0.0719 - val_mae: 0.1349\n",
      "Epoch 29/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0974 - mae: 0.1845 - val_loss: 0.0707 - val_mae: 0.1278\n",
      "Epoch 30/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0971 - mae: 0.1838 - val_loss: 0.0720 - val_mae: 0.1327\n",
      "Epoch 31/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0972 - mae: 0.1845 - val_loss: 0.0709 - val_mae: 0.1267\n",
      "Epoch 32/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0960 - mae: 0.1836 - val_loss: 0.0741 - val_mae: 0.1339\n",
      "Epoch 33/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0987 - mae: 0.1845 - val_loss: 0.0702 - val_mae: 0.1264\n",
      "Epoch 34/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0975 - mae: 0.1844 - val_loss: 0.0726 - val_mae: 0.1304\n",
      "Epoch 35/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0977 - mae: 0.1843 - val_loss: 0.0728 - val_mae: 0.1314\n",
      "Epoch 36/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0974 - mae: 0.1843 - val_loss: 0.0704 - val_mae: 0.1293\n",
      "Epoch 37/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0970 - mae: 0.1842 - val_loss: 0.0708 - val_mae: 0.1315\n",
      "Epoch 38/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0973 - mae: 0.1840 - val_loss: 0.0726 - val_mae: 0.1336\n",
      "Epoch 39/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0978 - mae: 0.1841 - val_loss: 0.0709 - val_mae: 0.1277\n",
      "Epoch 40/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0968 - mae: 0.1837 - val_loss: 0.0715 - val_mae: 0.1300\n",
      "Epoch 41/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0960 - mae: 0.1829 - val_loss: 0.0700 - val_mae: 0.1262\n",
      "Epoch 42/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0965 - mae: 0.1830 - val_loss: 0.0715 - val_mae: 0.1320\n",
      "Epoch 43/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0970 - mae: 0.1835 - val_loss: 0.0713 - val_mae: 0.1288\n",
      "Epoch 44/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0961 - mae: 0.1835 - val_loss: 0.0712 - val_mae: 0.1315\n",
      "Epoch 45/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0976 - mae: 0.1844 - val_loss: 0.0709 - val_mae: 0.1266\n",
      "Epoch 46/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0976 - mae: 0.1843 - val_loss: 0.0709 - val_mae: 0.1293\n",
      "Epoch 47/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0968 - mae: 0.1835 - val_loss: 0.0721 - val_mae: 0.1354\n",
      "Epoch 48/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0956 - mae: 0.1831 - val_loss: 0.0717 - val_mae: 0.1310\n",
      "Epoch 49/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0971 - mae: 0.1839 - val_loss: 0.0703 - val_mae: 0.1273\n",
      "Epoch 50/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0966 - mae: 0.1834 - val_loss: 0.0717 - val_mae: 0.1276\n",
      "Epoch 51/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0959 - mae: 0.1829 - val_loss: 0.0744 - val_mae: 0.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2294/2294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 734us/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.996887 3.358734e-10\n",
      "voltage_fall_time_pulse1 0.996730 3.446123e-10\n",
      "current_fall_time_pulse1 0.994112 9.425589e-10\n",
      "current_fall_time_pulse2 0.994040 9.428344e-10\n",
      "voltage_rise_time_pulse1 0.976340 5.327053e-10\n",
      "   ringing_frequency_MHz 0.974398 2.713996e-06\n",
      "      undershoot_pulse_2 0.971263 1.845545e+00\n",
      "       overshoot_pulse_1 0.969172 2.369116e+00\n",
      "      undershoot_pulse_1 0.968585 1.934708e+00\n",
      "voltage_rise_time_pulse2 0.895608 1.422548e-09\n",
      "current_rise_time_pulse2 0.874312 9.646222e-09\n",
      "       overshoot_pulse_2 0.869635 7.144922e+00\n",
      "current_rise_time_pulse1 0.670563 1.359558e-08\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.996730 3.431592e-10\n",
      "voltage_fall_time_pulse1 0.996633 3.486319e-10\n",
      "current_fall_time_pulse2 0.994074 9.389882e-10\n",
      "current_fall_time_pulse1 0.994055 9.463807e-10\n",
      "voltage_rise_time_pulse1 0.975662 5.395026e-10\n",
      "   ringing_frequency_MHz 0.974432 2.694088e-06\n",
      "      undershoot_pulse_2 0.971105 1.850630e+00\n",
      "      undershoot_pulse_1 0.968830 1.927908e+00\n",
      "       overshoot_pulse_1 0.968200 2.372725e+00\n",
      "voltage_rise_time_pulse2 0.885361 1.460109e-09\n",
      "current_rise_time_pulse2 0.871469 9.706436e-09\n",
      "       overshoot_pulse_2 0.868670 7.170887e+00\n",
      "current_rise_time_pulse1 0.666246 1.357433e-08\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 736us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.996864 3.369764e-10\n",
      "voltage_fall_time_pulse1 0.996716 3.452182e-10\n",
      "current_fall_time_pulse1 0.994104 9.431332e-10\n",
      "current_fall_time_pulse2 0.994045 9.422584e-10\n",
      "voltage_rise_time_pulse1 0.976239 5.337304e-10\n",
      "   ringing_frequency_MHz 0.974403 2.711019e-06\n",
      "      undershoot_pulse_2 0.971239 1.846309e+00\n",
      "       overshoot_pulse_1 0.969030 2.369658e+00\n",
      "      undershoot_pulse_1 0.968622 1.933689e+00\n",
      "voltage_rise_time_pulse2 0.894124 1.428245e-09\n",
      "current_rise_time_pulse2 0.873889 9.655278e-09\n",
      "       overshoot_pulse_2 0.869490 7.148823e+00\n",
      "current_rise_time_pulse1 0.669928 1.359240e-08\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step\n",
      "                  Target       R2         RMSE\n",
      "      undershoot_pulse_1 0.953437 2.907788e+00\n",
      "      undershoot_pulse_2 0.953202 2.921652e+00\n",
      "current_rise_time_pulse2 0.721766 6.112667e-09\n",
      "current_fall_time_pulse1 0.286781 5.333211e-09\n",
      "current_fall_time_pulse2 0.214297 5.508859e-09\n",
      "       overshoot_pulse_1 0.201673 6.769658e+00\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 1: BASELINE ANN WITH PART_NUMBER GENERALIZATION ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0280120D'\n",
    "\n",
    "BASE_DIR = \"first_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent_balanced.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")  # Add encoded device as feature\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE BASELINE ANN ==============\n",
    "def build_ann(input_dim, output_dim, dropout=0.2, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/baseline_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL (INTERNAL TEST) SCATTER PLOTS ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='lightblue')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f7221",
   "metadata": {},
   "source": [
    "## ITERATION - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c1afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1634 - mae: 0.2348 - val_loss: 0.0746 - val_mae: 0.1431\n",
      "Epoch 2/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0696 - mae: 0.1345 - val_loss: 0.0657 - val_mae: 0.1268\n",
      "Epoch 3/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0639 - mae: 0.1233 - val_loss: 0.0617 - val_mae: 0.1185\n",
      "Epoch 4/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0622 - mae: 0.1191 - val_loss: 0.0601 - val_mae: 0.1151\n",
      "Epoch 5/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0607 - mae: 0.1159 - val_loss: 0.0625 - val_mae: 0.1172\n",
      "Epoch 6/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0590 - mae: 0.1131 - val_loss: 0.0601 - val_mae: 0.1131\n",
      "Epoch 7/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0583 - mae: 0.1121 - val_loss: 0.0591 - val_mae: 0.1134\n",
      "Epoch 8/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0580 - mae: 0.1112 - val_loss: 0.0585 - val_mae: 0.1086\n",
      "Epoch 9/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0570 - mae: 0.1098 - val_loss: 0.0590 - val_mae: 0.1115\n",
      "Epoch 10/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0580 - mae: 0.1098 - val_loss: 0.0574 - val_mae: 0.1076\n",
      "Epoch 11/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0570 - mae: 0.1085 - val_loss: 0.0570 - val_mae: 0.1088\n",
      "Epoch 12/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0563 - mae: 0.1079 - val_loss: 0.0580 - val_mae: 0.1096\n",
      "Epoch 13/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0562 - mae: 0.1077 - val_loss: 0.0568 - val_mae: 0.1084\n",
      "Epoch 14/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0564 - mae: 0.1078 - val_loss: 0.0603 - val_mae: 0.1098\n",
      "Epoch 15/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0562 - mae: 0.1073 - val_loss: 0.0574 - val_mae: 0.1069\n",
      "Epoch 16/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0552 - mae: 0.1064 - val_loss: 0.0578 - val_mae: 0.1075\n",
      "Epoch 17/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0543 - mae: 0.1055 - val_loss: 0.0571 - val_mae: 0.1092\n",
      "Epoch 18/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0545 - mae: 0.1057 - val_loss: 0.0575 - val_mae: 0.1088\n",
      "Epoch 19/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0554 - mae: 0.1058 - val_loss: 0.0571 - val_mae: 0.1063\n",
      "Epoch 20/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0547 - mae: 0.1054 - val_loss: 0.0557 - val_mae: 0.1054\n",
      "Epoch 21/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0550 - mae: 0.1054 - val_loss: 0.0560 - val_mae: 0.1058\n",
      "Epoch 22/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0550 - mae: 0.1054 - val_loss: 0.0568 - val_mae: 0.1046\n",
      "Epoch 23/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0543 - mae: 0.1047 - val_loss: 0.0560 - val_mae: 0.1048\n",
      "Epoch 24/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0539 - mae: 0.1043 - val_loss: 0.0561 - val_mae: 0.1046\n",
      "Epoch 25/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0547 - mae: 0.1044 - val_loss: 0.0568 - val_mae: 0.1061\n",
      "Epoch 26/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0539 - mae: 0.1040 - val_loss: 0.0561 - val_mae: 0.1047\n",
      "Epoch 27/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0539 - mae: 0.1038 - val_loss: 0.0563 - val_mae: 0.1062\n",
      "Epoch 28/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0539 - mae: 0.1037 - val_loss: 0.0562 - val_mae: 0.1052\n",
      "Epoch 29/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0538 - mae: 0.1037 - val_loss: 0.0568 - val_mae: 0.1086\n",
      "Epoch 30/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0536 - mae: 0.1034 - val_loss: 0.0578 - val_mae: 0.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2294/2294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 637us/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.999015 1.889026e-10\n",
      "voltage_fall_time_pulse1 0.999006 1.900073e-10\n",
      "current_fall_time_pulse2 0.998493 4.741788e-10\n",
      "current_fall_time_pulse1 0.998469 4.806277e-10\n",
      "   ringing_frequency_MHz 0.995367 1.154531e-06\n",
      "voltage_rise_time_pulse1 0.989127 3.611277e-10\n",
      "       overshoot_pulse_1 0.981652 1.827739e+00\n",
      "      undershoot_pulse_2 0.979909 1.543133e+00\n",
      "      undershoot_pulse_1 0.977799 1.626431e+00\n",
      "voltage_rise_time_pulse2 0.912729 1.300669e-09\n",
      "current_rise_time_pulse2 0.899569 8.622724e-09\n",
      "       overshoot_pulse_2 0.883233 6.762028e+00\n",
      "current_rise_time_pulse1 0.689346 1.320231e-08\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998994 1.903870e-10\n",
      "voltage_fall_time_pulse1 0.998973 1.925826e-10\n",
      "current_fall_time_pulse2 0.998437 4.821769e-10\n",
      "current_fall_time_pulse1 0.998422 4.875398e-10\n",
      "   ringing_frequency_MHz 0.995320 1.152645e-06\n",
      "voltage_rise_time_pulse1 0.988617 3.689533e-10\n",
      "       overshoot_pulse_1 0.980733 1.846883e+00\n",
      "      undershoot_pulse_2 0.978629 1.591556e+00\n",
      "      undershoot_pulse_1 0.975938 1.693875e+00\n",
      "current_rise_time_pulse2 0.895257 8.762306e-09\n",
      "voltage_rise_time_pulse2 0.892433 1.414353e-09\n",
      "       overshoot_pulse_2 0.881882 6.800620e+00\n",
      "current_rise_time_pulse1 0.678896 1.331460e-08\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 678us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.999012 1.891260e-10\n",
      "voltage_fall_time_pulse1 0.999001 1.903959e-10\n",
      "current_fall_time_pulse2 0.998484 4.753871e-10\n",
      "current_fall_time_pulse1 0.998462 4.816709e-10\n",
      "   ringing_frequency_MHz 0.995360 1.154248e-06\n",
      "voltage_rise_time_pulse1 0.989050 3.623124e-10\n",
      "       overshoot_pulse_1 0.981517 1.830624e+00\n",
      "      undershoot_pulse_2 0.979717 1.550494e+00\n",
      "      undershoot_pulse_1 0.977519 1.636725e+00\n",
      "voltage_rise_time_pulse2 0.909791 1.318348e-09\n",
      "current_rise_time_pulse2 0.898928 8.643806e-09\n",
      "       overshoot_pulse_2 0.883030 6.767831e+00\n",
      "current_rise_time_pulse1 0.687803 1.321922e-08\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750us/step\n",
      "                  Target       R2         RMSE\n",
      "       overshoot_pulse_1 0.611286 4.723799e+00\n",
      "voltage_rise_time_pulse2 0.529896 1.442282e-09\n",
      "       overshoot_pulse_2 0.194025 1.259736e+01\n",
      "current_rise_time_pulse2 0.178204 1.050529e-08\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 729us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 2: NO REGULARIZATION ANN ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0280120D'\n",
    "\n",
    "BASE_DIR = \"second_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent_balanced.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")  # Add encoded device as feature\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE NO REGULARIZATION ANN ==============\n",
    "def build_ann(input_dim, output_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/no_regularization_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL (INTERNAL TEST) SCATTER PLOTS ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='orange')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281b869",
   "metadata": {},
   "source": [
    "## ITERATION - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd31de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.2391 - mae: 0.3007 - val_loss: 0.0960 - val_mae: 0.1617\n",
      "Epoch 2/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1126 - mae: 0.1927 - val_loss: 0.0983 - val_mae: 0.1678\n",
      "Epoch 3/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1017 - mae: 0.1806 - val_loss: 0.0813 - val_mae: 0.1419\n",
      "Epoch 4/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0959 - mae: 0.1711 - val_loss: 0.0808 - val_mae: 0.1461\n",
      "Epoch 5/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0920 - mae: 0.1664 - val_loss: 0.0776 - val_mae: 0.1502\n",
      "Epoch 6/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0880 - mae: 0.1609 - val_loss: 0.0783 - val_mae: 0.1410\n",
      "Epoch 7/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0880 - mae: 0.1604 - val_loss: 0.0712 - val_mae: 0.1261\n",
      "Epoch 8/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0850 - mae: 0.1582 - val_loss: 0.0761 - val_mae: 0.1419\n",
      "Epoch 9/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0852 - mae: 0.1576 - val_loss: 0.0716 - val_mae: 0.1333\n",
      "Epoch 10/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0844 - mae: 0.1572 - val_loss: 0.0736 - val_mae: 0.1376\n",
      "Epoch 11/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0838 - mae: 0.1557 - val_loss: 0.0722 - val_mae: 0.1320\n",
      "Epoch 12/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0834 - mae: 0.1554 - val_loss: 0.0700 - val_mae: 0.1290\n",
      "Epoch 13/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0822 - mae: 0.1526 - val_loss: 0.0698 - val_mae: 0.1274\n",
      "Epoch 14/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0830 - mae: 0.1536 - val_loss: 0.0700 - val_mae: 0.1284\n",
      "Epoch 15/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0804 - mae: 0.1508 - val_loss: 0.0727 - val_mae: 0.1335\n",
      "Epoch 16/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0807 - mae: 0.1507 - val_loss: 0.0699 - val_mae: 0.1287\n",
      "Epoch 17/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0801 - mae: 0.1508 - val_loss: 0.0703 - val_mae: 0.1343\n",
      "Epoch 18/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0812 - mae: 0.1525 - val_loss: 0.0691 - val_mae: 0.1277\n",
      "Epoch 19/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0795 - mae: 0.1493 - val_loss: 0.0692 - val_mae: 0.1270\n",
      "Epoch 20/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0788 - mae: 0.1471 - val_loss: 0.0716 - val_mae: 0.1387\n",
      "Epoch 21/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0783 - mae: 0.1490 - val_loss: 0.0695 - val_mae: 0.1276\n",
      "Epoch 22/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0785 - mae: 0.1464 - val_loss: 0.0708 - val_mae: 0.1256\n",
      "Epoch 23/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0780 - mae: 0.1477 - val_loss: 0.0711 - val_mae: 0.1289\n",
      "Epoch 24/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0775 - mae: 0.1453 - val_loss: 0.0683 - val_mae: 0.1247\n",
      "Epoch 25/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0764 - mae: 0.1436 - val_loss: 0.0702 - val_mae: 0.1288\n",
      "Epoch 26/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0782 - mae: 0.1458 - val_loss: 0.0719 - val_mae: 0.1354\n",
      "Epoch 27/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0765 - mae: 0.1435 - val_loss: 0.0659 - val_mae: 0.1209\n",
      "Epoch 28/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0759 - mae: 0.1434 - val_loss: 0.0672 - val_mae: 0.1247\n",
      "Epoch 29/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0757 - mae: 0.1421 - val_loss: 0.0688 - val_mae: 0.1277\n",
      "Epoch 30/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0758 - mae: 0.1410 - val_loss: 0.0676 - val_mae: 0.1244\n",
      "Epoch 31/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0742 - mae: 0.1409 - val_loss: 0.0676 - val_mae: 0.1254\n",
      "Epoch 32/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0747 - mae: 0.1400 - val_loss: 0.0652 - val_mae: 0.1178\n",
      "Epoch 33/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0747 - mae: 0.1390 - val_loss: 0.0688 - val_mae: 0.1307\n",
      "Epoch 34/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0736 - mae: 0.1391 - val_loss: 0.0655 - val_mae: 0.1198\n",
      "Epoch 35/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0749 - mae: 0.1390 - val_loss: 0.0652 - val_mae: 0.1165\n",
      "Epoch 36/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0741 - mae: 0.1379 - val_loss: 0.0661 - val_mae: 0.1230\n",
      "Epoch 37/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0747 - mae: 0.1394 - val_loss: 0.0673 - val_mae: 0.1234\n",
      "Epoch 38/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0736 - mae: 0.1372 - val_loss: 0.0668 - val_mae: 0.1206\n",
      "Epoch 39/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0723 - mae: 0.1359 - val_loss: 0.0695 - val_mae: 0.1284\n",
      "Epoch 40/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0744 - mae: 0.1380 - val_loss: 0.0685 - val_mae: 0.1268\n",
      "Epoch 41/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0733 - mae: 0.1363 - val_loss: 0.0662 - val_mae: 0.1214\n",
      "Epoch 42/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0736 - mae: 0.1364 - val_loss: 0.0662 - val_mae: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2294/2294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 750us/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse1 0.998150 2.591866e-10\n",
      "voltage_fall_time_pulse2 0.998090 2.630559e-10\n",
      "current_fall_time_pulse2 0.997176 6.490208e-10\n",
      "current_fall_time_pulse1 0.997121 6.591136e-10\n",
      "   ringing_frequency_MHz 0.994104 1.302421e-06\n",
      "voltage_rise_time_pulse1 0.980979 4.776322e-10\n",
      "       overshoot_pulse_1 0.977588 2.020023e+00\n",
      "      undershoot_pulse_2 0.971257 1.845735e+00\n",
      "      undershoot_pulse_1 0.969922 1.893093e+00\n",
      "voltage_rise_time_pulse2 0.890344 1.457968e-09\n",
      "current_rise_time_pulse2 0.885950 9.188802e-09\n",
      "       overshoot_pulse_2 0.877887 6.915084e+00\n",
      "current_rise_time_pulse1 0.678903 1.342240e-08\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse1 0.998172 2.568711e-10\n",
      "voltage_fall_time_pulse2 0.998109 2.609907e-10\n",
      "current_fall_time_pulse1 0.997156 6.546183e-10\n",
      "current_fall_time_pulse2 0.997151 6.511016e-10\n",
      "   ringing_frequency_MHz 0.993991 1.306073e-06\n",
      "voltage_rise_time_pulse1 0.980229 4.862538e-10\n",
      "       overshoot_pulse_1 0.977438 1.998604e+00\n",
      "      undershoot_pulse_2 0.970686 1.863991e+00\n",
      "      undershoot_pulse_1 0.969083 1.920066e+00\n",
      "current_rise_time_pulse2 0.884179 9.214029e-09\n",
      "voltage_rise_time_pulse2 0.877351 1.510260e-09\n",
      "       overshoot_pulse_2 0.876878 6.943178e+00\n",
      "current_rise_time_pulse1 0.675487 1.338509e-08\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 799us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse1 0.998154 2.588406e-10\n",
      "voltage_fall_time_pulse2 0.998093 2.627471e-10\n",
      "current_fall_time_pulse2 0.997172 6.493334e-10\n",
      "current_fall_time_pulse1 0.997126 6.584412e-10\n",
      "   ringing_frequency_MHz 0.994087 1.302969e-06\n",
      "voltage_rise_time_pulse1 0.980867 4.789354e-10\n",
      "       overshoot_pulse_1 0.977566 2.016825e+00\n",
      "      undershoot_pulse_2 0.971171 1.848485e+00\n",
      "      undershoot_pulse_1 0.969796 1.897164e+00\n",
      "voltage_rise_time_pulse2 0.888463 1.465932e-09\n",
      "current_rise_time_pulse2 0.885686 9.192591e-09\n",
      "       overshoot_pulse_2 0.877736 6.919305e+00\n",
      "current_rise_time_pulse1 0.678401 1.341681e-08\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step\n",
      "                  Target       R2         RMSE\n",
      "      undershoot_pulse_1 0.935720 3.416485e+00\n",
      "      undershoot_pulse_2 0.934225 3.463726e+00\n",
      "       overshoot_pulse_1 0.794362 3.435802e+00\n",
      "voltage_fall_time_pulse2 0.724669 3.423995e-10\n",
      "current_fall_time_pulse1 0.710185 3.399679e-09\n",
      "voltage_fall_time_pulse1 0.687887 3.622375e-10\n",
      "current_fall_time_pulse2 0.604792 3.907018e-09\n",
      "voltage_rise_time_pulse2 0.572241 1.375792e-09\n",
      "current_rise_time_pulse2 0.493137 8.250329e-09\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 696us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 3: BATCHNORM + L2, NO DROPOUT ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0280120D'\n",
    "\n",
    "BASE_DIR = \"third_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent_balanced.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE ANN with BATCHNORM + L2 =================\n",
    "def build_ann(input_dim, output_dim, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/bn_l2_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL SCATTER PLOTS (TEST) ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='purple')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5b0ca",
   "metadata": {},
   "source": [
    "## ITERATION - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb77ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.3545 - mae: 0.4059 - val_loss: 0.0737 - val_mae: 0.1372 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1260 - mae: 0.2258 - val_loss: 0.0609 - val_mae: 0.1303 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1051 - mae: 0.2063 - val_loss: 0.0572 - val_mae: 0.1311 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1002 - mae: 0.2029 - val_loss: 0.0519 - val_mae: 0.1189 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0943 - mae: 0.1965 - val_loss: 0.0494 - val_mae: 0.1130 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0897 - mae: 0.1910 - val_loss: 0.0476 - val_mae: 0.1171 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0899 - mae: 0.1912 - val_loss: 0.0505 - val_mae: 0.1217 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0874 - mae: 0.1877 - val_loss: 0.0453 - val_mae: 0.1102 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0873 - mae: 0.1872 - val_loss: 0.0476 - val_mae: 0.1118 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0878 - mae: 0.1870 - val_loss: 0.0459 - val_mae: 0.1112 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0864 - mae: 0.1853 - val_loss: 0.0477 - val_mae: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0862 - mae: 0.1845 - val_loss: 0.0459 - val_mae: 0.1114 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m1141/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0871 - mae: 0.1852\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0871 - mae: 0.1852 - val_loss: 0.0474 - val_mae: 0.1087 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0824 - mae: 0.1791 - val_loss: 0.0435 - val_mae: 0.1088 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0810 - mae: 0.1783 - val_loss: 0.0419 - val_mae: 0.1031 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0815 - mae: 0.1787 - val_loss: 0.0442 - val_mae: 0.1147 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0791 - mae: 0.1767 - val_loss: 0.0435 - val_mae: 0.1090 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0792 - mae: 0.1769 - val_loss: 0.0422 - val_mae: 0.1049 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0790 - mae: 0.1762 - val_loss: 0.0434 - val_mae: 0.1057 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m1122/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0787 - mae: 0.1762\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0787 - mae: 0.1762 - val_loss: 0.0428 - val_mae: 0.1080 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0773 - mae: 0.1734 - val_loss: 0.0405 - val_mae: 0.1009 - learning_rate: 2.5000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0771 - mae: 0.1733 - val_loss: 0.0405 - val_mae: 0.1041 - learning_rate: 2.5000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0762 - mae: 0.1726 - val_loss: 0.0398 - val_mae: 0.0994 - learning_rate: 2.5000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0763 - mae: 0.1726 - val_loss: 0.0391 - val_mae: 0.0980 - learning_rate: 2.5000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0760 - mae: 0.1731 - val_loss: 0.0410 - val_mae: 0.1072 - learning_rate: 2.5000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0763 - mae: 0.1727 - val_loss: 0.0396 - val_mae: 0.1002 - learning_rate: 2.5000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0757 - mae: 0.1724 - val_loss: 0.0395 - val_mae: 0.1017 - learning_rate: 2.5000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0767 - mae: 0.1744 - val_loss: 0.0408 - val_mae: 0.1054 - learning_rate: 2.5000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0748 - mae: 0.1724\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0748 - mae: 0.1724 - val_loss: 0.0398 - val_mae: 0.1038 - learning_rate: 2.5000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0739 - mae: 0.1710 - val_loss: 0.0382 - val_mae: 0.0972 - learning_rate: 1.2500e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0745 - mae: 0.1714 - val_loss: 0.0379 - val_mae: 0.0969 - learning_rate: 1.2500e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0734 - mae: 0.1702 - val_loss: 0.0382 - val_mae: 0.0982 - learning_rate: 1.2500e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0737 - mae: 0.1702 - val_loss: 0.0388 - val_mae: 0.1011 - learning_rate: 1.2500e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0743 - mae: 0.1708 - val_loss: 0.0384 - val_mae: 0.0980 - learning_rate: 1.2500e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0740 - mae: 0.1706 - val_loss: 0.0378 - val_mae: 0.0970 - learning_rate: 1.2500e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0732 - mae: 0.1698 - val_loss: 0.0374 - val_mae: 0.0959 - learning_rate: 1.2500e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0733 - mae: 0.1698 - val_loss: 0.0384 - val_mae: 0.1006 - learning_rate: 1.2500e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0734 - mae: 0.1708 - val_loss: 0.0382 - val_mae: 0.1010 - learning_rate: 1.2500e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0731 - mae: 0.1702 - val_loss: 0.0387 - val_mae: 0.1025 - learning_rate: 1.2500e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0728 - mae: 0.1694 - val_loss: 0.0379 - val_mae: 0.0999 - learning_rate: 1.2500e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m1134/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0738 - mae: 0.1703\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0738 - mae: 0.1703 - val_loss: 0.0373 - val_mae: 0.0963 - learning_rate: 1.2500e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0730 - mae: 0.1695 - val_loss: 0.0373 - val_mae: 0.0972 - learning_rate: 6.2500e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0723 - mae: 0.1690 - val_loss: 0.0377 - val_mae: 0.0992 - learning_rate: 6.2500e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0712 - mae: 0.1677 - val_loss: 0.0373 - val_mae: 0.0981 - learning_rate: 6.2500e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0721 - mae: 0.1688 - val_loss: 0.0368 - val_mae: 0.0952 - learning_rate: 6.2500e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0725 - mae: 0.1692 - val_loss: 0.0372 - val_mae: 0.0984 - learning_rate: 6.2500e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0731 - mae: 0.1701 - val_loss: 0.0365 - val_mae: 0.0938 - learning_rate: 6.2500e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0729 - mae: 0.1692 - val_loss: 0.0368 - val_mae: 0.0963 - learning_rate: 6.2500e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0709 - mae: 0.1682 - val_loss: 0.0374 - val_mae: 0.1002 - learning_rate: 6.2500e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0717 - mae: 0.1687 - val_loss: 0.0370 - val_mae: 0.0978 - learning_rate: 6.2500e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0725 - mae: 0.1695 - val_loss: 0.0370 - val_mae: 0.0962 - learning_rate: 6.2500e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m1143/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0720 - mae: 0.1695\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0720 - mae: 0.1695 - val_loss: 0.0366 - val_mae: 0.0954 - learning_rate: 6.2500e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0713 - mae: 0.1678 - val_loss: 0.0368 - val_mae: 0.0964 - learning_rate: 3.1250e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0708 - mae: 0.1675 - val_loss: 0.0366 - val_mae: 0.0954 - learning_rate: 3.1250e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0723 - mae: 0.1687 - val_loss: 0.0364 - val_mae: 0.0949 - learning_rate: 3.1250e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0716 - mae: 0.1683 - val_loss: 0.0367 - val_mae: 0.0964 - learning_rate: 3.1250e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0704 - mae: 0.1668 - val_loss: 0.0367 - val_mae: 0.0970 - learning_rate: 3.1250e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0711 - mae: 0.1679 - val_loss: 0.0364 - val_mae: 0.0954 - learning_rate: 3.1250e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0712 - mae: 0.1679 - val_loss: 0.0365 - val_mae: 0.0958 - learning_rate: 3.1250e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0716 - mae: 0.1676\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0716 - mae: 0.1676 - val_loss: 0.0370 - val_mae: 0.0982 - learning_rate: 3.1250e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0711 - mae: 0.1678 - val_loss: 0.0365 - val_mae: 0.0957 - learning_rate: 1.5625e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0713 - mae: 0.1683 - val_loss: 0.0365 - val_mae: 0.0961 - learning_rate: 1.5625e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0715 - mae: 0.1683 - val_loss: 0.0366 - val_mae: 0.0968 - learning_rate: 1.5625e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0703 - mae: 0.1671 - val_loss: 0.0364 - val_mae: 0.0957 - learning_rate: 1.5625e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m1126/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0704 - mae: 0.1668\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0705 - mae: 0.1668 - val_loss: 0.0364 - val_mae: 0.0958 - learning_rate: 1.5625e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0715 - mae: 0.1683 - val_loss: 0.0364 - val_mae: 0.0956 - learning_rate: 7.8125e-06\n",
      "Epoch 67/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0708 - mae: 0.1671 - val_loss: 0.0363 - val_mae: 0.0958 - learning_rate: 7.8125e-06\n",
      "Epoch 68/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0704 - mae: 0.1665 - val_loss: 0.0363 - val_mae: 0.0953 - learning_rate: 7.8125e-06\n",
      "Epoch 69/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0714 - mae: 0.1678 - val_loss: 0.0364 - val_mae: 0.0958 - learning_rate: 7.8125e-06\n",
      "Epoch 70/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0710 - mae: 0.1673 - val_loss: 0.0362 - val_mae: 0.0952 - learning_rate: 7.8125e-06\n",
      "Epoch 71/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0708 - mae: 0.1671 - val_loss: 0.0362 - val_mae: 0.0952 - learning_rate: 7.8125e-06\n",
      "Epoch 72/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0700 - mae: 0.1663 - val_loss: 0.0363 - val_mae: 0.0954 - learning_rate: 7.8125e-06\n",
      "Epoch 73/200\n",
      "\u001b[1m1128/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0705 - mae: 0.1672\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0705 - mae: 0.1672 - val_loss: 0.0362 - val_mae: 0.0948 - learning_rate: 7.8125e-06\n",
      "Epoch 74/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0708 - mae: 0.1666 - val_loss: 0.0362 - val_mae: 0.0949 - learning_rate: 3.9063e-06\n",
      "Epoch 75/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0702 - mae: 0.1666 - val_loss: 0.0363 - val_mae: 0.0954 - learning_rate: 3.9063e-06\n",
      "Epoch 76/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0702 - mae: 0.1667 - val_loss: 0.0364 - val_mae: 0.0955 - learning_rate: 3.9063e-06\n",
      "Epoch 77/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0705 - mae: 0.1671 - val_loss: 0.0362 - val_mae: 0.0945 - learning_rate: 3.9063e-06\n",
      "Epoch 78/200\n",
      "\u001b[1m1128/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0692 - mae: 0.1656\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0692 - mae: 0.1656 - val_loss: 0.0364 - val_mae: 0.0958 - learning_rate: 3.9063e-06\n",
      "Epoch 79/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0701 - mae: 0.1669 - val_loss: 0.0363 - val_mae: 0.0953 - learning_rate: 1.9531e-06\n",
      "Epoch 80/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0706 - mae: 0.1676 - val_loss: 0.0364 - val_mae: 0.0956 - learning_rate: 1.9531e-06\n",
      "Epoch 81/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0709 - mae: 0.1676 - val_loss: 0.0362 - val_mae: 0.0948 - learning_rate: 1.9531e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2294/2294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 763us/step\n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.997708 2.578707e-10\n",
      "voltage_fall_time_pulse1 0.997708 2.582018e-10\n",
      "current_fall_time_pulse1 0.997197 6.543143e-10\n",
      "current_fall_time_pulse2 0.997169 6.563879e-10\n",
      "      undershoot_pulse_2 0.984734 1.694730e+00\n",
      "      undershoot_pulse_1 0.983744 1.745580e+00\n",
      "voltage_rise_time_pulse1 0.982606 4.250890e-10\n",
      "   ringing_frequency_MHz 0.979833 4.252868e-06\n",
      "       overshoot_pulse_1 0.972761 2.107410e+00\n",
      "current_rise_time_pulse1 0.942562 1.152708e-08\n",
      "       overshoot_pulse_2 0.942214 6.598523e+00\n",
      "voltage_rise_time_pulse2 0.878941 1.342769e-09\n",
      "current_rise_time_pulse2 0.862826 8.734629e-09\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.997720 2.560418e-10\n",
      "voltage_fall_time_pulse1 0.997711 2.568564e-10\n",
      "current_fall_time_pulse1 0.997207 6.461655e-10\n",
      "current_fall_time_pulse2 0.997148 6.517671e-10\n",
      "      undershoot_pulse_2 0.985610 1.647724e+00\n",
      "      undershoot_pulse_1 0.984682 1.697592e+00\n",
      "voltage_rise_time_pulse1 0.982763 4.207239e-10\n",
      "   ringing_frequency_MHz 0.979852 4.213906e-06\n",
      "       overshoot_pulse_1 0.971627 2.094234e+00\n",
      "current_rise_time_pulse1 0.948197 1.080422e-08\n",
      "       overshoot_pulse_2 0.940332 6.640041e+00\n",
      "voltage_rise_time_pulse2 0.883574 1.302084e-09\n",
      "current_rise_time_pulse2 0.862945 8.674112e-09\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 738us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.997710 2.575972e-10\n",
      "voltage_fall_time_pulse1 0.997708 2.580004e-10\n",
      "current_fall_time_pulse1 0.997199 6.530984e-10\n",
      "current_fall_time_pulse2 0.997166 6.556969e-10\n",
      "      undershoot_pulse_2 0.984865 1.687762e+00\n",
      "      undershoot_pulse_1 0.983885 1.738466e+00\n",
      "voltage_rise_time_pulse1 0.982629 4.244371e-10\n",
      "   ringing_frequency_MHz 0.979836 4.247046e-06\n",
      "       overshoot_pulse_1 0.972599 2.105439e+00\n",
      "current_rise_time_pulse1 0.943389 1.142156e-08\n",
      "       overshoot_pulse_2 0.941937 6.604768e+00\n",
      "voltage_rise_time_pulse2 0.879623 1.336745e-09\n",
      "current_rise_time_pulse2 0.862847 8.725578e-09\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse2 0.930679 2.899862e-09\n",
      "current_fall_time_pulse1 0.927572 2.969888e-09\n",
      "       overshoot_pulse_1 0.473182 1.090591e+01\n",
      "current_rise_time_pulse2 0.189019 1.229910e-08\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 800us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 4: DROPOUT + L2 + LR SCHEDULER ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0025120D'\n",
    "\n",
    "BASE_DIR = \"fourth_iteration\"\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent_balanced.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED FEATURE ==============\n",
    "def compute_ringing_frequency(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    if L_eq > 0 and C_eq > 0:\n",
    "        return 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6\n",
    "    return np.nan\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['ringing_frequency_MHz'] = df_.apply(compute_ringing_frequency, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE =================\n",
    "encoder = LabelEncoder()\n",
    "all_parts = pd.concat([train_df['Part_Number'], test_df['Part_Number']])\n",
    "encoder.fit(all_parts)\n",
    "\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "]\n",
    "INPUT_COLUMNS.append(\"Part_encoded\")\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE ANN with DROPOUT + BN + L2 =================\n",
    "def build_ann(input_dim, output_dim, dropout=0.2, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(128, activation=None, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAIN MODEL ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop, lr_schedule], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/dropout_l2_scheduler_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL SCATTER PLOTS (TEST) ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='teal')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42388d67",
   "metadata": {},
   "source": [
    "## ITERATION - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ff22b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3478 - mae: 0.4006 - val_loss: 0.0780 - val_mae: 0.1469 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1237 - mae: 0.2241 - val_loss: 0.0676 - val_mae: 0.1427 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1080 - mae: 0.2093 - val_loss: 0.0570 - val_mae: 0.1298 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0981 - mae: 0.2005 - val_loss: 0.0519 - val_mae: 0.1231 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0945 - mae: 0.1966 - val_loss: 0.0486 - val_mae: 0.1150 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0921 - mae: 0.1939 - val_loss: 0.0473 - val_mae: 0.1106 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0905 - mae: 0.1912 - val_loss: 0.0475 - val_mae: 0.1154 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0898 - mae: 0.1897 - val_loss: 0.0497 - val_mae: 0.1208 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0884 - mae: 0.1882 - val_loss: 0.0466 - val_mae: 0.1161 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0866 - mae: 0.1865 - val_loss: 0.0494 - val_mae: 0.1232 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0868 - mae: 0.1866 - val_loss: 0.0479 - val_mae: 0.1144 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0859 - mae: 0.1849 - val_loss: 0.0482 - val_mae: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0871 - mae: 0.1860 - val_loss: 0.0472 - val_mae: 0.1173 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m1127/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0854 - mae: 0.1835\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0854 - mae: 0.1835 - val_loss: 0.0475 - val_mae: 0.1215 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0837 - mae: 0.1804 - val_loss: 0.0428 - val_mae: 0.1033 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0810 - mae: 0.1777 - val_loss: 0.0426 - val_mae: 0.1047 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0815 - mae: 0.1788 - val_loss: 0.0426 - val_mae: 0.1025 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0796 - mae: 0.1772 - val_loss: 0.0429 - val_mae: 0.1044 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0803 - mae: 0.1776 - val_loss: 0.0419 - val_mae: 0.1053 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0800 - mae: 0.1771 - val_loss: 0.0432 - val_mae: 0.1078 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0790 - mae: 0.1767 - val_loss: 0.0427 - val_mae: 0.1101 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0792 - mae: 0.1774 - val_loss: 0.0410 - val_mae: 0.1030 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0791 - mae: 0.1769 - val_loss: 0.0407 - val_mae: 0.1005 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0790 - mae: 0.1768 - val_loss: 0.0442 - val_mae: 0.1112 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0781 - mae: 0.1758 - val_loss: 0.0419 - val_mae: 0.1064 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0793 - mae: 0.1765 - val_loss: 0.0412 - val_mae: 0.1003 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0787 - mae: 0.1764 - val_loss: 0.0419 - val_mae: 0.1085 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m1127/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0783 - mae: 0.1757\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0783 - mae: 0.1757 - val_loss: 0.0408 - val_mae: 0.1031 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0760 - mae: 0.1730 - val_loss: 0.0397 - val_mae: 0.1021 - learning_rate: 2.5000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0757 - mae: 0.1723 - val_loss: 0.0394 - val_mae: 0.0994 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0761 - mae: 0.1732 - val_loss: 0.0393 - val_mae: 0.1020 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0761 - mae: 0.1729 - val_loss: 0.0396 - val_mae: 0.0996 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0759 - mae: 0.1719 - val_loss: 0.0390 - val_mae: 0.0997 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0750 - mae: 0.1726 - val_loss: 0.0405 - val_mae: 0.1043 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0755 - mae: 0.1716 - val_loss: 0.0387 - val_mae: 0.1002 - learning_rate: 2.5000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0750 - mae: 0.1720 - val_loss: 0.0399 - val_mae: 0.1036 - learning_rate: 2.5000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0744 - mae: 0.1715 - val_loss: 0.0412 - val_mae: 0.1082 - learning_rate: 2.5000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0750 - mae: 0.1724 - val_loss: 0.0423 - val_mae: 0.1125 - learning_rate: 2.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0753 - mae: 0.1722 - val_loss: 0.0389 - val_mae: 0.1007 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m1127/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0748 - mae: 0.1723\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0748 - mae: 0.1723 - val_loss: 0.0407 - val_mae: 0.1074 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0737 - mae: 0.1707 - val_loss: 0.0383 - val_mae: 0.1004 - learning_rate: 1.2500e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0731 - mae: 0.1702 - val_loss: 0.0381 - val_mae: 0.0970 - learning_rate: 1.2500e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0744 - mae: 0.1706 - val_loss: 0.0374 - val_mae: 0.0965 - learning_rate: 1.2500e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0733 - mae: 0.1698 - val_loss: 0.0390 - val_mae: 0.1040 - learning_rate: 1.2500e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0732 - mae: 0.1701 - val_loss: 0.0372 - val_mae: 0.0954 - learning_rate: 1.2500e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0730 - mae: 0.1699 - val_loss: 0.0372 - val_mae: 0.0954 - learning_rate: 1.2500e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0722 - mae: 0.1693 - val_loss: 0.0379 - val_mae: 0.0993 - learning_rate: 1.2500e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0735 - mae: 0.1703 - val_loss: 0.0374 - val_mae: 0.0965 - learning_rate: 1.2500e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0725 - mae: 0.1700 - val_loss: 0.0379 - val_mae: 0.1002 - learning_rate: 1.2500e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m1126/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0724 - mae: 0.1695\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0724 - mae: 0.1695 - val_loss: 0.0380 - val_mae: 0.1015 - learning_rate: 1.2500e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0719 - mae: 0.1690 - val_loss: 0.0376 - val_mae: 0.0996 - learning_rate: 6.2500e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0719 - mae: 0.1687 - val_loss: 0.0369 - val_mae: 0.0960 - learning_rate: 6.2500e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0724 - mae: 0.1689 - val_loss: 0.0369 - val_mae: 0.0965 - learning_rate: 6.2500e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0723 - mae: 0.1696 - val_loss: 0.0366 - val_mae: 0.0956 - learning_rate: 6.2500e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0712 - mae: 0.1681 - val_loss: 0.0370 - val_mae: 0.0973 - learning_rate: 6.2500e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0712 - mae: 0.1682 - val_loss: 0.0363 - val_mae: 0.0944 - learning_rate: 6.2500e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0713 - mae: 0.1678 - val_loss: 0.0369 - val_mae: 0.0976 - learning_rate: 6.2500e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0712 - mae: 0.1677 - val_loss: 0.0365 - val_mae: 0.0953 - learning_rate: 6.2500e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0713 - mae: 0.1682 - val_loss: 0.0362 - val_mae: 0.0942 - learning_rate: 6.2500e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0720 - mae: 0.1686 - val_loss: 0.0364 - val_mae: 0.0954 - learning_rate: 6.2500e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0704 - mae: 0.1671 - val_loss: 0.0372 - val_mae: 0.0998 - learning_rate: 6.2500e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0717 - mae: 0.1684 - val_loss: 0.0365 - val_mae: 0.0951 - learning_rate: 6.2500e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0717 - mae: 0.1685 - val_loss: 0.0368 - val_mae: 0.0969 - learning_rate: 6.2500e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m1138/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0705 - mae: 0.1671\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0705 - mae: 0.1671 - val_loss: 0.0369 - val_mae: 0.0979 - learning_rate: 6.2500e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0700 - mae: 0.1668 - val_loss: 0.0363 - val_mae: 0.0949 - learning_rate: 3.1250e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0707 - mae: 0.1674 - val_loss: 0.0365 - val_mae: 0.0967 - learning_rate: 3.1250e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0719 - mae: 0.1679 - val_loss: 0.0364 - val_mae: 0.0959 - learning_rate: 3.1250e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0712 - mae: 0.1673 - val_loss: 0.0364 - val_mae: 0.0959 - learning_rate: 3.1250e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m1139/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0707 - mae: 0.1673\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0707 - mae: 0.1673 - val_loss: 0.0363 - val_mae: 0.0955 - learning_rate: 3.1250e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2294/2294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step  \n",
      "\n",
      "Evaluation on train:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998039 2.385409e-10\n",
      "voltage_fall_time_pulse1 0.998034 2.391438e-10\n",
      "current_fall_time_pulse2 0.997526 6.136638e-10\n",
      "current_fall_time_pulse1 0.997525 6.148464e-10\n",
      "   ringing_frequency_MHz 0.989397 3.174561e+00\n",
      "      undershoot_pulse_2 0.984345 1.716170e+00\n",
      "      undershoot_pulse_1 0.983250 1.771914e+00\n",
      "voltage_rise_time_pulse1 0.981148 4.425448e-10\n",
      "       overshoot_pulse_1 0.971991 2.136983e+00\n",
      "       overshoot_pulse_2 0.943000 6.553546e+00\n",
      "current_rise_time_pulse1 0.942939 1.148919e-08\n",
      "voltage_rise_time_pulse2 0.879842 1.337767e-09\n",
      "current_rise_time_pulse2 0.863467 8.714201e-09\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "Evaluation on val:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998094 2.340690e-10\n",
      "voltage_fall_time_pulse1 0.998065 2.361723e-10\n",
      "current_fall_time_pulse1 0.997561 6.038076e-10\n",
      "current_fall_time_pulse2 0.997520 6.077356e-10\n",
      "   ringing_frequency_MHz 0.989307 3.171571e+00\n",
      "      undershoot_pulse_2 0.984999 1.682319e+00\n",
      "      undershoot_pulse_1 0.984165 1.726001e+00\n",
      "voltage_rise_time_pulse1 0.981299 4.382272e-10\n",
      "       overshoot_pulse_1 0.970647 2.130092e+00\n",
      "current_rise_time_pulse1 0.948542 1.076810e-08\n",
      "       overshoot_pulse_2 0.941194 6.591896e+00\n",
      "voltage_rise_time_pulse2 0.885018 1.293979e-09\n",
      "current_rise_time_pulse2 0.864112 8.637096e-09\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 698us/step\n",
      "\n",
      "Evaluation on test:\n",
      "                  Target       R2         RMSE\n",
      "voltage_fall_time_pulse2 0.998047 2.378754e-10\n",
      "voltage_fall_time_pulse1 0.998038 2.387004e-10\n",
      "current_fall_time_pulse1 0.997530 6.132032e-10\n",
      "current_fall_time_pulse2 0.997525 6.127782e-10\n",
      "   ringing_frequency_MHz 0.989383 3.174113e+00\n",
      "      undershoot_pulse_2 0.984443 1.711135e+00\n",
      "      undershoot_pulse_1 0.983388 1.765103e+00\n",
      "voltage_rise_time_pulse1 0.981171 4.418998e-10\n",
      "       overshoot_pulse_1 0.971799 2.135950e+00\n",
      "current_rise_time_pulse1 0.943761 1.138393e-08\n",
      "       overshoot_pulse_2 0.942733 6.559313e+00\n",
      "voltage_rise_time_pulse2 0.880603 1.331290e-09\n",
      "current_rise_time_pulse2 0.863566 8.702678e-09\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step\n",
      "                  Target       R2         RMSE\n",
      "current_fall_time_pulse1 0.955439 2.329494e-09\n",
      "current_fall_time_pulse2 0.952454 2.401614e-09\n",
      "       overshoot_pulse_1 0.682424 8.467503e+00\n",
      "      undershoot_pulse_2 0.373106 5.717314e+00\n",
      "      undershoot_pulse_1 0.332242 5.928062e+00\n",
      "       overshoot_pulse_2 0.195327 1.642060e+01\n",
      "current_rise_time_pulse1 0.023212 2.153509e-08\n",
      "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 827us/step\n"
     ]
    }
   ],
   "source": [
    "# ==================== ITERATION 5: PHYSICS FEATURES + ALL REGULARIZATION ====================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "SEED = 42\n",
    "UNSEEN_PART = 'C2M0025120D'\n",
    "BASE_DIR = \"fifth_iteration\"\n",
    "\n",
    "os.makedirs(f\"{BASE_DIR}/r2_rmse_tables\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/train_val_loss_curves\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/predicted_vs_actual\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/models\", exist_ok=True)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Neural_Network_Models\\merged_train_5_MOSFETs_25percent_balanced.csv\")\n",
    "\n",
    "# ============== TARGETS + DROPS ==============\n",
    "TARGET_COLUMNS = [\n",
    "    'voltage_rise_time_pulse1', 'voltage_rise_time_pulse2',\n",
    "    'voltage_fall_time_pulse1', 'voltage_fall_time_pulse2',\n",
    "    'current_rise_time_pulse1', 'current_rise_time_pulse2',\n",
    "    'current_fall_time_pulse1', 'current_fall_time_pulse2',\n",
    "    'overshoot_pulse_1', 'overshoot_pulse_2',\n",
    "    'undershoot_pulse_1', 'undershoot_pulse_2',\n",
    "    'ringing_frequency_MHz'\n",
    "]\n",
    "DROP_COLUMNS = ['DeviceID', 'MOSFET', 'Part_Number']\n",
    "\n",
    "# ============== SPLIT SEEN/UNSEEN DEVICES ==============\n",
    "seen_parts = df['Part_Number'].unique().tolist()\n",
    "seen_parts = [p for p in seen_parts if p != UNSEEN_PART]\n",
    "train_df = df[df['Part_Number'].isin(seen_parts)].copy()\n",
    "test_df = df[df['Part_Number'] == UNSEEN_PART].copy()\n",
    "\n",
    "# ============== DERIVED PHYSICS FEATURES ==============\n",
    "def compute_physics_features(row):\n",
    "    L_eq = row[['Ls4', 'Ls5', 'Ls6', 'Ls7', 'Ls8', 'Ls9', 'Ls10', 'Ls11']].sum()\n",
    "    C_eq = row.get(\"Coss\", 1e-12)\n",
    "    f_resonance = 1 / (2 * np.pi * np.sqrt(L_eq * C_eq)) / 1e6 if L_eq > 0 and C_eq > 0 else 0\n",
    "    overshoot_est = row[\"VDS_max\"] - row[\"Vbus\"]\n",
    "    undershoot_est = 0 - row[\"VGS_th_min\"]\n",
    "    dVdt_est = row[\"VDS_max\"] / row[\"Tp1\"] if row[\"Tp1\"] != 0 else 0\n",
    "    dIdt_est = row[\"ID_max_25C\"] / row[\"Tp1\"] if row[\"Tp1\"] != 0 else 0\n",
    "    return pd.Series([f_resonance, overshoot_est, undershoot_est, dVdt_est, dIdt_est])\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_[['f_resonance', 'overshoot_est', 'undershoot_est', 'dVdt_est', 'dIdt_est']] = df_.apply(compute_physics_features, axis=1)\n",
    "\n",
    "# ============== ENCODE DEVICE ==============\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(pd.concat([train_df['Part_Number'], test_df['Part_Number']]))\n",
    "train_df['Part_encoded'] = encoder.transform(train_df['Part_Number'])\n",
    "test_df['Part_encoded'] = encoder.transform(test_df['Part_Number'])\n",
    "\n",
    "# ============== INPUT FEATURES ==============\n",
    "INPUT_COLUMNS = [\n",
    "    col for col in df.columns if col not in TARGET_COLUMNS + DROP_COLUMNS\n",
    "] + ['f_resonance', 'overshoot_est', 'undershoot_est', 'dVdt_est', 'dIdt_est', 'Part_encoded']\n",
    "\n",
    "# ============== SCALE INPUTS ==============\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(pd.concat([train_df[INPUT_COLUMNS], test_df[INPUT_COLUMNS]]))\n",
    "X_train_all = input_scaler.transform(train_df[INPUT_COLUMNS])\n",
    "X_test_all = input_scaler.transform(test_df[INPUT_COLUMNS])\n",
    "\n",
    "# ============== SCALE OUTPUTS ==============\n",
    "output_scalers = {}\n",
    "y_train_scaled = pd.DataFrame()\n",
    "y_test_scaled = pd.DataFrame()\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    scaler = MinMaxScaler() if col == 'ringing_frequency_MHz' else StandardScaler()\n",
    "    y_train_scaled[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "    y_test_scaled[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    output_scalers[col] = scaler\n",
    "\n",
    "# ============== SPLIT TRAIN/VAL (70/15/15) ==============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_all, y_train_scaled.values, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "# ============== DEFINE ANN =================\n",
    "def build_ann(input_dim, output_dim, dropout=0.2, l2_reg=1e-4):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(128, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_ann(X_train.shape[1], y_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# ============== TRAINING ==============\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=200, batch_size=64, callbacks=[early_stop, lr_schedule], verbose=1)\n",
    "\n",
    "model.save(f\"{BASE_DIR}/models/iteration5_final_ann.h5\")\n",
    "\n",
    "# ============== SAVE LOSS CURVE ==============\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{BASE_DIR}/train_val_loss_curves/loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============== EVALUATION FUNCTION ==============\n",
    "def evaluate_and_save(X, y_scaled, name, filter_positive=False):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    results = []\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        y_true = output_scalers[col].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        y_pred = output_scalers[col].inverse_transform(y_pred_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        results.append((col, r2, rmse))\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Target\", \"R2\", \"RMSE\"])\n",
    "    df_results = df_results.sort_values(\"R2\", ascending=False)\n",
    "    if filter_positive:\n",
    "        df_results = df_results[df_results[\"R2\"] > 0]\n",
    "    else:\n",
    "        print(f\"\\nEvaluation on {name}:\")\n",
    "\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(f\"{BASE_DIR}/r2_rmse_tables/{name}.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "# ============== RUN EVALUATIONS ==============\n",
    "evaluate_and_save(X_train, y_train, \"train\")\n",
    "evaluate_and_save(X_val, y_val, \"val\")\n",
    "evaluate_and_save(X_train_all, y_train_scaled.values, \"test\")\n",
    "evaluate_and_save(X_test_all, y_test_scaled.values, \"unseen\", filter_positive=True)\n",
    "\n",
    "# ============== PREDICTED vs ACTUAL SCATTER PLOTS ==============\n",
    "y_pred_scaled_test = model.predict(X_train_all)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    y_true = output_scalers[col].inverse_transform(y_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "    y_pred = output_scalers[col].inverse_transform(y_pred_scaled_test[:, i].reshape(-1, 1)).flatten()\n",
    "    axes[i].scatter(y_true, y_pred, s=10, alpha=0.6, color='darkblue')\n",
    "    axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
    "    axes[i].set_title(f\"{col}\")\n",
    "    axes[i].set_xlabel(\"Actual\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{BASE_DIR}/predicted_vs_actual/scatter_internal_test.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
