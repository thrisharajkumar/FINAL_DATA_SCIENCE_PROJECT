(base) PS C:\Users\pc\Desktop\PROJECT_THESIS_Thrisha_Rajkumar> $env:PYTHONPATH="."; python src/ANN/optuna_tuner.py
2025-08-04 17:45:40.001670: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-04 17:45:41.164163: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
ðŸš€ Starting Optuna Tuning...
[I 2025-08-04 17:45:46,123] A new study created in memory with name: no-name-8264a34f-3c3d-45e1-bccd-d127327d6737
2025-08-04 17:45:49.720319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[I 2025-08-04 17:46:33,181] Trial 0 finished with value: 1.247093247561272 and parameters: {'n_layers': 3, 'n_units': 142, 'activation': 'tanh', 'dropout': 0.17663503807858305, 'optimizer': 'rmsprop', 'learning_rate': 0.0002342526950876245, 'batch_size': 256}. Best is trial 0 with value: 1.247093247561272.
[I 2025-08-04 17:47:44,393] Trial 1 finished with value: 124.36652668695608 and parameters: {'n_layers': 4, 'n_units': 339, 'activation': 'tanh', 'dropout': 0.05552259759790651, 'optimizer': 'rmsprop', 'learning_rate': 0.00021919112911375632, 'batch_size': 1024}. Best is trial 0 with value: 1.247093247561272.
[I 2025-08-04 17:47:59,398] Trial 2 finished with value: 312.47668935844047 and parameters: {'n_layers': 1, 'n_units': 355, 'activation': 'relu', 'dropout': 0.31104872564461344, 'optimizer': 'rmsprop', 'learning_rate': 0.0028974616781215824, 'batch_size': 512}. Best is trial 0 with value: 1.247093247561272.
[I 2025-08-04 17:48:14,795] Trial 3 finished with value: 15.696679311709671 and parameters: {'n_layers': 1, 'n_units': 188, 'activation': 'relu', 'dropout': 0.2601429685663327, 'optimizer': 'rmsprop', 'learning_rate': 0.0008857643529475178, 'batch_size': 1024}. Best is trial 0 with value: 1.247093247561272.
[I 2025-08-04 17:48:26,685] Trial 4 finished with value: 10.280433885047078 and parameters: {'n_layers': 2, 'n_units': 146, 'activation': 'relu', 'dropout': 0.46372963730781125, 'optimizer': 'rmsprop', 'learning_rate': 0.0012555504552870047, 'batch_size': 1024}. Best is trial 0 with value: 1.247093247561272.
[I 2025-08-04 17:48:50,823] Trial 5 finished with value: 6.199292205679159 and parameters: {'n_layers': 4, 'n_units': 149, 'activation': 'tanh', 'dropout': 0.48171951645650124, 'optimizer': 'adam', 'learning_rate': 0.0063314540576185, 'batch_size': 256}. Best is trial 0 with value: 1.247093247561272. 
[I 2025-08-04 17:49:35,684] Trial 6 finished with value: 226.64249458602933 and parameters: {'n_layers': 2, 'n_units': 418, 'activation': 'relu', 'dropout': 0.0760858401631378, 'optimizer': 'rmsprop', 'learning_rate': 4.1169700317355645e-05, 'batch_size': 512}. Best is trial 0 with value: 1.247093247561272.
[I 2025-08-04 17:50:16,602] Trial 7 finished with value: 0.28802753759337163 and parameters: {'n_layers': 2, 'n_units': 101, 'activation': 'tanh', 'dropout': 0.08950275678450748, 'optimizer': 'adam', 'learning_rate': 0.0026460932214446564, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:51:14,470] Trial 8 finished with value: 34.86832142700008 and parameters: {'n_layers': 2, 'n_units': 236, 'activation': 'tanh', 'dropout': 0.3477470152918892, 'optimizer': 'rmsprop', 'learning_rate': 1.7394315082643356e-05, 'batch_size': 512}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:51:45,744] Trial 9 finished with value: 291.4046145681461 and parameters: {'n_layers': 1, 'n_units': 402, 'activation': 'relu', 'dropout': 0.06769454811859688, 'optimizer': 'rmsprop', 'learning_rate': 1.670110260840978e-05, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:52:11,806] Trial 10 finished with value: 0.3177941059141397 and parameters: {'n_layers': 3, 'n_units': 65, 'activation': 'tanh', 'dropout': 0.16101619804314138, 'optimizer': 'adam', 'learning_rate': 0.006434711086072563, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:52:30,386] Trial 11 finished with value: 0.487993244499105 and parameters: {'n_layers': 3, 'n_units': 83, 'activation': 'tanh', 'dropout': 0.1673488657649158, 'optimizer': 'adam', 'learning_rate': 0.009794338466581615, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:53:17,778] Trial 12 finished with value: 981.2098765486563 and parameters: {'n_layers': 3, 'n_units': 511, 'activation': 'tanh', 'dropout': 2.3600673749513135e-05, 'optimizer': 'adam', 'learning_rate': 0.0024518385608682993, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:53:50,579] Trial 13 finished with value: 0.3177683500213756 and parameters: {'n_layers': 3, 'n_units': 66, 'activation': 'tanh', 'dropout': 0.17652741788525184, 'optimizer': 'adam', 'learning_rate': 0.0007740258427809463, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:55:02,268] Trial 14 finished with value: 1.3566626285117351 and parameters: {'n_layers': 2, 'n_units': 241, 'activation': 'tanh', 'dropout': 0.13253265285143973, 'optimizer': 'adam', 'learning_rate': 0.0005821813887571298, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:56:22,906] Trial 15 finished with value: 18.27963031485457 and parameters: {'n_layers': 4, 'n_units': 64, 'activation': 'tanh', 'dropout': 0.2324395231489526, 'optimizer': 'adam', 'learning_rate': 0.00010512432457010416, 'batch_size': 256}. Best is trial 7 with value: 0.28802753759337163.
[I 2025-08-04 17:57:56,669] Trial 16 finished with value: 0.275381907865063 and parameters: {'n_layers': 3, 'n_units': 211, 'activation': 'tanh', 'dropout': 0.2390853098559367, 'optimizer': 'adam', 'learning_rate': 0.0018275990890550013, 'batch_size': 256}. Best is trial 16 with value: 0.275381907865063.
[I 2025-08-04 17:59:05,556] Trial 17 finished with value: 34.067876816811264 and parameters: {'n_layers': 2, 'n_units': 239, 'activation': 'tanh', 'dropout': 0.3932229769040755, 'optimizer': 'adam', 'learning_rate': 0.0024709385234512903, 'batch_size': 256}. Best is trial 16 with value: 0.275381907865063.
[I 2025-08-04 17:59:48,079] Trial 18 finished with value: 132.40206076889712 and parameters: {'n_layers': 3, 'n_units': 287, 'activation': 'tanh', 'dropout': 0.24914562678868787, 'optimizer': 'adam', 'learning_rate': 0.001806616885238858, 'batch_size': 1024}. Best is trial 16 with value: 0.275381907865063.
[I 2025-08-04 18:00:37,952] Trial 19 finished with value: 66.28688300154056 and parameters: {'n_layers': 2, 'n_units': 196, 'activation': 'tanh', 'dropout': 0.10019520727348746, 'optimizer': 'adam', 'learning_rate': 0.0004692277346969577, 'batch_size': 512}. Best is trial 16 with value: 0.275381907865063.
âœ… Best trial: {'n_layers': 3, 'n_units': 211, 'activation': 'tanh', 'dropout': 0.2390853098559367, 'optimizer': 'adam', 'learning_rate': 0.0018275990890550013, 'batch_size': 256}