# src/preprocessing.py

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
import joblib
import json

from src.config import (
    TARGET_COLUMNS, DROP_COLUMNS,
    INPUT_SCALER_PATH, OUTPUT_SCALER_PATH, FEATURES_JSON_PATH
)

def preprocess_data(df, num_features=50):
    # Drop NaNs
    df = df.dropna()

    # Split features and targets
    X = df.drop(columns=DROP_COLUMNS)
    y = df[TARGET_COLUMNS]

    # Scale inputs
    input_scaler = StandardScaler()
    X_scaled = input_scaler.fit_transform(X)

    # Variance-based feature selection (safe for multi-output)
    selector = VarianceThreshold(threshold=0.0)
    X_selected = selector.fit_transform(X_scaled)

    selected_features = X.columns[selector.get_support()].tolist()
    if len(selected_features) > num_features:
        selected_features = selected_features[:num_features]
        selected_indices = [X.columns.get_loc(f) for f in selected_features]
        X_selected = X_scaled[:, selected_indices]

    # Scale targets
    output_scaler = StandardScaler()
    y_scaled = output_scaler.fit_transform(y)

    # Save all artifacts
    joblib.dump(input_scaler, INPUT_SCALER_PATH)
    joblib.dump(output_scaler, OUTPUT_SCALER_PATH)
    with open(FEATURES_JSON_PATH, "w") as f:
        json.dump(selected_features, f)

    return X_selected, y_scaled, input_scaler, output_scaler, selected_features

def preprocess_from_file(path, num_features=50):
    df = pd.read_csv(path)
    return preprocess_data(df, num_features=num_features)
