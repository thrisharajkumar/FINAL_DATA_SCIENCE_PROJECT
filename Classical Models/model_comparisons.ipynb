{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e755c25f",
   "metadata": {},
   "source": [
    "Models \n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Random Forest\n",
    "\n",
    "LightGBM\n",
    "\n",
    "XGBoost\n",
    "\n",
    "Support Vector Regression (on a 20K-row sample)\n",
    "\n",
    "Keras-based Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75770041",
   "metadata": {},
   "source": [
    "R¬≤ (how well predictions match actuals)\n",
    "\n",
    "RMSE (root mean squared error)\n",
    "\n",
    "MAE (mean absolute error)\n",
    "\n",
    "Training time (in seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a3954",
   "metadata": {},
   "source": [
    "| Model    | R¬≤ Avg | Best Target R¬≤ | Worst Target R¬≤ | Train Time | Notes   |\n",
    "| -------- | ------ | -------------- | --------------- | ---------- | ------- |\n",
    "| Linear   |        |                |                 |            |         |\n",
    "| RF       |        |                |                 |            |         |\n",
    "| LightGBM |        |                |                 |            |         |\n",
    "| XGBoost  |        |                |                 |            |         |\n",
    "| **SVR**  |        |                |                 |            | Sampled |\n",
    "| **ANN**  |        |                |                 |            | Final   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cba8ca",
   "metadata": {},
   "source": [
    "üéØ PRIMARY GOALS\n",
    "1. Faster replication of simulation data\n",
    "Predict voltage/current rise/fall times, overshoot, and ringing frequency as accurately as the simulator (~95‚Äì100%).\n",
    "\n",
    "2. Device consistency under parasitic tolerance\n",
    "Model and analyze best/worst-case EMI behavior under inductive and resistive parasitic variations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ddf521",
   "metadata": {},
   "source": [
    "üß± FULL STRUCTURE: 6 MODEL PIPELINE\n",
    "üîπ Stage 1: Baseline Models (for benchmarking)\n",
    "Model #\tModel Name\tPurpose\n",
    "- Linear Regression\tBenchmark; sets a lower bound for model skill\n",
    "- Random Forest\tCaptures non-linearities + feature importance\n",
    "- LightGBM\tFast training + very strong non-linear performance\n",
    "- XGBoost\tGradient-boosted trees, accurate with tuning, handles 500K rows well\n",
    "\n",
    "üîπ Stage 2: Your Focus Models (Advanced & Deep)\n",
    "Model #\tModel Name\tPurpose\n",
    "- Support Vector Machine (SVR)\tPrecise control, great for small to mid-range test sets (use sampling)\n",
    "- Artificial Neural Network (ANN)\tLearn complex parasitic‚Äìoutput relationships, generalizes across configs\n",
    "\n",
    "üîç MODEL CONFIGURATIONS (Details)\n",
    "- Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = MultiOutputRegressor(LinearRegression())\n",
    "Simple, interpretable; baseline for comparison.\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=25))\n",
    "Visualize feature importance, builds confidence in signal strength.\n",
    "\n",
    "- LightGBM\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "model = MultiOutputRegressor(LGBMRegressor(n_estimators=200, learning_rate=0.1, num_leaves=64))\n",
    "Efficient gradient boosting, quick to train.\n",
    "\n",
    "- XGBoost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "model = MultiOutputRegressor(XGBRegressor(n_estimators=200, max_depth=8, learning_rate=0.1))\n",
    "Highly tunable, great for fine-grained prediction.\n",
    "\n",
    "-  ADVANCED MODELS (Your Focus)\n",
    "- Support Vector Regression (SVR)\n",
    "- Use sampling (e.g., 20,000 rows) due to scaling complexity of SVR.\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "model = MultiOutputRegressor(SVR(kernel='rbf', C=10, epsilon=0.01))\n",
    "Best when features are scaled and targets normalized. Sensitive to noise but accurate when tuned.\n",
    "\n",
    "- Artificial Neural Network (Keras)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(target_columns))  # 13 outputs\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "model.fit(X_scaled, y, validation_split=0.1, epochs=50, batch_size=512)\n",
    "Strongest option for your final accuracy ‚Äî non-linear, multi-target, scalable with GPU.\n",
    "\n",
    "- ADDITIONAL FEATURES FOR DEVICE SCORECARD\n",
    "Once you‚Äôve trained these models:\n",
    "\n",
    "Predict outputs over a grid of parasitic combinations (Ls4‚ÄìLs11)\n",
    "\n",
    "Measure:\n",
    "\n",
    "Min/Max ringing\n",
    "\n",
    "Max overshoot\n",
    "\n",
    "Standard deviation of outputs per DeviceID\n",
    "\n",
    "To generate your ‚Äúdevice performance report‚Äù under parasitic tolerance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
